[
    {
        "continuation": {
            "0-46": "DMN",
            "47-86": "ToM",
            "87-130": "ToM",
            "131-180": "ToM",
            "182-241": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 91,
        "prompt_tokens": 747,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor makes the cappuccino using the milk in the pitcher.\\nQuestion: Does Noor believe the milk pitcher contains oat milk or almond milk?\\nChoose one of the following:\\na)Noor believes the milk pitcher contains oat milk.\\nb)Noor believes the milk pitcher contains almond milk.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Noor filled the pitcher with oat milk and was unaware of the coworker\u2019s swap. She has no information that it was changed, so she continues to believe it contains oat milk.\n\nAnswer:a) Noor believes the milk pitcher contains oat milk."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Noor is working as a barista at a busy coffee shop. Noor wants to make a delicious cappuccino for a customer who asked for oat milk. Noor grabs a milk pitcher and fills it with oat milk. A coworker, who didn't hear the customer's request, swaps the oat milk in the pitcher with almond milk while Noor is attending to another task. Noor makes the cappuccino using the milk in the pitcher.\nQuestion: Does Noor believe the milk pitcher contains oat milk or almond milk?\nChoose one of the following:\na)Noor believes the milk pitcher contains oat milk.\nb)Noor believes the milk pitcher contains almond milk.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Noor filled the pitcher with oat milk and was unaware of the coworker\u2019s swap. She has no information that it was changed, so she continues to believe it contains oat milk.\n\nAnswer:a) Noor believes the milk pitcher contains oat milk."
    },
    {
        "continuation": {
            "0-71": "DMN",
            "72-179": "ToM",
            "180-245": "ToM",
            "248-316": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 100,
        "prompt_tokens": 782,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Zara is attending an art exhibition at a local gallery in her town. Zara wants to learn more about a particular abstract painting she admires. The label next to the artwork states that the painting is the work of a famous artist, and Zara reads the label carefully. A gallery staff member, realizing there was a mix-up, changes the label with the correct information attributing the painting to an emerging artist instead. Zara looks for more information about the famous artist.\\nQuestion: Does Zara believe the painting is the work of a famous artist or an emerging artist?\\nChoose one of the following:\\na)Zara believes the painting is the work of a famous artist.\\nb)Zara believes the painting is the work of an emerging artist.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Zara initially reads the label saying it\u2019s by a famous artist. Even after the staff corrects the label, she proceeds to look for more information about the famous artist, indicating she still believes the painting is by a famous artist.  \nAnswer: a)Zara believes the painting is the work of a famous artist."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Zara is attending an art exhibition at a local gallery in her town. Zara wants to learn more about a particular abstract painting she admires. The label next to the artwork states that the painting is the work of a famous artist, and Zara reads the label carefully. A gallery staff member, realizing there was a mix-up, changes the label with the correct information attributing the painting to an emerging artist instead. Zara looks for more information about the famous artist.\nQuestion: Does Zara believe the painting is the work of a famous artist or an emerging artist?\nChoose one of the following:\na)Zara believes the painting is the work of a famous artist.\nb)Zara believes the painting is the work of an emerging artist.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Zara initially reads the label saying it\u2019s by a famous artist. Even after the staff corrects the label, she proceeds to look for more information about the famous artist, indicating she still believes the painting is by a famous artist.  \nAnswer: a)Zara believes the painting is the work of a famous artist."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-131": "ToM",
            "132-237": "ToM",
            "239-292": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 94,
        "prompt_tokens": 747,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Carlos is a farmer in a small village in Mexico, tending to his cornfield. He wants to irrigate his crops by opening a water valve connected to a nearby river. Carlos closed the valve the previous evening. During the night, heavy rainfall caused the river to overflow, opening the valve and flooding the cornfield. Carlos calls his neighbour to ask them if they could open the valve.\\nQuestion: Does Carlos believe the water valve is closed or open?\\nChoose one of the following:\\na)Carlos believes the water valve is open.\\nb)Carlos believes the water valve is closed.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Carlos closed the valve the previous evening and, not knowing it was forced open by the flood, believes it remains closed. He calls his neighbor to ask them to open it, which only makes sense if he thinks it is currently closed.\n\nAnswer: b) Carlos believes the water valve is closed."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Carlos is a farmer in a small village in Mexico, tending to his cornfield. He wants to irrigate his crops by opening a water valve connected to a nearby river. Carlos closed the valve the previous evening. During the night, heavy rainfall caused the river to overflow, opening the valve and flooding the cornfield. Carlos calls his neighbour to ask them if they could open the valve.\nQuestion: Does Carlos believe the water valve is closed or open?\nChoose one of the following:\na)Carlos believes the water valve is open.\nb)Carlos believes the water valve is closed.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Carlos closed the valve the previous evening and, not knowing it was forced open by the flood, believes it remains closed. He calls his neighbor to ask them to open it, which only makes sense if he thinks it is currently closed.\n\nAnswer: b) Carlos believes the water valve is closed."
    },
    {
        "continuation": {
            "0-55": "DMN",
            "56-89": "DMN",
            "90-151": "DMN",
            "152-226": "ToM",
            "229-289": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 106,
        "prompt_tokens": 757,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kofi is a fisherman from a small village in Ghana. He wants to catch enough fish today to provide for his family and sell the surplus at the market. Kofi repaired his fishing net last night. While Kofi is away from his boat, a group of monkeys comes and plays with the fishing net, tearing it apart. Kofi goes fishing with his net.\\nQuestion: Does Kofi believe his fishing net is in good condition or torn apart?\\nChoose one of the following:\\na)Kofi believes his fishing net is in good condition.\\nb)Kofi believes his fishing net is torn apart.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kofi repaired his net and left it by his boat. He was away when monkeys tore it. There\u2019s no indication he saw the damage before going fishing. Since he proceeds to fish with it, he must believe it\u2019s in good condition.  \nAnswer:a)Kofi believes his fishing net is in good condition."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kofi is a fisherman from a small village in Ghana. He wants to catch enough fish today to provide for his family and sell the surplus at the market. Kofi repaired his fishing net last night. While Kofi is away from his boat, a group of monkeys comes and plays with the fishing net, tearing it apart. Kofi goes fishing with his net.\nQuestion: Does Kofi believe his fishing net is in good condition or torn apart?\nChoose one of the following:\na)Kofi believes his fishing net is in good condition.\nb)Kofi believes his fishing net is torn apart.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kofi repaired his net and left it by his boat. He was away when monkeys tore it. There\u2019s no indication he saw the damage before going fishing. Since he proceeds to fish with it, he must believe it\u2019s in good condition.  \nAnswer:a)Kofi believes his fishing net is in good condition."
    },
    {
        "continuation": {
            "9-105": "DMN",
            "106-142": "ToM",
            "143-231": "ToM",
            "232-333": "ToM",
            "343-403": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 121,
        "prompt_tokens": 788,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Azola is a teacher at a small school in rural China. She wants to surprise her students with a new book for the school library. Azola got a notification from Amazon stating that the package containing the book was left at her doorstep. A gust of wind blows the package off her doorstep, and a neighbor replaces it with a different package containing a hand-knit scarf. Azola brings the package with the book to school.\\nQuestion: Does Azola believe the package contains the book she ordered or a hand-knit scarf?\\nChoose one of the following:\\na)Azola believes the package contains the book she ordered.\\nb)Azola believes the package contains a hand-knit scarf.\\nLet's think step by step:\"}]\n\n**Generation**\nThought:\nAzola ordered a book and received a notification that the package was delivered to her doorstep. She did not witness any replacement. Unbeknownst to her, a neighbor swapped the package for one containing a hand-knit scarf. Since Azola is unaware of this swap, she continues to believe the package holds the book she ordered.\n\nAnswer:\na) Azola believes the package contains the book she ordered."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Azola is a teacher at a small school in rural China. She wants to surprise her students with a new book for the school library. Azola got a notification from Amazon stating that the package containing the book was left at her doorstep. A gust of wind blows the package off her doorstep, and a neighbor replaces it with a different package containing a hand-knit scarf. Azola brings the package with the book to school.\nQuestion: Does Azola believe the package contains the book she ordered or a hand-knit scarf?\nChoose one of the following:\na)Azola believes the package contains the book she ordered.\nb)Azola believes the package contains a hand-knit scarf.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought:\nAzola ordered a book and received a notification that the package was delivered to her doorstep. She did not witness any replacement. Unbeknownst to her, a neighbor swapped the package for one containing a hand-knit scarf. Since Azola is unaware of this swap, she continues to believe the package holds the book she ordered.\n\nAnswer:\na) Azola believes the package contains the book she ordered."
    },
    {
        "continuation": {
            "0-104": "DMN",
            "105-233": "ToM",
            "235-287": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 88,
        "prompt_tokens": 761,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Ravi is a farmer in a small Indian village, tending to his mango orchard. Ravi wants to harvest ripe mangoes to sell at the local market. He notices a mango tree with a heavy branch laden with ripe mangoes. A mischievous monkey comes along and eats all the ripe mangoes on the branch while Ravi is gathering his harvesting tools. Ravi gets ready to harvest the ripe mangoes from the branch.\\nQuestion: Does Ravi believe the branch has ripe mangoes or no ripe mangoes?\\nChoose one of the following:\\na)Ravi believes the branch has ripe mangoes.\\nb)Ravi believes the branch has no ripe mangoes.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Ravi initially notices the branch laden with ripe mangoes and then goes off to fetch his tools. He does not observe the monkey eating the mangoes, so when he returns to harvest, he still believes the branch has ripe mangoes.\n\nAnswer:a) Ravi believes the branch has ripe mangoes."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Ravi is a farmer in a small Indian village, tending to his mango orchard. Ravi wants to harvest ripe mangoes to sell at the local market. He notices a mango tree with a heavy branch laden with ripe mangoes. A mischievous monkey comes along and eats all the ripe mangoes on the branch while Ravi is gathering his harvesting tools. Ravi gets ready to harvest the ripe mangoes from the branch.\nQuestion: Does Ravi believe the branch has ripe mangoes or no ripe mangoes?\nChoose one of the following:\na)Ravi believes the branch has ripe mangoes.\nb)Ravi believes the branch has no ripe mangoes.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Ravi initially notices the branch laden with ripe mangoes and then goes off to fetch his tools. He does not observe the monkey eating the mangoes, so when he returns to harvest, he still believes the branch has ripe mangoes.\n\nAnswer:a) Ravi believes the branch has ripe mangoes."
    },
    {
        "continuation": {
            "0-60": "ToM",
            "61-136": "ToM",
            "139-196": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 72,
        "prompt_tokens": 735,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kofi is a chef at a popular Ghanaian restaurant. Kofi wants to prepare a traditional groundnut soup for the dinner menu. Kofi adds a small container of peanut butter to the soup pot on the stove. An apprentice chef, mistakenly thinking it was a seasoning paste, removes the peanut butter from the soup and replaces it with tomato paste. Kofi continues cooking the soup.\\nQuestion: Does Kofi believe the soup contains peanut butter or tomato paste?\\nChoose one of the following:\\na)Kofi believes the soup contains peanut butter.\\nb)Kofi believes the soup contains tomato paste.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kofi added peanut butter and didn\u2019t see it removed. He is unaware of the swap, so he still thinks peanut butter is in the soup.  \nAnswer: a) Kofi believes the soup contains peanut butter."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kofi is a chef at a popular Ghanaian restaurant. Kofi wants to prepare a traditional groundnut soup for the dinner menu. Kofi adds a small container of peanut butter to the soup pot on the stove. An apprentice chef, mistakenly thinking it was a seasoning paste, removes the peanut butter from the soup and replaces it with tomato paste. Kofi continues cooking the soup.\nQuestion: Does Kofi believe the soup contains peanut butter or tomato paste?\nChoose one of the following:\na)Kofi believes the soup contains peanut butter.\nb)Kofi believes the soup contains tomato paste.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kofi added peanut butter and didn\u2019t see it removed. He is unaware of the swap, so he still thinks peanut butter is in the soup.  \nAnswer: a) Kofi believes the soup contains peanut butter."
    },
    {
        "continuation": {
            "0-84": "DMN",
            "85-198": "ToM",
            "199-267": "ToM",
            "270-332": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 108,
        "prompt_tokens": 774,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kael is a farmer in a remote village in the Andes. Kael wants to plant a new crop of high-altitude potatoes in one of his terraced fields. The soil in the field appears rich and well-fertilized. Overnight, heavy rainfall washes away much of the fertile soil, leaving mostly rocky dirt behind. Kael plants the high-altitude potatoes in the field.\\nQuestion: Does Kael believe the soil is rich and well-fertilized or mostly rocky dirt?\\nChoose one of the following:\\na)Kael believes the soil is rich and well-fertilized.\\nb)Kael believes the soil is mostly rocky dirt.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kael observed the field before planting and saw rich, well-fertilized soil. Although heavy rain washed away much of that soil, the story doesn\u2019t say he realized this change before planting. Therefore, Kael still believes the soil is rich and well-fertilized.  \nAnswer: a) Kael believes the soil is rich and well-fertilized."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kael is a farmer in a remote village in the Andes. Kael wants to plant a new crop of high-altitude potatoes in one of his terraced fields. The soil in the field appears rich and well-fertilized. Overnight, heavy rainfall washes away much of the fertile soil, leaving mostly rocky dirt behind. Kael plants the high-altitude potatoes in the field.\nQuestion: Does Kael believe the soil is rich and well-fertilized or mostly rocky dirt?\nChoose one of the following:\na)Kael believes the soil is rich and well-fertilized.\nb)Kael believes the soil is mostly rocky dirt.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kael observed the field before planting and saw rich, well-fertilized soil. Although heavy rain washed away much of that soil, the story doesn\u2019t say he realized this change before planting. Therefore, Kael still believes the soil is rich and well-fertilized.  \nAnswer: a) Kael believes the soil is rich and well-fertilized."
    },
    {
        "continuation": {
            "0-138": "ToM",
            "139-215": "ToM",
            "217-280": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 90,
        "prompt_tokens": 784,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Farhan is a street food vendor in a bustling market in Istanbul. Farhan wants to prepare a delicious chicken shawarma wrap for a customer who asked for extra garlic sauce. Farhan takes a bottle of garlic sauce from the shelf and pours it onto the shawarma wrap. His assistant, who is busy preparing a different wrap, accidentally swaps the garlic sauce with a bottle of hot sauce, and Farhan unknowingly pours the hot sauce onto the shawarma wrap. Farhan finishes preparing the chicken shawarma wrap.\\nQuestion: Does Farhan believe the sauce he poured is garlic sauce or hot sauce?\\nChoose one of the following:\\na)Farhan believes the sauce he poured is garlic sauce.\\nb)Farhan believes the sauce he poured is hot sauce.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Farhan intended to use the garlic sauce and, not noticing the swap, takes what he thinks is the garlic sauce bottle and pours it. He doesn\u2019t realize it\u2019s hot sauce, so he believes he\u2019s pouring garlic sauce.\n\nAnswer: a) Farhan believes the sauce he poured is garlic sauce."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Farhan is a street food vendor in a bustling market in Istanbul. Farhan wants to prepare a delicious chicken shawarma wrap for a customer who asked for extra garlic sauce. Farhan takes a bottle of garlic sauce from the shelf and pours it onto the shawarma wrap. His assistant, who is busy preparing a different wrap, accidentally swaps the garlic sauce with a bottle of hot sauce, and Farhan unknowingly pours the hot sauce onto the shawarma wrap. Farhan finishes preparing the chicken shawarma wrap.\nQuestion: Does Farhan believe the sauce he poured is garlic sauce or hot sauce?\nChoose one of the following:\na)Farhan believes the sauce he poured is garlic sauce.\nb)Farhan believes the sauce he poured is hot sauce.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Farhan intended to use the garlic sauce and, not noticing the swap, takes what he thinks is the garlic sauce bottle and pours it. He doesn\u2019t realize it\u2019s hot sauce, so he believes he\u2019s pouring garlic sauce.\n\nAnswer: a) Farhan believes the sauce he poured is garlic sauce."
    },
    {
        "continuation": {
            "0-49": "DMN",
            "50-122": "ToM",
            "123-201": "ToM",
            "203-254": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 94,
        "prompt_tokens": 755,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Abhay is a young farmer in a small Indian village. He wants to water his crops with the water stored in a large clay pot. Abhay checks the water level and sees that the pot is full. Unbeknownst to him, a hole in the bottom of the pot has caused the water to leak out, leaving the pot empty. Abhay starts watering his crops with the water from the pot.\\nQuestion: Does Abhay believe the pot is full of water or empty?\\nChoose one of the following:\\na)Abhay believes the pot is empty.\\nb)Abhay believes the pot is full of water.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Abhay looked at the pot and saw it full. He does not know about the hidden hole or that the water has leaked out. Therefore, based on his knowledge, he believes the pot is still full of water.\n\nAnswer: b) Abhay believes the pot is full of water."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Abhay is a young farmer in a small Indian village. He wants to water his crops with the water stored in a large clay pot. Abhay checks the water level and sees that the pot is full. Unbeknownst to him, a hole in the bottom of the pot has caused the water to leak out, leaving the pot empty. Abhay starts watering his crops with the water from the pot.\nQuestion: Does Abhay believe the pot is full of water or empty?\nChoose one of the following:\na)Abhay believes the pot is empty.\nb)Abhay believes the pot is full of water.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Abhay looked at the pot and saw it full. He does not know about the hidden hole or that the water has leaked out. Therefore, based on his knowledge, he believes the pot is still full of water.\n\nAnswer: b) Abhay believes the pot is full of water."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-101": "DMN",
            "102-211": "ToM",
            "214-270": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 84,
        "prompt_tokens": 748,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Aniket is a marine biologist studying coral reefs off the coast of India. Aniket needs to collect samples of coral to analyze the effects of climate change on the reef. Aniket spots a healthy-looking coral formation in a specific area of the reef. A sudden wave surge stirs up sediment, covering the once healthy coral formation and causing it to become damaged. Aniket collects samples from the healthy coral formation he found.\\nQuestion: Does Aniket believe the coral formation is healthy or damaged?\\nChoose one of the following:\\na)Aniket believes the coral formation is damaged.\\nb)Aniket believes the coral formation is healthy.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Aniket initially spots and collects samples from a coral formation he identifies as healthy. Despite the subsequent damage by sediment, his belief at the time of collection is that the coral is healthy.  \nAnswer:b)Aniket believes the coral formation is healthy."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Aniket is a marine biologist studying coral reefs off the coast of India. Aniket needs to collect samples of coral to analyze the effects of climate change on the reef. Aniket spots a healthy-looking coral formation in a specific area of the reef. A sudden wave surge stirs up sediment, covering the once healthy coral formation and causing it to become damaged. Aniket collects samples from the healthy coral formation he found.\nQuestion: Does Aniket believe the coral formation is healthy or damaged?\nChoose one of the following:\na)Aniket believes the coral formation is damaged.\nb)Aniket believes the coral formation is healthy.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Aniket initially spots and collects samples from a coral formation he identifies as healthy. Despite the subsequent damage by sediment, his belief at the time of collection is that the coral is healthy.  \nAnswer:b)Aniket believes the coral formation is healthy."
    },
    {
        "continuation": {
            "0-88": "DMN",
            "89-228": "ToM",
            "230-240": "LN",
            "241-284": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 97,
        "prompt_tokens": 768,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Lila is a beekeeper in a small village in Eastern Europe. She wants to harvest honey from one of her beehives. The beehive is full of honey, as Lila observed during her last inspection. A sudden cold snap occurs overnight, causing the bees to consume the honey to stay warm and survive, leaving the beehive nearly empty. Lila harvests the honey from the beehive.\\nQuestion: Does Lila believe the beehive is full of honey or nearly empty?\\nChoose one of the following:\\na)Lila believes the beehive is full of honey.\\nb)Lila believes the beehive is nearly empty.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Lila last inspected the hive before the cold snap and saw it was full of honey. She has not inspected it since the bees ate the honey overnight, so she still believes it\u2019s full rather than knowing it\u2019s now nearly empty.\n\nAnswer: a) Lila believes the beehive is full of honey."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Lila is a beekeeper in a small village in Eastern Europe. She wants to harvest honey from one of her beehives. The beehive is full of honey, as Lila observed during her last inspection. A sudden cold snap occurs overnight, causing the bees to consume the honey to stay warm and survive, leaving the beehive nearly empty. Lila harvests the honey from the beehive.\nQuestion: Does Lila believe the beehive is full of honey or nearly empty?\nChoose one of the following:\na)Lila believes the beehive is full of honey.\nb)Lila believes the beehive is nearly empty.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Lila last inspected the hive before the cold snap and saw it was full of honey. She has not inspected it since the bees ate the honey overnight, so she still believes it\u2019s full rather than knowing it\u2019s now nearly empty.\n\nAnswer: a) Lila believes the beehive is full of honey."
    },
    {
        "continuation": {
            "0-131": "ToM",
            "134-203": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 65,
        "prompt_tokens": 756,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kavi is a chef at a popular Indian restaurant in a bustling city. Kavi wants to make his signature dish, a spicy curry, for the customers tonight. Kavi checks the pantry and sees a full bottle of his favorite hot sauce. When Kavi is away, a new chef accidentally knocks over the hot sauce, spilling its contents all over the floor. Kavi will start preparations for his signature spicy curry.\\nQuestion: Does Kavi believe there is a full bottle of hot sauce available or that the hot sauce has been spilled?\\nChoose one of the following:\\na)Kavi believes the hot sauce has been spilled.\\nb)Kavi believes there is a full bottle of hot sauce available.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kavi last saw the full bottle and was unaware of the spill, so he continues to believe the hot sauce is available in full.  \nAnswer:b)Kavi believes there is a full bottle of hot sauce available."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kavi is a chef at a popular Indian restaurant in a bustling city. Kavi wants to make his signature dish, a spicy curry, for the customers tonight. Kavi checks the pantry and sees a full bottle of his favorite hot sauce. When Kavi is away, a new chef accidentally knocks over the hot sauce, spilling its contents all over the floor. Kavi will start preparations for his signature spicy curry.\nQuestion: Does Kavi believe there is a full bottle of hot sauce available or that the hot sauce has been spilled?\nChoose one of the following:\na)Kavi believes the hot sauce has been spilled.\nb)Kavi believes there is a full bottle of hot sauce available.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kavi last saw the full bottle and was unaware of the spill, so he continues to believe the hot sauce is available in full.  \nAnswer:b)Kavi believes there is a full bottle of hot sauce available."
    },
    {
        "continuation": {
            "0-174": "ToM",
            "177-239": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 68,
        "prompt_tokens": 744,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Hiroshi is a fisherman in a small coastal village in Japan. Hiroshi wants to catch some fish to provide dinner for his family. Hiroshi checks the weather forecast and sees that it will be sunny throughout the day. Suddenly, a heavy rainstorm moves in, darkening the sky and causing strong waves. Hiroshi prepares his fishing gear and heads out to sea.\\nQuestion: Does Hiroshi believe the weather will be sunny or does he expect a rainstorm?\\nChoose one of the following:\\na)Hiroshi believes there will be a rainstorm.\\nb)Hiroshi believes the weather will be sunny all day.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Hiroshi checks the weather forecast and sees that it will be sunny throughout the day, so he believes in sunny weather despite the unexpected rainstorm that arrives.  \nAnswer: b) Hiroshi believes the weather will be sunny all day."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Hiroshi is a fisherman in a small coastal village in Japan. Hiroshi wants to catch some fish to provide dinner for his family. Hiroshi checks the weather forecast and sees that it will be sunny throughout the day. Suddenly, a heavy rainstorm moves in, darkening the sky and causing strong waves. Hiroshi prepares his fishing gear and heads out to sea.\nQuestion: Does Hiroshi believe the weather will be sunny or does he expect a rainstorm?\nChoose one of the following:\na)Hiroshi believes there will be a rainstorm.\nb)Hiroshi believes the weather will be sunny all day.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Hiroshi checks the weather forecast and sees that it will be sunny throughout the day, so he believes in sunny weather despite the unexpected rainstorm that arrives.  \nAnswer: b) Hiroshi believes the weather will be sunny all day."
    },
    {
        "continuation": {
            "0-105": "DMN",
            "106-164": "ToM",
            "165-268": "ToM",
            "271-330": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 100,
        "prompt_tokens": 764,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Juan is a young chef in a small Mexican town. He wants to make a delicious salsa for the upcoming town festival. Juan buys fresh tomatoes from the market, which he believes are ripe and perfect for the salsa. While Juan is away from his kitchen, a mischievous cat knocks over the bag of tomatoes, crushing them and making them unsuitable for the salsa. Juan starts making the salsa with the tomatoes he bought.\\nQuestion: Does Juan believe the tomatoes are ripe and perfect or crushed and unsuitable?\\nChoose one of the following:\\na)Juan believes the tomatoes are ripe and perfect.\\nb)Juan believes the tomatoes are crushed and unsuitable.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Juan buys fresh tomatoes which he believes are ripe and perfect for salsa. He is unaware that the cat crushed them while he was away. Therefore, Juan\u2019s belief remains that they are ripe and perfect, even though they are actually damaged.  \nAnswer: a) Juan believes the tomatoes are ripe and perfect."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Juan is a young chef in a small Mexican town. He wants to make a delicious salsa for the upcoming town festival. Juan buys fresh tomatoes from the market, which he believes are ripe and perfect for the salsa. While Juan is away from his kitchen, a mischievous cat knocks over the bag of tomatoes, crushing them and making them unsuitable for the salsa. Juan starts making the salsa with the tomatoes he bought.\nQuestion: Does Juan believe the tomatoes are ripe and perfect or crushed and unsuitable?\nChoose one of the following:\na)Juan believes the tomatoes are ripe and perfect.\nb)Juan believes the tomatoes are crushed and unsuitable.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Juan buys fresh tomatoes which he believes are ripe and perfect for salsa. He is unaware that the cat crushed them while he was away. Therefore, Juan\u2019s belief remains that they are ripe and perfect, even though they are actually damaged.  \nAnswer: a) Juan believes the tomatoes are ripe and perfect."
    },
    {
        "continuation": {
            "0-129": "ToM",
            "132-207": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 69,
        "prompt_tokens": 762,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Juanita is a skilled chef who owns a small restaurant in Mexico. Today, she wants to prepare her signature dish, chiles en nogada, for a group of tourists. Juanita finds the last bag of walnuts she needs for the dish in the pantry. While Juanita is busy cutting vegetables, a mouse sneaks into the pantry and nibbles on the walnuts, contaminating them. Juanita proceeds to prepare the dish using the walnuts.\\nQuestion: Does Juanita believe the bag of walnuts is fresh and uncontaminated or contaminated?\\nChoose one of the following:\\na)Juanita believes the bag of walnuts is fresh and uncontaminated.\\nb)Juanita believes the bag of walnuts is contaminated.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Juanita never sees the mouse or knows the nuts were nibbled on, so she assumes the walnuts are fresh and uncontaminated.  \nAnswer: a) Juanita believes the bag of walnuts is fresh and uncontaminated."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Juanita is a skilled chef who owns a small restaurant in Mexico. Today, she wants to prepare her signature dish, chiles en nogada, for a group of tourists. Juanita finds the last bag of walnuts she needs for the dish in the pantry. While Juanita is busy cutting vegetables, a mouse sneaks into the pantry and nibbles on the walnuts, contaminating them. Juanita proceeds to prepare the dish using the walnuts.\nQuestion: Does Juanita believe the bag of walnuts is fresh and uncontaminated or contaminated?\nChoose one of the following:\na)Juanita believes the bag of walnuts is fresh and uncontaminated.\nb)Juanita believes the bag of walnuts is contaminated.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Juanita never sees the mouse or knows the nuts were nibbled on, so she assumes the walnuts are fresh and uncontaminated.  \nAnswer: a) Juanita believes the bag of walnuts is fresh and uncontaminated."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-52": "DMN",
            "53-130": "ToM",
            "131-192": "ToM",
            "195-256": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 92,
        "prompt_tokens": 745,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Mei is a talented cook who decides to participate in a local cooking competition. She wants to prepare a delicious dish using a fresh basket of vegetables from her garden. Mei sees that the vegetables are freshly picked and vibrant. Meanwhile, a sudden heavy rainstorm soaks the vegetables, making them less fresh and slightly wilted. Mei starts preparing the dish using the fresh vegetables.\\nQuestion: Does Mei believe the vegetables are fresh and vibrant or wilted and less fresh?\\nChoose one of the following:\\na)Mei believes the vegetables are wilted and less fresh.\\nb)Mei believes the vegetables are fresh and vibrant.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Although a rainstorm wilted the vegetables, Mei proceeds as if they are fresh and vibrant by starting her dish with them. This indicates she believes they are still fresh and vibrant.  \nAnswer: b) Mei believes the vegetables are fresh and vibrant."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Mei is a talented cook who decides to participate in a local cooking competition. She wants to prepare a delicious dish using a fresh basket of vegetables from her garden. Mei sees that the vegetables are freshly picked and vibrant. Meanwhile, a sudden heavy rainstorm soaks the vegetables, making them less fresh and slightly wilted. Mei starts preparing the dish using the fresh vegetables.\nQuestion: Does Mei believe the vegetables are fresh and vibrant or wilted and less fresh?\nChoose one of the following:\na)Mei believes the vegetables are wilted and less fresh.\nb)Mei believes the vegetables are fresh and vibrant.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Although a rainstorm wilted the vegetables, Mei proceeds as if they are fresh and vibrant by starting her dish with them. This indicates she believes they are still fresh and vibrant.  \nAnswer: b) Mei believes the vegetables are fresh and vibrant."
    },
    {
        "continuation": {
            "0-172": "ToM",
            "174-230": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 69,
        "prompt_tokens": 731,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Cheng is a teacher in a rural Chinese village. Cheng wants to use a projector to show a documentary about Chinese history to his students. The projector is plugged in and turned on, as Cheng has prepared it beforehand. As the wind blows outside, the power goes out, causing the projector to turn off. Cheng starts playing the documentary using the projector.\\nQuestion: Does Cheng believe the projector is ready to use or turned off?\\nChoose one of the following:\\na)Cheng believes the projector is turned off.\\nb)Cheng believes the projector is ready to use.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Cheng goes to start playing the documentary with the projector, so he must believe it is on and ready to use, despite the power outage that actually turned it off.\n\nAnswer: b) Cheng believes the projector is ready to use."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Cheng is a teacher in a rural Chinese village. Cheng wants to use a projector to show a documentary about Chinese history to his students. The projector is plugged in and turned on, as Cheng has prepared it beforehand. As the wind blows outside, the power goes out, causing the projector to turn off. Cheng starts playing the documentary using the projector.\nQuestion: Does Cheng believe the projector is ready to use or turned off?\nChoose one of the following:\na)Cheng believes the projector is turned off.\nb)Cheng believes the projector is ready to use.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Cheng goes to start playing the documentary with the projector, so he must believe it is on and ready to use, despite the power outage that actually turned it off.\n\nAnswer: b) Cheng believes the projector is ready to use."
    },
    {
        "continuation": {
            "0-78": "ToM",
            "79-127": "ToM",
            "128-184": "ToM",
            "187-272": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 90,
        "prompt_tokens": 794,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Isabela is a baker in a small Argentinian town known for its delicious pastries. Isabela wants to make a batch of alfajores, a traditional Argentinian dessert, for her bakery's customers. Isabela gets a bag of powdered sugar from her pantry to dust the alfajores. Her assistant, who was organizing the pantry earlier, accidentally placed a bag of flour in the powdered sugar's spot, and Isabela unknowingly grabs the bag of flour instead. Isabela dusts the alfajores with what she believes is powdered sugar.\\nQuestion: Does Isabela believe the bag she took from the pantry contains powdered sugar or flour?\\nChoose one of the following:\\na)Isabela believes the bag she took from the pantry contains powdered sugar.\\nb)Isabela believes the bag she took from the pantry contains flour.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Isabela thinks she\u2019s grabbing the powdered sugar from its usual spot; she doesn\u2019t know her assistant swapped the bags, so she believes the bag she picked up is powdered sugar.  \nAnswer: a) Isabela believes the bag she took from the pantry contains powdered sugar."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Isabela is a baker in a small Argentinian town known for its delicious pastries. Isabela wants to make a batch of alfajores, a traditional Argentinian dessert, for her bakery's customers. Isabela gets a bag of powdered sugar from her pantry to dust the alfajores. Her assistant, who was organizing the pantry earlier, accidentally placed a bag of flour in the powdered sugar's spot, and Isabela unknowingly grabs the bag of flour instead. Isabela dusts the alfajores with what she believes is powdered sugar.\nQuestion: Does Isabela believe the bag she took from the pantry contains powdered sugar or flour?\nChoose one of the following:\na)Isabela believes the bag she took from the pantry contains powdered sugar.\nb)Isabela believes the bag she took from the pantry contains flour.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Isabela thinks she\u2019s grabbing the powdered sugar from its usual spot; she doesn\u2019t know her assistant swapped the bags, so she believes the bag she picked up is powdered sugar.  \nAnswer: a) Isabela believes the bag she took from the pantry contains powdered sugar."
    },
    {
        "continuation": {
            "0-103": "DMN",
            "104-220": "ToM",
            "221-277": "ToM",
            "280-289": "LN",
            "290-359": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 108,
        "prompt_tokens": 788,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Emeka is a farmer in a small village in Nigeria. Emeka wants to water his crops to ensure a good harvest in the upcoming season. Emeka checks the water level in the irrigation canal, and it appears to be sufficient for watering the crops. Overnight, a heavy rainfall causes flooding in the area, and the water level in the irrigation canal rises significantly, becoming too high for safe irrigation. Emeka irrigates his crops with the water from the canal.\\nQuestion: Does Emeka believe the water level in the irrigation canal is sufficient or too high?\\nChoose one of the following:\\na)Emeka believes the water level in the irrigation canal is too high.\\nb)Emeka believes the water level in the irrigation canal is sufficient.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that when Emeka checks the canal, it appears to be sufficient for irrigation. Although rainfall later raises the level too high, there's no indication he reassessed his belief before irrigating. Therefore, Emeka believes the water level is sufficient.  \nAnswer:b) Emeka believes the water level in the irrigation canal is sufficient."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Emeka is a farmer in a small village in Nigeria. Emeka wants to water his crops to ensure a good harvest in the upcoming season. Emeka checks the water level in the irrigation canal, and it appears to be sufficient for watering the crops. Overnight, a heavy rainfall causes flooding in the area, and the water level in the irrigation canal rises significantly, becoming too high for safe irrigation. Emeka irrigates his crops with the water from the canal.\nQuestion: Does Emeka believe the water level in the irrigation canal is sufficient or too high?\nChoose one of the following:\na)Emeka believes the water level in the irrigation canal is too high.\nb)Emeka believes the water level in the irrigation canal is sufficient.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that when Emeka checks the canal, it appears to be sufficient for irrigation. Although rainfall later raises the level too high, there's no indication he reassessed his belief before irrigating. Therefore, Emeka believes the water level is sufficient.  \nAnswer:b) Emeka believes the water level in the irrigation canal is sufficient."
    },
    {
        "continuation": {
            "0-103": "ToM",
            "104-146": "ToM",
            "149-201": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 70,
        "prompt_tokens": 750,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Amaury is a baker in a small village in France. Amaury wants to make pain aux raisins, a pastry filled with raisins and custard, for the customers visiting his bakery. Amaury takes a jar of raisins from the pantry to add to the pastry dough. His assistant, while tidying the pantry, accidentally replaces the raisin jar with a jar containing sultanas without realizing the difference. Amaury adds the contents of the jar to the pastry dough.\\nQuestion: Does Amaury believe the jar contains raisins or sultanas?\\nChoose one of the following:\\na)Amaury believes the jar contains raisins.\\nb)Amaury believes the jar contains sultanas.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Amaury picked what he believed was the raisin jar and is unaware it was swapped with sultanas. Therefore he believes it contains raisins.  \nAnswer: a) Amaury believes the jar contains raisins."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Amaury is a baker in a small village in France. Amaury wants to make pain aux raisins, a pastry filled with raisins and custard, for the customers visiting his bakery. Amaury takes a jar of raisins from the pantry to add to the pastry dough. His assistant, while tidying the pantry, accidentally replaces the raisin jar with a jar containing sultanas without realizing the difference. Amaury adds the contents of the jar to the pastry dough.\nQuestion: Does Amaury believe the jar contains raisins or sultanas?\nChoose one of the following:\na)Amaury believes the jar contains raisins.\nb)Amaury believes the jar contains sultanas.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Amaury picked what he believed was the raisin jar and is unaware it was swapped with sultanas. Therefore he believes it contains raisins.  \nAnswer: a) Amaury believes the jar contains raisins."
    },
    {
        "continuation": {
            "0-99": "DMN",
            "100-219": "ToM",
            "222-273": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 87,
        "prompt_tokens": 737,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Chi is a fisherman living on a small island in the South Pacific. Chi wants to go on a fishing trip to catch some fish for his family's dinner. The sea appears calm and inviting. Suddenly, a strong storm rolls in, causing the sea to become rough and dangerous. Chi embarks on his fishing trip.\\nQuestion: Does Chi believe the sea is calm and inviting or rough and dangerous?\\nChoose one of the following:\\na)Chi believes the sea is calm and inviting.\\nb)Chi believes the sea is rough and dangerous.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: At the start of Chi\u2019s fishing trip, he sets out because the sea appears calm and inviting. Although a storm later makes it rough and dangerous, Chi\u2019s belief when he embarks is that the sea is calm and inviting.  \nAnswer:a)Chi believes the sea is calm and inviting."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Chi is a fisherman living on a small island in the South Pacific. Chi wants to go on a fishing trip to catch some fish for his family's dinner. The sea appears calm and inviting. Suddenly, a strong storm rolls in, causing the sea to become rough and dangerous. Chi embarks on his fishing trip.\nQuestion: Does Chi believe the sea is calm and inviting or rough and dangerous?\nChoose one of the following:\na)Chi believes the sea is calm and inviting.\nb)Chi believes the sea is rough and dangerous.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: At the start of Chi\u2019s fishing trip, he sets out because the sea appears calm and inviting. Although a storm later makes it rough and dangerous, Chi\u2019s belief when he embarks is that the sea is calm and inviting.  \nAnswer:a)Chi believes the sea is calm and inviting."
    },
    {
        "continuation": {
            "0-122": "ToM",
            "125-189": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 62,
        "prompt_tokens": 743,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Mei is a pearl diver in a small coastal village in Japan. Mei wants to find a perfect pearl to give to her grandmother for her birthday. Mei spots an oyster at the bottom of the sea that looks to be the right size and age to contain a pearl. A curious octopus opens the oyster, revealing that there is no pearl inside, and then swims away. Mei dives down to collect the oyster.\\nQuestion: Does Mei believe the oyster she spotted contains a pearl or that it is empty?\\nChoose one of the following:\\na)Mei believes the oyster she spotted is empty.\\nb)Mei believes the oyster she spotted contains a pearl.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Mei spots the oyster because it looks the right size and age to contain a pearl, so she expects it holds a pearl.  \nAnswer: b) MeI believes the oyster she spotted contains a pearl."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Mei is a pearl diver in a small coastal village in Japan. Mei wants to find a perfect pearl to give to her grandmother for her birthday. Mei spots an oyster at the bottom of the sea that looks to be the right size and age to contain a pearl. A curious octopus opens the oyster, revealing that there is no pearl inside, and then swims away. Mei dives down to collect the oyster.\nQuestion: Does Mei believe the oyster she spotted contains a pearl or that it is empty?\nChoose one of the following:\na)Mei believes the oyster she spotted is empty.\nb)Mei believes the oyster she spotted contains a pearl.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Mei spots the oyster because it looks the right size and age to contain a pearl, so she expects it holds a pearl.  \nAnswer: b) MeI believes the oyster she spotted contains a pearl."
    },
    {
        "continuation": {
            "0-155": "ToM",
            "158-223": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 65,
        "prompt_tokens": 755,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Mei is a pearl diver in a small coastal town in Japan. Mei wants to dive deep into the sea to find a rare and valuable black pearl for her grandmother's birthday gift. The water in the cove is calm with barely any waves. Suddenly, a storm rolls in, and the once calm water becomes rough and choppy. Mei dives into the calm water to search for the black pearl.\\nQuestion: Does Mei believe the water is calm and perfect for diving or rough and choppy due to the storm?\\nChoose one of the following:\\na)Mei believes the water is calm and perfect for diving.\\nb)Mei believes the water is rough and choppy due to the storm.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The passage states that Mei dives into calm water to search for the black pearl, indicating she believes the water is calm and perfect for diving.  \nAnswer: a) Mei believes the water is calm and perfect for diving."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Mei is a pearl diver in a small coastal town in Japan. Mei wants to dive deep into the sea to find a rare and valuable black pearl for her grandmother's birthday gift. The water in the cove is calm with barely any waves. Suddenly, a storm rolls in, and the once calm water becomes rough and choppy. Mei dives into the calm water to search for the black pearl.\nQuestion: Does Mei believe the water is calm and perfect for diving or rough and choppy due to the storm?\nChoose one of the following:\na)Mei believes the water is calm and perfect for diving.\nb)Mei believes the water is rough and choppy due to the storm.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The passage states that Mei dives into calm water to search for the black pearl, indicating she believes the water is calm and perfect for diving.  \nAnswer: a) Mei believes the water is calm and perfect for diving."
    },
    {
        "continuation": {
            "0-180": "ToM",
            "183-241": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 70,
        "prompt_tokens": 757,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Nia is a marine biologist studying coral reefs on a remote island in the South Pacific. Nia wants to collect samples of healthy coral to study their adaptability to changing ocean temperatures. The coral she observes appears to be vibrant and teeming with life. However, a sudden underwater landslide nearby stirs up sediment, which settles on the coral and causes it to become stressed and bleached. Nia collects samples of the healthy coral.\\nQuestion: Does Nia believe the coral is healthy and thriving or stressed and bleached?\\nChoose one of the following:\\na)Nia believes the coral is healthy and thriving.\\nb)Nia believes the coral is stressed and bleached.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Nia set out to collect samples of healthy coral and observes that the coral she takes is vibrant and teeming with life, indicating she believes it is healthy and thriving.  \nAnswer: a) Nia believes the coral is healthy and thriving."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Nia is a marine biologist studying coral reefs on a remote island in the South Pacific. Nia wants to collect samples of healthy coral to study their adaptability to changing ocean temperatures. The coral she observes appears to be vibrant and teeming with life. However, a sudden underwater landslide nearby stirs up sediment, which settles on the coral and causes it to become stressed and bleached. Nia collects samples of the healthy coral.\nQuestion: Does Nia believe the coral is healthy and thriving or stressed and bleached?\nChoose one of the following:\na)Nia believes the coral is healthy and thriving.\nb)Nia believes the coral is stressed and bleached.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Nia set out to collect samples of healthy coral and observes that the coral she takes is vibrant and teeming with life, indicating she believes it is healthy and thriving.  \nAnswer: a) Nia believes the coral is healthy and thriving."
    },
    {
        "continuation": {
            "0-142": "ToM",
            "143-180": "ToM",
            "183-265": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 86,
        "prompt_tokens": 776,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Amina is an artist in a small town in Morocco, working in her home studio. She wants to complete her latest painting by adding a few final brushstrokes. Amina mixed a specific shade of blue paint before taking a short break. While she is away, her mischievous cat jumps onto the table, knocking over the paint, which spills and dries on the floor. Amina uses the blue paint to finish her painting.\\nQuestion: Does Amina believe that her mixed shade of blue paint is still fresh or spilled and dried?\\nChoose one of the following:\\na)Amina believes the shade of blue paint has spilled and dried.\\nb)Amina believes the shade of blue paint is still fresh and ready to use.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Amina mixed the paint, took a break without noticing the spill, then resumed painting using what she assumes is her fresh blue paint. She isn\u2019t aware it spilled and dried.  \nAnswer: b) Amina believes the shade of blue paint is still fresh and ready to use."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Amina is an artist in a small town in Morocco, working in her home studio. She wants to complete her latest painting by adding a few final brushstrokes. Amina mixed a specific shade of blue paint before taking a short break. While she is away, her mischievous cat jumps onto the table, knocking over the paint, which spills and dries on the floor. Amina uses the blue paint to finish her painting.\nQuestion: Does Amina believe that her mixed shade of blue paint is still fresh or spilled and dried?\nChoose one of the following:\na)Amina believes the shade of blue paint has spilled and dried.\nb)Amina believes the shade of blue paint is still fresh and ready to use.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Amina mixed the paint, took a break without noticing the spill, then resumed painting using what she assumes is her fresh blue paint. She isn\u2019t aware it spilled and dried.  \nAnswer: b) Amina believes the shade of blue paint is still fresh and ready to use."
    },
    {
        "continuation": {
            "0-125": "ToM",
            "126-167": "ToM",
            "170-221": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 77,
        "prompt_tokens": 738,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Fatima is a potter in a small Moroccan village known for its vibrant ceramics. Fatima wants to create a beautiful vase as a wedding gift for her cousin. Fatima has left her kiln turned off overnight to cool down. A power surge during the night causes the kiln to turn on unexpectedly, making it dangerously hot. Fatima begins working on the vase.\\nQuestion: Does Fatima believe her kiln is off and cool or unexpectedly hot?\\nChoose one of the following:\\na)Fatima believes her kiln is unexpectedly hot.\\nb)Fatima believes her kiln is off and cool.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Fatima turned her kiln off overnight expecting it to cool, and she\u2019s unaware of the power surge that\u2019s turned it on. Therefore she believes it\u2019s off and cool.  \nAnswer:b) Fatima believes her kiln is off and cool."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Fatima is a potter in a small Moroccan village known for its vibrant ceramics. Fatima wants to create a beautiful vase as a wedding gift for her cousin. Fatima has left her kiln turned off overnight to cool down. A power surge during the night causes the kiln to turn on unexpectedly, making it dangerously hot. Fatima begins working on the vase.\nQuestion: Does Fatima believe her kiln is off and cool or unexpectedly hot?\nChoose one of the following:\na)Fatima believes her kiln is unexpectedly hot.\nb)Fatima believes her kiln is off and cool.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Fatima turned her kiln off overnight expecting it to cool, and she\u2019s unaware of the power surge that\u2019s turned it on. Therefore she believes it\u2019s off and cool.  \nAnswer:b) Fatima believes her kiln is off and cool."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-187": "DMN",
            "188-234": "ToM",
            "236-319": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 100,
        "prompt_tokens": 758,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Carlos is a farmer in a small village in Mexico. Carlos wants to harvest his corn crop before the rainy season begins. The corn plants are tall, strong, and ready for harvest in his field. A sudden and unexpected hailstorm strikes, severely damaging the corn plants. Carlos prepares to harvest the ripe corn crop.\\nQuestion: Does Carlos believe the corn plants are tall, strong, and ready for harvest, or does he believe they are severely damaged?\\nChoose one of the following:\\na)Carlos believes the corn plants are severely damaged.\\nb)Carlos believes the corn plants are tall, strong, and ready for harvest.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The passage states that before the hailstorm the corn plants are tall, strong, and ready for harvest, and despite the hail damage, Carlos still prepares to harvest the ripe crop, indicating he believes they are harvest-ready.\n\nAnswer: b) Carlos believes the corn plants are tall, strong, and ready for harvest."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Carlos is a farmer in a small village in Mexico. Carlos wants to harvest his corn crop before the rainy season begins. The corn plants are tall, strong, and ready for harvest in his field. A sudden and unexpected hailstorm strikes, severely damaging the corn plants. Carlos prepares to harvest the ripe corn crop.\nQuestion: Does Carlos believe the corn plants are tall, strong, and ready for harvest, or does he believe they are severely damaged?\nChoose one of the following:\na)Carlos believes the corn plants are severely damaged.\nb)Carlos believes the corn plants are tall, strong, and ready for harvest.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The passage states that before the hailstorm the corn plants are tall, strong, and ready for harvest, and despite the hail damage, Carlos still prepares to harvest the ripe crop, indicating he believes they are harvest-ready.\n\nAnswer: b) Carlos believes the corn plants are tall, strong, and ready for harvest."
    },
    {
        "continuation": {
            "0-110": "DMN",
            "111-227": "DMN",
            "228-319": "ToM",
            "321-398": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 112,
        "prompt_tokens": 780,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Miguel is a chef at a popular restaurant in Mexico City. Miguel wants to prepare a delicious and authentic mole sauce for the evening's special dish. Miguel notices that the chocolate he has in the pantry is of high quality and perfect for the sauce. A sudden power outage causes the temperature in the pantry to rise, melting the chocolate. Miguel uses the high-quality chocolate from the pantry to make the mole sauce.\\nQuestion: Does Miguel believe the chocolate is perfect for making the mole sauce or that the chocolate has melted?\\nChoose one of the following:\\na)Miguel believes the chocolate has melted.\\nb)Miguel believes the chocolate is perfect for making the mole sauce.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Miguel notices the chocolate is of high quality and perfect for the mole sauce. Although the chocolate melts due to a power outage, Miguel still uses that high-quality chocolate to make his sauce. The belief he holds is that the chocolate is perfect for the sauce, not that it has melted.\n\nAnswer:b) Miguel believes the chocolate is perfect for making the mole sauce."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Miguel is a chef at a popular restaurant in Mexico City. Miguel wants to prepare a delicious and authentic mole sauce for the evening's special dish. Miguel notices that the chocolate he has in the pantry is of high quality and perfect for the sauce. A sudden power outage causes the temperature in the pantry to rise, melting the chocolate. Miguel uses the high-quality chocolate from the pantry to make the mole sauce.\nQuestion: Does Miguel believe the chocolate is perfect for making the mole sauce or that the chocolate has melted?\nChoose one of the following:\na)Miguel believes the chocolate has melted.\nb)Miguel believes the chocolate is perfect for making the mole sauce.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Miguel notices the chocolate is of high quality and perfect for the mole sauce. Although the chocolate melts due to a power outage, Miguel still uses that high-quality chocolate to make his sauce. The belief he holds is that the chocolate is perfect for the sauce, not that it has melted.\n\nAnswer:b) Miguel believes the chocolate is perfect for making the mole sauce."
    },
    {
        "continuation": {
            "0-61": "DMN",
            "62-150": "ToM",
            "153-203": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 69,
        "prompt_tokens": 729,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Carlos is a farmer in a small village in the Andes. Carlos wants to collect fresh eggs from his chickens to sell at the local market. The nest appears to be full of eggs when Carlos checks it in the morning. A crafty fox sneaks into the chicken coop and steals all the eggs from the nest. Carlos prepares to collect the eggs to sell at the market.\\nQuestion: Does Carlos believe the nest contains eggs or that the fox has stolen them?\\nChoose one of the following:\\na)Carlos believes the fox has stolen the eggs.\\nb)Carlos believes the nest contains eggs.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Carlos checked the nest in the morning and saw eggs. The fox\u2019s theft happened without his knowledge, so he still believes the eggs are there.  \nAnswer: b) Carlos believes the nest contains eggs."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Carlos is a farmer in a small village in the Andes. Carlos wants to collect fresh eggs from his chickens to sell at the local market. The nest appears to be full of eggs when Carlos checks it in the morning. A crafty fox sneaks into the chicken coop and steals all the eggs from the nest. Carlos prepares to collect the eggs to sell at the market.\nQuestion: Does Carlos believe the nest contains eggs or that the fox has stolen them?\nChoose one of the following:\na)Carlos believes the fox has stolen the eggs.\nb)Carlos believes the nest contains eggs.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Carlos checked the nest in the morning and saw eggs. The fox\u2019s theft happened without his knowledge, so he still believes the eggs are there.  \nAnswer: b) Carlos believes the nest contains eggs."
    },
    {
        "continuation": {
            "0-79": "DMN",
            "80-229": "ToM",
            "231-298": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 87,
        "prompt_tokens": 759,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Carlos is a skilled chef working at a renowned restaurant in Mexico City. Carlos wants to perfect his mole sauce recipe for a special charity event. He tastes the mole sauce and finds it to be rich and balanced in flavors. A few hours before the event, a kitchen assistant accidentally adds too much chili powder to the mole sauce, making it extremely spicy. Carlos confidently serves the mole sauce at the charity event.\\nQuestion: Does Carlos believe the mole sauce has a perfect taste or that it is extremely spicy?\\nChoose one of the following:\\na)Carlos believes the mole sauce is extremely spicy.\\nb)Carlos believes that the mole sauce has the perfect taste.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Carlos initially tastes the mole sauce and finds it rich and balanced. He does not re\u2010taste it after the assistant\u2019s mistake, yet he remains confident in serving it, indicating he still believes it has the perfect taste.\n\nAnswer:b)Carlos believes that the mole sauce has the perfect taste."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Carlos is a skilled chef working at a renowned restaurant in Mexico City. Carlos wants to perfect his mole sauce recipe for a special charity event. He tastes the mole sauce and finds it to be rich and balanced in flavors. A few hours before the event, a kitchen assistant accidentally adds too much chili powder to the mole sauce, making it extremely spicy. Carlos confidently serves the mole sauce at the charity event.\nQuestion: Does Carlos believe the mole sauce has a perfect taste or that it is extremely spicy?\nChoose one of the following:\na)Carlos believes the mole sauce is extremely spicy.\nb)Carlos believes that the mole sauce has the perfect taste.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Carlos initially tastes the mole sauce and finds it rich and balanced. He does not re\u2010taste it after the assistant\u2019s mistake, yet he remains confident in serving it, indicating he still believes it has the perfect taste.\n\nAnswer:b)Carlos believes that the mole sauce has the perfect taste."
    },
    {
        "continuation": {
            "0-122": "ToM",
            "123-189": "ToM",
            "192-266": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 80,
        "prompt_tokens": 761,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Fatima is a talented artist living in a small Moroccan village. Fatima wants to create a beautiful painting to donate for the local charity auction. She has a brand-new set of high-quality paintbrushes in her studio. While she is away, her younger brother accidentally ruins the paintbrushes by using them for a school project with glue. Fatima starts working on her painting with the paintbrushes.\\nQuestion: Does Fatima believe she has high-quality paintbrushes or ruined paintbrushes in her studio?\\nChoose one of the following:\\na)Fatima believes the paintbrushes are ruined.\\nb)Fatima believes she has high-quality paintbrushes in her studio.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Fatima originally bought a brand-new set of high-quality paintbrushes and is unaware her brother has ruined them. She starts painting believing they are still in perfect condition.  \nAnswer:b) Fatima believes she has high-quality paintbrushes in her studio."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Fatima is a talented artist living in a small Moroccan village. Fatima wants to create a beautiful painting to donate for the local charity auction. She has a brand-new set of high-quality paintbrushes in her studio. While she is away, her younger brother accidentally ruins the paintbrushes by using them for a school project with glue. Fatima starts working on her painting with the paintbrushes.\nQuestion: Does Fatima believe she has high-quality paintbrushes or ruined paintbrushes in her studio?\nChoose one of the following:\na)Fatima believes the paintbrushes are ruined.\nb)Fatima believes she has high-quality paintbrushes in her studio.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Fatima originally bought a brand-new set of high-quality paintbrushes and is unaware her brother has ruined them. She starts painting believing they are still in perfect condition.  \nAnswer:b) Fatima believes she has high-quality paintbrushes in her studio."
    },
    {
        "continuation": {
            "0-94": "DMN",
            "95-218": "ToM",
            "221-300": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 93,
        "prompt_tokens": 781,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Aisha is a beekeeper in a small village near the Sahara Desert. Aisha wants to harvest honey from her beehives to sell at the local market. The beehives appear to be full of honey and ready for harvest. A sudden sandstorm blows through the area, filling the beehives with sand and making the honey difficult to extract. Aisha prepares to harvests the honey and sell it at the local market.\\nQuestion: Does Aisha believe the beehives are full of honey or filled with sand?\\nChoose one of the following:\\na)Aisha believes the beehives are filled with sand.\\nb)Aisha believes the beehives are full of honey and ready for harvest.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Aisha sees her beehives as full of honey and ready for harvest. Although a sandstorm fills them with sand, she still prepares to harvest honey, showing she believes there is honey inside.  \nAnswer: b) Aisha believes the beehives are full of honey and ready for harvest."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Aisha is a beekeeper in a small village near the Sahara Desert. Aisha wants to harvest honey from her beehives to sell at the local market. The beehives appear to be full of honey and ready for harvest. A sudden sandstorm blows through the area, filling the beehives with sand and making the honey difficult to extract. Aisha prepares to harvests the honey and sell it at the local market.\nQuestion: Does Aisha believe the beehives are full of honey or filled with sand?\nChoose one of the following:\na)Aisha believes the beehives are filled with sand.\nb)Aisha believes the beehives are full of honey and ready for harvest.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Aisha sees her beehives as full of honey and ready for harvest. Although a sandstorm fills them with sand, she still prepares to harvest honey, showing she believes there is honey inside.  \nAnswer: b) Aisha believes the beehives are full of honey and ready for harvest."
    },
    {
        "continuation": {
            "0-206": "ToM",
            "209-281": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 80,
        "prompt_tokens": 774,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Amara is a beekeeper in a small Kenyan village. She wants to collect honey from her beehives to sell at the local market. The honeycombs in the beehives appear full and ready for harvest. A sudden thunderstorm floods the area around her beehives, washing away some of the honeycombs and diluting the remaining honey. Amara harvests the honey from the honeycombs.\\nQuestion: Does Amara believe the honeycombs are full and ready for harvest or washed away and diluted?\\nChoose one of the following:\\na)Amara believes the honeycombs are washed away and diluted.\\nb)Amara believes the honeycombs are full and ready for harvest.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that initially the honeycombs \u201cappear full and ready for harvest,\u201d and Amara proceeds to collect honey, so she believes they are full and ready rather than washed away and diluted.  \nAnswer: b) Amara believes the honeycombs are full and ready for harvest."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Amara is a beekeeper in a small Kenyan village. She wants to collect honey from her beehives to sell at the local market. The honeycombs in the beehives appear full and ready for harvest. A sudden thunderstorm floods the area around her beehives, washing away some of the honeycombs and diluting the remaining honey. Amara harvests the honey from the honeycombs.\nQuestion: Does Amara believe the honeycombs are full and ready for harvest or washed away and diluted?\nChoose one of the following:\na)Amara believes the honeycombs are washed away and diluted.\nb)Amara believes the honeycombs are full and ready for harvest.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that initially the honeycombs \u201cappear full and ready for harvest,\u201d and Amara proceeds to collect honey, so she believes they are full and ready rather than washed away and diluted.  \nAnswer: b) Amara believes the honeycombs are full and ready for harvest."
    },
    {
        "continuation": {
            "0-117": "ToM",
            "120-196": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 59,
        "prompt_tokens": 748,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Juan is a gardener in a small village in Mexico, tending to a flower garden near the town plaza. Juan wants to water a specific flower bed to ensure its health and beauty. Juan sees that the garden hose is neatly coiled and ready for use. Unexpectedly, a group of children playing nearby accidentally puncture the hose with a sharp stick while running around. Juan uses the garden hose to water the flower bed.\\nQuestion: Does Juan believe the garden hose is in good condition and functional or punctured and non-functional?\\nChoose one of the following:\\na)Juan believes the garden hose is in good condition and functional.\\nb)Juan believes the garden hose is punctured and non-functional.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Juan uses the hose to water the flower bed, indicating he believes it works despite the accidental puncture.  \nAnswer: a)Juan believes the garden hose is in good condition and functional."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Juan is a gardener in a small village in Mexico, tending to a flower garden near the town plaza. Juan wants to water a specific flower bed to ensure its health and beauty. Juan sees that the garden hose is neatly coiled and ready for use. Unexpectedly, a group of children playing nearby accidentally puncture the hose with a sharp stick while running around. Juan uses the garden hose to water the flower bed.\nQuestion: Does Juan believe the garden hose is in good condition and functional or punctured and non-functional?\nChoose one of the following:\na)Juan believes the garden hose is in good condition and functional.\nb)Juan believes the garden hose is punctured and non-functional.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Juan uses the hose to water the flower bed, indicating he believes it works despite the accidental puncture.  \nAnswer: a)Juan believes the garden hose is in good condition and functional."
    },
    {
        "continuation": {
            "0-96": "ToM",
            "97-190": "ToM",
            "192-256": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 83,
        "prompt_tokens": 763,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Hiro is a bonsai enthusiast in a small village in Japan. He wants to trim his favorite bonsai tree to maintain its shape and health. Hiro sees that the tree's branches are overgrown and need pruning. A gust of wind blows through Hiro's garden, snapping off several of the overgrown branches from the bonsai tree. Hiro trims the overgrown branches from the bonsai tree.\\nQuestion: Does Hiro believe the bonsai tree has overgrown branches or that they have been snapped off by the wind?\\nChoose one of the following:\\na)Hiro believes the bonsai tree has overgrown branches.\\nb)Hiro believes the branches have been snapped off by the wind.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Hiro decided to trim because he saw the branches were overgrown and in need of pruning. His belief is that they are overgrown, not that they\u2019ve already been snapped off by the wind.\n\nAnswer: a) Hiro believes the bonsai tree has overgrown branches."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Hiro is a bonsai enthusiast in a small village in Japan. He wants to trim his favorite bonsai tree to maintain its shape and health. Hiro sees that the tree's branches are overgrown and need pruning. A gust of wind blows through Hiro's garden, snapping off several of the overgrown branches from the bonsai tree. Hiro trims the overgrown branches from the bonsai tree.\nQuestion: Does Hiro believe the bonsai tree has overgrown branches or that they have been snapped off by the wind?\nChoose one of the following:\na)Hiro believes the bonsai tree has overgrown branches.\nb)Hiro believes the branches have been snapped off by the wind.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Hiro decided to trim because he saw the branches were overgrown and in need of pruning. His belief is that they are overgrown, not that they\u2019ve already been snapped off by the wind.\n\nAnswer: a) Hiro believes the bonsai tree has overgrown branches."
    },
    {
        "continuation": {
            "0-85": "DMN",
            "86-209": "ToM",
            "210-270": "ToM",
            "273-340": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 103,
        "prompt_tokens": 768,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Tumelo is a skilled fisherman in a coastal village in South Africa. He wants to catch fish using his large, sturdy fishing net. Tumelo inspected the net before leaving home and saw that it was in perfect condition. As he walks to the shore, a mischievous seagull pecks at the net, causing a large tear in it. Tumelo throws the net into the water to catch fish.\\nQuestion: Does Tumelo believe the fishing net is in perfect condition or torn?\\nChoose one of the following:\\na)Tumelo believes the fishing net is torn.\\nb)Tumelo believes the fishing net is in perfect condition.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Tumelo inspected the net before leaving and saw it was in perfect condition. The seagull\u2019s tear happened after his inspection, and there\u2019s no indication Tumelo noticed the damage before using the net. Therefore he still believes the net is in perfect condition.  \nAnswer: b) Tumelo believes the fishing net is in perfect condition."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Tumelo is a skilled fisherman in a coastal village in South Africa. He wants to catch fish using his large, sturdy fishing net. Tumelo inspected the net before leaving home and saw that it was in perfect condition. As he walks to the shore, a mischievous seagull pecks at the net, causing a large tear in it. Tumelo throws the net into the water to catch fish.\nQuestion: Does Tumelo believe the fishing net is in perfect condition or torn?\nChoose one of the following:\na)Tumelo believes the fishing net is torn.\nb)Tumelo believes the fishing net is in perfect condition.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Tumelo inspected the net before leaving and saw it was in perfect condition. The seagull\u2019s tear happened after his inspection, and there\u2019s no indication Tumelo noticed the damage before using the net. Therefore he still believes the net is in perfect condition.  \nAnswer: b) Tumelo believes the fishing net is in perfect condition."
    },
    {
        "continuation": {
            "0-147": "ToM",
            "150-209": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 65,
        "prompt_tokens": 737,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kofi is a fisherman in a small coastal village in Ghana. He wants to set sail on his boat to catch fish for his family and the village market. Kofi checks the weather and sees clear skies and calm waters. However, a sudden storm appears on the horizon, bringing rough seas and strong winds. Kofi sets sail to catch fish in the calm waters.\\nQuestion: Does Kofi believe the weather is perfect for sailing or is there a storm approaching?\\nChoose one of the following:\\na)Kofi believes the weather is perfect for sailing.\\nb)Kofi believes there is a storm approaching.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kofi checks the weather and sees clear skies and calm waters, then decides to set sail, so he believes the weather is perfect for sailing.  \nAnswer: a)Kofi believes the weather is perfect for sailing."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kofi is a fisherman in a small coastal village in Ghana. He wants to set sail on his boat to catch fish for his family and the village market. Kofi checks the weather and sees clear skies and calm waters. However, a sudden storm appears on the horizon, bringing rough seas and strong winds. Kofi sets sail to catch fish in the calm waters.\nQuestion: Does Kofi believe the weather is perfect for sailing or is there a storm approaching?\nChoose one of the following:\na)Kofi believes the weather is perfect for sailing.\nb)Kofi believes there is a storm approaching.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kofi checks the weather and sees clear skies and calm waters, then decides to set sail, so he believes the weather is perfect for sailing.  \nAnswer: a)Kofi believes the weather is perfect for sailing."
    },
    {
        "continuation": {
            "0-95": "DMN",
            "96-152": "ToM",
            "153-204": "ToM",
            "206-270": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 89,
        "prompt_tokens": 740,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Odette is a street performer in a bustling city in France. She wants to entertain the passing crowd by playing her accordion. Odette sees that her accordion is in working condition. However, a mischievous child nearby secretly loosened the straps, making it difficult for her to play. Odette starts playing her accordion for the crowd.\\nQuestion: Does Odette believe her accordion is in working condition or has loosened straps?\\nChoose one of the following:\\na)Odette believes her accordion is in working condition.\\nb)Odette believes her accordion has loosened straps.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Odette checks her accordion and sees it is in working condition. She is unaware of the child\u2019s tampering with the straps. Therefore, she believes it is in working condition.\n\nAnswer:a) Odette believes her accordion is in working condition."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Odette is a street performer in a bustling city in France. She wants to entertain the passing crowd by playing her accordion. Odette sees that her accordion is in working condition. However, a mischievous child nearby secretly loosened the straps, making it difficult for her to play. Odette starts playing her accordion for the crowd.\nQuestion: Does Odette believe her accordion is in working condition or has loosened straps?\nChoose one of the following:\na)Odette believes her accordion is in working condition.\nb)Odette believes her accordion has loosened straps.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Odette checks her accordion and sees it is in working condition. She is unaware of the child\u2019s tampering with the straps. Therefore, she believes it is in working condition.\n\nAnswer:a) Odette believes her accordion is in working condition."
    },
    {
        "continuation": {
            "0-81": "ToM",
            "82-194": "ToM",
            "197-244": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 91,
        "prompt_tokens": 765,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Mei is a fisherman in a small coastal village in China. She wants to go fishing early in the morning when the tide is high. Mei checks the tide chart and sees that the tide will be high at 5 AM. However, a strong storm far out at sea has caused the tide to come in much earlier, at 3 AM. Mei waits until 5 AM to go fishing.\\nQuestion: Does Mei believe the tide is high at 5 AM or 3 AM?\\nChoose one of the following:\\na)Mei believes the tide is high at 5 AM.\\nb)Mei believes the tide is high at 3 AM.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Mei checks the tide chart and believes the high tide will occur at 5 AM. She is unaware that the storm caused the tide to come in at 3 AM, so she still believes it will be high at 5 AM.  \nAnswer:a)Mei believes the tide is high at 5 AM."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Mei is a fisherman in a small coastal village in China. She wants to go fishing early in the morning when the tide is high. Mei checks the tide chart and sees that the tide will be high at 5 AM. However, a strong storm far out at sea has caused the tide to come in much earlier, at 3 AM. Mei waits until 5 AM to go fishing.\nQuestion: Does Mei believe the tide is high at 5 AM or 3 AM?\nChoose one of the following:\na)Mei believes the tide is high at 5 AM.\nb)Mei believes the tide is high at 3 AM.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Mei checks the tide chart and believes the high tide will occur at 5 AM. She is unaware that the storm caused the tide to come in at 3 AM, so she still believes it will be high at 5 AM.  \nAnswer:a)Mei believes the tide is high at 5 AM."
    },
    {
        "continuation": {
            "0-172": "ToM",
            "173-234": "ToM",
            "237-311": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 90,
        "prompt_tokens": 780,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Fatima is an archaeologist working on a dig site in Egypt, searching for ancient artifacts. She wants to carefully excavate a small, fragile artifact she has discovered. Fatima observes that the soil around the artifact is soft and loose. Just as Fatima is about to begin excavating, a strong gust of wind blows through the site, hardening the soil around the artifact. Fatima proceeds to excavate the artifact using her original technique for soft and loose soil.\\nQuestion: Does Fatima believe the soil around the artifact is soft and loose or hardened?\\nChoose one of the following:\\na)Fatima believes the soil around the artifact has hardened.\\nb)Fatima believes the soil around the artifact is soft and loose.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Fatima initially observes the soil as soft and loose and, despite the wind hardening it, she continues with her original excavation technique for soft, loose soil. This indicates she still believes the soil is soft and loose.  \nAnswer: b) Fatima believes the soil around the artifact is soft and loose."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Fatima is an archaeologist working on a dig site in Egypt, searching for ancient artifacts. She wants to carefully excavate a small, fragile artifact she has discovered. Fatima observes that the soil around the artifact is soft and loose. Just as Fatima is about to begin excavating, a strong gust of wind blows through the site, hardening the soil around the artifact. Fatima proceeds to excavate the artifact using her original technique for soft and loose soil.\nQuestion: Does Fatima believe the soil around the artifact is soft and loose or hardened?\nChoose one of the following:\na)Fatima believes the soil around the artifact has hardened.\nb)Fatima believes the soil around the artifact is soft and loose.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Fatima initially observes the soil as soft and loose and, despite the wind hardening it, she continues with her original excavation technique for soft, loose soil. This indicates she still believes the soil is soft and loose.  \nAnswer: b) Fatima believes the soil around the artifact is soft and loose."
    },
    {
        "continuation": {
            "0-83": "ToM",
            "84-212": "ToM",
            "214-294": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 86,
        "prompt_tokens": 778,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kwame is a skilled woodcarver in a small African village known for its exceptional craftsmanship. Kwame wants to create a beautiful wooden sculpture for the upcoming village festival. Kwame examines a large piece of wood and notices it is strong and free of imperfections. While Kwame is away, termites infest the wood, weakening its structure and leaving it unsuitable for carving. Kwame starts carving his sculpture using the seemingly perfect piece of wood.\\nQuestion: Does Kwame believe the piece of wood is strong and free of imperfections or weakened and infested with termites?\\nChoose one of the following:\\na)Kwame believes the piece of wood is strong and free of imperfections.\\nb)Kwame believes the piece of wood is weakened and infested with termites.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kwame initially inspects the wood and perceives it as strong and flawless. The termite infestation happens in his absence, so he remains unaware of the damage and continues believing the wood is perfect.\n\nAnswer: a) Kwame believes the piece of wood is strong and free of imperfections."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kwame is a skilled woodcarver in a small African village known for its exceptional craftsmanship. Kwame wants to create a beautiful wooden sculpture for the upcoming village festival. Kwame examines a large piece of wood and notices it is strong and free of imperfections. While Kwame is away, termites infest the wood, weakening its structure and leaving it unsuitable for carving. Kwame starts carving his sculpture using the seemingly perfect piece of wood.\nQuestion: Does Kwame believe the piece of wood is strong and free of imperfections or weakened and infested with termites?\nChoose one of the following:\na)Kwame believes the piece of wood is strong and free of imperfections.\nb)Kwame believes the piece of wood is weakened and infested with termites.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kwame initially inspects the wood and perceives it as strong and flawless. The termite infestation happens in his absence, so he remains unaware of the damage and continues believing the wood is perfect.\n\nAnswer: a) Kwame believes the piece of wood is strong and free of imperfections."
    },
    {
        "continuation": {
            "0-139": "ToM",
            "142-215": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 69,
        "prompt_tokens": 786,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Olumide, a skilled woodcarver in a Nigerian village, is preparing to carve a beautiful sculpture for the village chief. Olumide wants to use a sharp chisel to create intricate details on the sculpture. Olumide observes his set of chisels and sees one that appears to be sharp and in perfect condition. However, while Olumide is talking to a fellow artisan, a child from the village accidentally drops the chisel, causing it to become blunt and damaged. Olumide begins carving the sculpture with the chisel.\\nQuestion: Does Olumide believe the chisel is sharp and in perfect condition or blunt and damaged?\\nChoose one of the following:\\na)Olumide believes the chisel is blunt and damaged.\\nb)Olumide believes the chisel is sharp and in perfect condition.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Olumide saw the chisel in perfect condition and didn\u2019t witness it getting dropped, so he still believes it is sharp and undamaged.  \nAnswer: b) Olumide believes the chisel is sharp and in perfect condition."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Olumide, a skilled woodcarver in a Nigerian village, is preparing to carve a beautiful sculpture for the village chief. Olumide wants to use a sharp chisel to create intricate details on the sculpture. Olumide observes his set of chisels and sees one that appears to be sharp and in perfect condition. However, while Olumide is talking to a fellow artisan, a child from the village accidentally drops the chisel, causing it to become blunt and damaged. Olumide begins carving the sculpture with the chisel.\nQuestion: Does Olumide believe the chisel is sharp and in perfect condition or blunt and damaged?\nChoose one of the following:\na)Olumide believes the chisel is blunt and damaged.\nb)Olumide believes the chisel is sharp and in perfect condition.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Olumide saw the chisel in perfect condition and didn\u2019t witness it getting dropped, so he still believes it is sharp and undamaged.  \nAnswer: b) Olumide believes the chisel is sharp and in perfect condition."
    },
    {
        "continuation": {
            "0-209": "ToM",
            "212-295": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 79,
        "prompt_tokens": 764,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Odion is an archaeologist conducting research at an ancient Egyptian site. He wants to discover a hidden chamber within the pyramid he is exploring. Odion observes a wall with an unusual pattern that appears to be a concealed entrance. While Odion is away, a sandstorm fills the entrance, making it impossible to access the hidden chamber. Odion tries to open the concealed entrance.\\nQuestion: Does Odion believe the wall with the unusual pattern is hiding the entrance or that the entrance is blocked by sand?\\nChoose one of the following:\\na)Odion believes the wall with the unusual pattern is hiding the entrance.\\nb)Odion believes the entrance is blocked by sand.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Odion initially notices the unusual wall pattern and concludes it conceals an entrance, then later the sandstorm blocks that opening. His belief about the wall\u2019s purpose is that it hides the entrance.  \nAnswer: a) Odion believes the wall with the unusual pattern is hiding the entrance."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Odion is an archaeologist conducting research at an ancient Egyptian site. He wants to discover a hidden chamber within the pyramid he is exploring. Odion observes a wall with an unusual pattern that appears to be a concealed entrance. While Odion is away, a sandstorm fills the entrance, making it impossible to access the hidden chamber. Odion tries to open the concealed entrance.\nQuestion: Does Odion believe the wall with the unusual pattern is hiding the entrance or that the entrance is blocked by sand?\nChoose one of the following:\na)Odion believes the wall with the unusual pattern is hiding the entrance.\nb)Odion believes the entrance is blocked by sand.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Odion initially notices the unusual wall pattern and concludes it conceals an entrance, then later the sandstorm blocks that opening. His belief about the wall\u2019s purpose is that it hides the entrance.  \nAnswer: a) Odion believes the wall with the unusual pattern is hiding the entrance."
    },
    {
        "continuation": {
            "0-100": "ToM",
            "101-227": "ToM",
            "230-289": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 92,
        "prompt_tokens": 775,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Ibrahim is a talented potter in a small Middle Eastern village. He wants to make a beautiful, large clay pot to display at the upcoming village bazaar. Ibrahim finds a pile of clay in his workshop, which appears to be moist and malleable. However, while Ibrahim is taking a break, the sun shines directly onto the clay, causing it to dry out and become brittle. Ibrahim starts shaping the clay into a large pot.\\nQuestion: Does Ibrahim believe the clay is moist and malleable or dried out and brittle?\\nChoose one of the following:\\na)Ibrahim believes the clay is dried out and brittle.\\nb)Ibrahim believes the clay is moist and malleable.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Ibrahim begins shaping the clay because he believes it is still in a workable, moist state. Even though the sun has dried it while he was away, he does not realize it has become brittle and thus treats it as malleable.  \nAnswer: b)Ibrahim believes the clay is moist and malleable."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Ibrahim is a talented potter in a small Middle Eastern village. He wants to make a beautiful, large clay pot to display at the upcoming village bazaar. Ibrahim finds a pile of clay in his workshop, which appears to be moist and malleable. However, while Ibrahim is taking a break, the sun shines directly onto the clay, causing it to dry out and become brittle. Ibrahim starts shaping the clay into a large pot.\nQuestion: Does Ibrahim believe the clay is moist and malleable or dried out and brittle?\nChoose one of the following:\na)Ibrahim believes the clay is dried out and brittle.\nb)Ibrahim believes the clay is moist and malleable.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Ibrahim begins shaping the clay because he believes it is still in a workable, moist state. Even though the sun has dried it while he was away, he does not realize it has become brittle and thus treats it as malleable.  \nAnswer: b)Ibrahim believes the clay is moist and malleable."
    },
    {
        "continuation": {
            "0-200": "ToM",
            "203-266": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 84,
        "prompt_tokens": 781,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Rohan is a potter in a small Indian village. Rohan wants to make a beautiful ceramic vase as a gift for his friend's housewarming party. Rohan selects a bag of high-quality clay in his workshop that appears to be soft and malleable. While Rohan is away for lunch, the hot sun dries the clay, making it hard and unworkable. Rohan starts making the vase with the clay.\\nQuestion: Does Rohan believe the bag of clay is soft and malleable or hard and unworkable?\\nChoose one of the following:\\na)Rohan believes the bag of clay is hard and unworkable.\\nb)Rohan believes the bag of clay is soft and malleable.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Rohan picks the clay when it appears soft and malleable, then leaves. He doesn\u2019t notice it has dried in the sun, so he still believes it is soft and workable when he begins to shape his vase.  \nAnswer:b) Rohan believes the bag of clay is soft and malleable."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Rohan is a potter in a small Indian village. Rohan wants to make a beautiful ceramic vase as a gift for his friend's housewarming party. Rohan selects a bag of high-quality clay in his workshop that appears to be soft and malleable. While Rohan is away for lunch, the hot sun dries the clay, making it hard and unworkable. Rohan starts making the vase with the clay.\nQuestion: Does Rohan believe the bag of clay is soft and malleable or hard and unworkable?\nChoose one of the following:\na)Rohan believes the bag of clay is hard and unworkable.\nb)Rohan believes the bag of clay is soft and malleable.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Rohan picks the clay when it appears soft and malleable, then leaves. He doesn\u2019t notice it has dried in the sun, so he still believes it is soft and workable when he begins to shape his vase.  \nAnswer:b) Rohan believes the bag of clay is soft and malleable."
    },
    {
        "continuation": {
            "0-141": "ToM",
            "144-151": "LN",
            "151-153": "LN",
            "154-223": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 79,
        "prompt_tokens": 778,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Rohit is a renowned painter from India who has been commissioned to create a mural on a city building. Rohit wants to paint an intricate scene depicting the vibrant culture and history of India. Rohit selects a set of high-quality, long-lasting paint colors for the mural. While Rohit is out for lunch, a leak in the ceiling drips water onto the paint colors, diluting them and making them less durable. Rohit begins painting the mural using the paint colors he initially selected.\\nQuestion: Does Rohit believe the paint colors are of high quality and long-lasting or water-damaged and less durable?\\nChoose one of the following:\\na)Rohit believes the paint colors are water-damaged and less durable.\\nb)Rohit believes the paint colors are of high quality and long-lasting.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Rohit selected and started using the paints believing they were high-quality and long-lasting; he\u2019s unaware they were water-damaged.  \nAnswer:b) Rohit believes the paint colors are of high quality and long-lasting."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Rohit is a renowned painter from India who has been commissioned to create a mural on a city building. Rohit wants to paint an intricate scene depicting the vibrant culture and history of India. Rohit selects a set of high-quality, long-lasting paint colors for the mural. While Rohit is out for lunch, a leak in the ceiling drips water onto the paint colors, diluting them and making them less durable. Rohit begins painting the mural using the paint colors he initially selected.\nQuestion: Does Rohit believe the paint colors are of high quality and long-lasting or water-damaged and less durable?\nChoose one of the following:\na)Rohit believes the paint colors are water-damaged and less durable.\nb)Rohit believes the paint colors are of high quality and long-lasting.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Rohit selected and started using the paints believing they were high-quality and long-lasting; he\u2019s unaware they were water-damaged.  \nAnswer:b) Rohit believes the paint colors are of high quality and long-lasting."
    },
    {
        "continuation": {
            "0-260": "ToM",
            "263-321": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 96,
        "prompt_tokens": 793,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kunal is a talented potter in a small Indian village. Kunal wants to create a set of earthenware bowls as a gift for his friend's housewarming. Kunal inspects a large lump of clay, finding it to be moist and malleable. However, while Kunal is away for a short break, the sun beats down on the clay, drying it out and making it hard and brittle. Kunal starts shaping the bowls using the seemingly perfect clay.\\nQuestion: Does Kunal believe the clay is moist and malleable or hard and brittle?\\nChoose one of the following:\\na)Kunal believes the clay is hard and brittle.\\nb)Kunal believes the clay is moist and malleable.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kunal inspects the clay and finds it moist and malleable, and he begins shaping bowls using what he thinks is perfect clay. Even though the sun dried it while he was away, he is not aware of that change and believes it to still be moist and malleable.  \nAnswer: b) Kunal believes the clay is moist and malleable."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kunal is a talented potter in a small Indian village. Kunal wants to create a set of earthenware bowls as a gift for his friend's housewarming. Kunal inspects a large lump of clay, finding it to be moist and malleable. However, while Kunal is away for a short break, the sun beats down on the clay, drying it out and making it hard and brittle. Kunal starts shaping the bowls using the seemingly perfect clay.\nQuestion: Does Kunal believe the clay is moist and malleable or hard and brittle?\nChoose one of the following:\na)Kunal believes the clay is hard and brittle.\nb)Kunal believes the clay is moist and malleable.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kunal inspects the clay and finds it moist and malleable, and he begins shaping bowls using what he thinks is perfect clay. Even though the sun dried it while he was away, he is not aware of that change and believes it to still be moist and malleable.  \nAnswer: b) Kunal believes the clay is moist and malleable."
    },
    {
        "continuation": {
            "0-76": "DMN",
            "77-181": "ToM",
            "184-250": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 82,
        "prompt_tokens": 795,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Leilani is a skilled hula dancer from Hawaii who has been asked to perform at a cultural event. Leilani wants to wear her favorite traditional hula skirt made of ti leaves for the performance. Leilani sees her ti leaf skirt hanging in her closet, appearing to be in good condition. While Leilani is away from her room, her younger brother accidentally spills a glass of water near the closet, causing her ti leaf skirt to become wet and wilted. Leilani puts on the ti leaf skirt and prepares for her performance.\\nQuestion: Does Leilani believe the ti leaf skirt is in good condition or wet and wilted?\\nChoose one of the following:\\na)Leilani believes the ti leaf skirt is in good condition.\\nb)Leilani believes the ti leaf skirt is wet and wilted.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Leilani saw the skirt hanging and in good condition before leaving. She didn\u2019t witness the spill or check the skirt afterward, so she still believes it\u2019s in good condition.  \nAnswer: a)Leilani believes the ti leaf skirt is in good condition."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Leilani is a skilled hula dancer from Hawaii who has been asked to perform at a cultural event. Leilani wants to wear her favorite traditional hula skirt made of ti leaves for the performance. Leilani sees her ti leaf skirt hanging in her closet, appearing to be in good condition. While Leilani is away from her room, her younger brother accidentally spills a glass of water near the closet, causing her ti leaf skirt to become wet and wilted. Leilani puts on the ti leaf skirt and prepares for her performance.\nQuestion: Does Leilani believe the ti leaf skirt is in good condition or wet and wilted?\nChoose one of the following:\na)Leilani believes the ti leaf skirt is in good condition.\nb)Leilani believes the ti leaf skirt is wet and wilted.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Leilani saw the skirt hanging and in good condition before leaving. She didn\u2019t witness the spill or check the skirt afterward, so she still believes it\u2019s in good condition.  \nAnswer: a)Leilani believes the ti leaf skirt is in good condition."
    },
    {
        "continuation": {
            "9-149": "ToM",
            "150-279": "ToM",
            "282-358": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 93,
        "prompt_tokens": 785,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Amara, a skilled Nigerian weaver, is preparing to create a traditional textile for an upcoming cultural exhibition. Amara wants to use a unique combination of colorful threads that represents her heritage. Amara carefully chooses a set of vibrant, high-quality threads for her project. A curious toddler in her home finds the threads and tangles them, making them difficult to use in the weaving process. Amara starts weaving the textile using the threads she selected.\\nQuestion: Does Amara believe the threads are strong and perfect for her textile or tangled and difficult to use?\\nChoose one of the following:\\na)Amara believes the threads are tangled and difficult to use.\\nb)Amara believes the threads are strong and perfect for her textile.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Amara carefully selects vibrant, high-quality threads for her project, which shows she believes they are strong and perfect for her textile. The toddler\u2019s tangling makes them difficult in practice, but her belief about the threads themselves remains that they are ideal.  \nAnswer:b) Amara believes the threads are strong and perfect for her textile."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Amara, a skilled Nigerian weaver, is preparing to create a traditional textile for an upcoming cultural exhibition. Amara wants to use a unique combination of colorful threads that represents her heritage. Amara carefully chooses a set of vibrant, high-quality threads for her project. A curious toddler in her home finds the threads and tangles them, making them difficult to use in the weaving process. Amara starts weaving the textile using the threads she selected.\nQuestion: Does Amara believe the threads are strong and perfect for her textile or tangled and difficult to use?\nChoose one of the following:\na)Amara believes the threads are tangled and difficult to use.\nb)Amara believes the threads are strong and perfect for her textile.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Amara carefully selects vibrant, high-quality threads for her project, which shows she believes they are strong and perfect for her textile. The toddler\u2019s tangling makes them difficult in practice, but her belief about the threads themselves remains that they are ideal.  \nAnswer:b) Amara believes the threads are strong and perfect for her textile."
    },
    {
        "continuation": {
            "0-180": "ToM",
            "182-262": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 66,
        "prompt_tokens": 780,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Fatima is an archaeologist working on a dig site in Egypt. Fatima hopes to find an ancient artifact that could shed light on an important historical event. During her excavation, Fatima discovers a clay tablet with faded inscriptions that she believes might be valuable. As a sandstorm approaches, the strong winds blow sand onto the tablet, erasing the remaining inscriptions and making the tablet indistinguishable from any other piece of clay. Fatima brings a camera to take high-resolution photographs of the inscriptions.\\nQuestion: Does Fatima believe the clay tablet has important historical inscriptions or has become indistinguishable from any other piece of clay?\\nChoose one of the following:\\na)Fatima believes the clay tablet has become indistinguishable from any other piece of clay.\\nb)Fatima believes the clay tablet has important historical inscriptions.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Fatima initially believes the tablet holds valuable historical inscriptions, despite the sandstorm erasing them; she still photographs them hoping to capture what remains.\n\nAnswer: b)Fatima believes the clay tablet has important historical inscriptions."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Fatima is an archaeologist working on a dig site in Egypt. Fatima hopes to find an ancient artifact that could shed light on an important historical event. During her excavation, Fatima discovers a clay tablet with faded inscriptions that she believes might be valuable. As a sandstorm approaches, the strong winds blow sand onto the tablet, erasing the remaining inscriptions and making the tablet indistinguishable from any other piece of clay. Fatima brings a camera to take high-resolution photographs of the inscriptions.\nQuestion: Does Fatima believe the clay tablet has important historical inscriptions or has become indistinguishable from any other piece of clay?\nChoose one of the following:\na)Fatima believes the clay tablet has become indistinguishable from any other piece of clay.\nb)Fatima believes the clay tablet has important historical inscriptions.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Fatima initially believes the tablet holds valuable historical inscriptions, despite the sandstorm erasing them; she still photographs them hoping to capture what remains.\n\nAnswer: b)Fatima believes the clay tablet has important historical inscriptions."
    },
    {
        "continuation": {
            "0-109": "ToM",
            "110-206": "ToM",
            "208-276": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 85,
        "prompt_tokens": 747,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Xia is an experienced fisherman living in a coastal village in China. Xia wants to go out and fish in the ocean on his sturdy wooden boat. Xia checks the weather forecast and sees clear skies and calm seas. However, a sudden storm forms offshore, bringing strong winds and choppy waters. Xia sets sail on his wooden boat to fish in the ocean.\\nQuestion: Does Xia believe the weather is perfect for fishing or that a storm is approaching?\\nChoose one of the following:\\na)Xia believes the weather is perfect for a day of fishing.\\nb)Xia believes a storm is approaching.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Xia checks the forecast and sees clear skies and calm seas, so he believes it\u2019s perfect for fishing. The sudden storm forms later, but his belief before setting sail is that the weather is perfect.\n\nAnswer: a) Xia believes the weather is perfect for a day of fishing."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Xia is an experienced fisherman living in a coastal village in China. Xia wants to go out and fish in the ocean on his sturdy wooden boat. Xia checks the weather forecast and sees clear skies and calm seas. However, a sudden storm forms offshore, bringing strong winds and choppy waters. Xia sets sail on his wooden boat to fish in the ocean.\nQuestion: Does Xia believe the weather is perfect for fishing or that a storm is approaching?\nChoose one of the following:\na)Xia believes the weather is perfect for a day of fishing.\nb)Xia believes a storm is approaching.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Xia checks the forecast and sees clear skies and calm seas, so he believes it\u2019s perfect for fishing. The sudden storm forms later, but his belief before setting sail is that the weather is perfect.\n\nAnswer: a) Xia believes the weather is perfect for a day of fishing."
    },
    {
        "continuation": {
            "0-73": "ToM",
            "74-148": "ToM",
            "149-207": "ToM",
            "210-260": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 89,
        "prompt_tokens": 758,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Mei is a dedicated librarian in a small town in China. She wants to organize a reading corner for children using a large, colorful rug. Mei sees the rug rolled up in a storage room, appearing to be in good condition. However, when Mei is not looking, a leak in the storage room ceiling drips water onto the rug, causing it to become soaked and damp. Mei sets up the reading corner with the colorful rug.\\nQuestion: Does Mei believe the rug is in good condition or damp and soaked?\\nChoose one of the following:\\na)Mei believes the rug is damp and soaked.\\nb)Mei believes the rug is in good condition.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Mei saw the rug rolled up and believed it was in good condition. She did not witness the leak and has no knowledge of the rug being soaked. Therefore she still believes the rug is in good condition.  \nAnswer: b) Mei believes the rug is good condition."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Mei is a dedicated librarian in a small town in China. She wants to organize a reading corner for children using a large, colorful rug. Mei sees the rug rolled up in a storage room, appearing to be in good condition. However, when Mei is not looking, a leak in the storage room ceiling drips water onto the rug, causing it to become soaked and damp. Mei sets up the reading corner with the colorful rug.\nQuestion: Does Mei believe the rug is in good condition or damp and soaked?\nChoose one of the following:\na)Mei believes the rug is damp and soaked.\nb)Mei believes the rug is in good condition.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Mei saw the rug rolled up and believed it was in good condition. She did not witness the leak and has no knowledge of the rug being soaked. Therefore she still believes the rug is in good condition.  \nAnswer: b) Mei believes the rug is good condition."
    },
    {
        "continuation": {
            "0-136": "ToM",
            "139-184": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 62,
        "prompt_tokens": 754,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Olabisi is a beekeeper in a small town in Nigeria. Olabisi wants to collect honey from the beehives to sell at the local market. The bees appear to be calm and undisturbed as they go about their daily activities. Out of nowhere, a group of children playing nearby accidentally throw a ball into the beehives, agitating the bees and making them aggressive. Olabisi starts to collect honey from the calm beehives.\\nQuestion: Does Olabisi believe the bees are calm or agitated and aggressive?\\nChoose one of the following:\\na)Olabisi believes the bees are agitated and aggressive.\\nb)Olabisi believes the bees are calm.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Olabisi \u201cstarts to collect honey from the calm beehives,\u201d which indicates she believes the bees are calm.  \nAnswer:b) Olabisi believes the bees are calm."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Olabisi is a beekeeper in a small town in Nigeria. Olabisi wants to collect honey from the beehives to sell at the local market. The bees appear to be calm and undisturbed as they go about their daily activities. Out of nowhere, a group of children playing nearby accidentally throw a ball into the beehives, agitating the bees and making them aggressive. Olabisi starts to collect honey from the calm beehives.\nQuestion: Does Olabisi believe the bees are calm or agitated and aggressive?\nChoose one of the following:\na)Olabisi believes the bees are agitated and aggressive.\nb)Olabisi believes the bees are calm.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Olabisi \u201cstarts to collect honey from the calm beehives,\u201d which indicates she believes the bees are calm.  \nAnswer:b) Olabisi believes the bees are calm."
    },
    {
        "continuation": {
            "0-165": "ToM",
            "168-244": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 71,
        "prompt_tokens": 770,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kalyani is a traditional Indian dancer in a temple in Kerala, India. Kalyani wants to perform a special dance for the upcoming festival to pay homage to the temple's deity. The temple floor is clean and free of any obstacles, as Kalyani has prepared the space for her dance practice. An accidental spill of oil from a temple lamp causes a slippery patch on the floor. Kalyani starts her dance practice.\\nQuestion: Does Kalyani believe the temple floor is clean and perfect for dancing or slippery due to the oil spill?\\nChoose one of the following:\\na)Kalyani believes the temple floor is clean and perfect for dancing.\\nb)Kalyani believes the floor is slippery due to the oil spill.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kalyani prepared and inspected the floor herself and has no knowledge of the hidden oil spill, so she believes the space is clean and perfect for her dance.  \nAnswer:a)Kalyani believes the temple floor is clean and perfect for dancing."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kalyani is a traditional Indian dancer in a temple in Kerala, India. Kalyani wants to perform a special dance for the upcoming festival to pay homage to the temple's deity. The temple floor is clean and free of any obstacles, as Kalyani has prepared the space for her dance practice. An accidental spill of oil from a temple lamp causes a slippery patch on the floor. Kalyani starts her dance practice.\nQuestion: Does Kalyani believe the temple floor is clean and perfect for dancing or slippery due to the oil spill?\nChoose one of the following:\na)Kalyani believes the temple floor is clean and perfect for dancing.\nb)Kalyani believes the floor is slippery due to the oil spill.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kalyani prepared and inspected the floor herself and has no knowledge of the hidden oil spill, so she believes the space is clean and perfect for her dance.  \nAnswer:a)Kalyani believes the temple floor is clean and perfect for dancing."
    },
    {
        "continuation": {
            "0-141": "ToM",
            "144-217": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 66,
        "prompt_tokens": 767,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kavya is a florist in a vibrant Indian market. Kavya wants to create a beautiful bouquet of fresh roses for a customer's anniversary celebration. Kavya sees a batch of roses in her shop that appear to be fresh and vibrant. Unbeknownst to her, a mischievous monkey sneaks into the shop and nibbles on the rose petals, leaving them damaged and unsuitable for the bouquet. Kavya starts arranging the bouquet using the roses she initially saw.\\nQuestion: Does Kavya believe the roses are fresh and perfect for the bouquet or damaged by the monkey?\\nChoose one of the following:\\na)Kavya believes the roses are fresh and perfect for the bouquet.\\nb)Kavya believes the roses are damaged by the monkey.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kavya has not seen the monkey damage and only sees what appears to be fresh roses, so she believes they are perfect for the bouquet.  \nAnswer:a) Kavya believes the roses are fresh and perfect for the bouquet."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kavya is a florist in a vibrant Indian market. Kavya wants to create a beautiful bouquet of fresh roses for a customer's anniversary celebration. Kavya sees a batch of roses in her shop that appear to be fresh and vibrant. Unbeknownst to her, a mischievous monkey sneaks into the shop and nibbles on the rose petals, leaving them damaged and unsuitable for the bouquet. Kavya starts arranging the bouquet using the roses she initially saw.\nQuestion: Does Kavya believe the roses are fresh and perfect for the bouquet or damaged by the monkey?\nChoose one of the following:\na)Kavya believes the roses are fresh and perfect for the bouquet.\nb)Kavya believes the roses are damaged by the monkey.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kavya has not seen the monkey damage and only sees what appears to be fresh roses, so she believes they are perfect for the bouquet.  \nAnswer:a) Kavya believes the roses are fresh and perfect for the bouquet."
    },
    {
        "continuation": {
            "0-119": "DMN",
            "120-225": "ToM",
            "227-315": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 91,
        "prompt_tokens": 788,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Amara is a skilled potter in a small Nigerian village. Amara wants to create a beautiful, large clay pot to be used in the village's upcoming festival. The kiln, used for firing pottery, has been heated to the appropriate temperature for the clay. Unbeknownst to her, a sudden gust of wind blows open the kiln door, causing the temperature to drop significantly. Amara places the large clay pot inside the kiln for firing.\\nQuestion: Does Amara believe the kiln is at the right temperature for firing the large pot or that the temperature has dropped significantly?\\nChoose one of the following:\\na)Amara believes the kiln is at the right temperature for firing the large pot.\\nb)Amara believes the temperature in the kiln has dropped significantly.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Amara heated the kiln to the correct temperature and is unaware that wind has cooled it. Since she does not know the temperature dropped, she still believes it\u2019s at the right firing temperature.\n\nAnswer: a) Amara believes the kiln is at the right temperature for firing the large pot."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Amara is a skilled potter in a small Nigerian village. Amara wants to create a beautiful, large clay pot to be used in the village's upcoming festival. The kiln, used for firing pottery, has been heated to the appropriate temperature for the clay. Unbeknownst to her, a sudden gust of wind blows open the kiln door, causing the temperature to drop significantly. Amara places the large clay pot inside the kiln for firing.\nQuestion: Does Amara believe the kiln is at the right temperature for firing the large pot or that the temperature has dropped significantly?\nChoose one of the following:\na)Amara believes the kiln is at the right temperature for firing the large pot.\nb)Amara believes the temperature in the kiln has dropped significantly.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Amara heated the kiln to the correct temperature and is unaware that wind has cooled it. Since she does not know the temperature dropped, she still believes it\u2019s at the right firing temperature.\n\nAnswer: a) Amara believes the kiln is at the right temperature for firing the large pot."
    },
    {
        "continuation": {
            "0-99": "ToM",
            "100-194": "ToM",
            "196-206": "LN",
            "207-286": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 92,
        "prompt_tokens": 768,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Emilia is a talented baker in a small Italian town. She plans to bake a large batch of biscotti to sell at the local market. Emilia inspects the oven's temperature, which appears to be set at the correct temperature for baking. A power surge causes the oven's temperature to spike, making it too hot for baking biscotti. Emilia starts baking the biscotti at the oven's current temperature.\\nQuestion: Does Emilia believe the oven is set at the correct temperature for baking biscotti or too hot?\\nChoose one of the following:\\na)Emilia believes the oven is set at the correct temperature for baking biscotti.\\nb)Emilia believes the oven is too hot for baking biscotti.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Emilia inspected the oven before the surge and believed it was at the correct temperature. She does not recheck after the surge and proceeds to bake, so she still believes it\u2019s correct.\n\nAnswer: a) Emilia believes the oven is set at the correct temperature for baking biscotti."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Emilia is a talented baker in a small Italian town. She plans to bake a large batch of biscotti to sell at the local market. Emilia inspects the oven's temperature, which appears to be set at the correct temperature for baking. A power surge causes the oven's temperature to spike, making it too hot for baking biscotti. Emilia starts baking the biscotti at the oven's current temperature.\nQuestion: Does Emilia believe the oven is set at the correct temperature for baking biscotti or too hot?\nChoose one of the following:\na)Emilia believes the oven is set at the correct temperature for baking biscotti.\nb)Emilia believes the oven is too hot for baking biscotti.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Emilia inspected the oven before the surge and believed it was at the correct temperature. She does not recheck after the surge and proceeds to bake, so she still believes it\u2019s correct.\n\nAnswer: a) Emilia believes the oven is set at the correct temperature for baking biscotti."
    },
    {
        "continuation": {
            "0-111": "ToM",
            "114-185": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 159,
        "prompt_tokens": 733,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Mei is a talented chef in a small Chinese village. She wants to prepare a special dish using fresh vegetables from her garden. Mei sees that the vegetables in the garden are ripe and ready to be picked. Later, a hailstorm damages the garden, bruising and damaging the vegetables. Mei goes to the shed to get tools to start harvesting the vegetables.\\nQuestion: Does Mei believe the vegetables are ripe and ready to be picked or damaged by the hailstorm?\\nChoose one of the following:\\na)Mei believes the vegetables are damaged by the hailstorm.\\nb)Mei believes the vegetables are ripe and ready to be picked.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Mei initially observes that the vegetables are ripe and ready for picking before the hailstorm occurs.  \nAnswer: b) Mei believes the vegetables are ripe and ready to be picked."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Mei is a talented chef in a small Chinese village. She wants to prepare a special dish using fresh vegetables from her garden. Mei sees that the vegetables in the garden are ripe and ready to be picked. Later, a hailstorm damages the garden, bruising and damaging the vegetables. Mei goes to the shed to get tools to start harvesting the vegetables.\nQuestion: Does Mei believe the vegetables are ripe and ready to be picked or damaged by the hailstorm?\nChoose one of the following:\na)Mei believes the vegetables are damaged by the hailstorm.\nb)Mei believes the vegetables are ripe and ready to be picked.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Mei initially observes that the vegetables are ripe and ready for picking before the hailstorm occurs.  \nAnswer: b) Mei believes the vegetables are ripe and ready to be picked."
    },
    {
        "continuation": {
            "0-76": "DMN",
            "77-157": "ToM",
            "158-223": "ToM",
            "225-295": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 98,
        "prompt_tokens": 777,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Emeka is a skilled fisherman in a small Nigerian village. He wants to catch fish to feed his family and sell at the market. Emeka prepares his fishing net, which appears to be strong and without any holes. However, while Emeka is not looking, a mischievous child cuts a large hole in the net with a knife. Emeka casts his net into the water, expecting a large catch.\\nQuestion: Does Emeka believe the fishing net is strong and without holes, or that it has a large hole in it?\\nChoose one of the following:\\na)Emeka believes the fishing net is strong and without holes.\\nb)Emeka believes that the fishing net has a large hole in it.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Emeka prepared the net himself and saw it was strong and hole-free. He did not witness the child cutting the net, so he remains unaware of the hole. Therefore, he believes the net is still strong and without holes.\n\nAnswer: a) Emeka believes the fishing net is strong and without holes."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Emeka is a skilled fisherman in a small Nigerian village. He wants to catch fish to feed his family and sell at the market. Emeka prepares his fishing net, which appears to be strong and without any holes. However, while Emeka is not looking, a mischievous child cuts a large hole in the net with a knife. Emeka casts his net into the water, expecting a large catch.\nQuestion: Does Emeka believe the fishing net is strong and without holes, or that it has a large hole in it?\nChoose one of the following:\na)Emeka believes the fishing net is strong and without holes.\nb)Emeka believes that the fishing net has a large hole in it.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Emeka prepared the net himself and saw it was strong and hole-free. He did not witness the child cutting the net, so he remains unaware of the hole. Therefore, he believes the net is still strong and without holes.\n\nAnswer: a) Emeka believes the fishing net is strong and without holes."
    },
    {
        "continuation": {
            "0-122": "DMN",
            "123-206": "ToM",
            "208-292": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 91,
        "prompt_tokens": 785,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Aria is a talented baker from Italy who has been asked to bake a cake for a friend's birthday party. She wants to create a delicious, multi-layered cake that will impress everyone. Aria has a fresh batch of ingredients ready to use for the cake. However, a mischievous cat sneaks into her kitchen and nibbles on some of the ingredients, making them no longer fresh. Aria uses the ingredients to bake the cake.\\nQuestion: Does Aria believe her ingredients are fresh and suitable or nibbled on and no longer fresh?\\nChoose one of the following:\\na)Aria believes the ingredients are nibbled on and no longer fresh.\\nb)Aria believes her ingredients are fresh and suitable for baking the cake.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story says Aria has a fresh batch of ingredients, then a cat sneaks in and nibbles them without her noticing. Since she goes ahead and uses them, she still believes they are fresh and suitable.\n\nAnswer: b) Aria believes her ingredients are fresh and suitable for baking the cake."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Aria is a talented baker from Italy who has been asked to bake a cake for a friend's birthday party. She wants to create a delicious, multi-layered cake that will impress everyone. Aria has a fresh batch of ingredients ready to use for the cake. However, a mischievous cat sneaks into her kitchen and nibbles on some of the ingredients, making them no longer fresh. Aria uses the ingredients to bake the cake.\nQuestion: Does Aria believe her ingredients are fresh and suitable or nibbled on and no longer fresh?\nChoose one of the following:\na)Aria believes the ingredients are nibbled on and no longer fresh.\nb)Aria believes her ingredients are fresh and suitable for baking the cake.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story says Aria has a fresh batch of ingredients, then a cat sneaks in and nibbles them without her noticing. Since she goes ahead and uses them, she still believes they are fresh and suitable.\n\nAnswer: b) Aria believes her ingredients are fresh and suitable for baking the cake."
    },
    {
        "continuation": {
            "0-126": "DMN",
            "127-255": "ToM",
            "258-322": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 98,
        "prompt_tokens": 797,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Takeshi is a fisherman in a small Japanese coastal town. Takeshi wants to catch a large number of fish to sell at the market and provide for his family. He notices that the sea is calm and the fish are abundant, making it a perfect day for fishing. However, a sudden underwater volcanic eruption nearby disrupts the marine life, causing the fish to scatter and flee the area. Takeshi goes out on his boat to catch fish.\\nQuestion: Does Takeshi believe that the sea is calm and full of fish, or that the fish have scattered due to the volcanic eruption?\\nChoose one of the following:\\na)Takeshi believes that the sea is calm and full of fish.\\nb)Takeshi believes that the fish have scattered due to the volcanic eruption.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Takeshi notices the sea is calm and the fish are abundant, making it a perfect day for fishing. Although an eruption later causes fish to scatter, his initial belief when he goes out is that the sea is calm and full of fish.  \nAnswer:a)Takeshi believes that the sea is calm and full of fish."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Takeshi is a fisherman in a small Japanese coastal town. Takeshi wants to catch a large number of fish to sell at the market and provide for his family. He notices that the sea is calm and the fish are abundant, making it a perfect day for fishing. However, a sudden underwater volcanic eruption nearby disrupts the marine life, causing the fish to scatter and flee the area. Takeshi goes out on his boat to catch fish.\nQuestion: Does Takeshi believe that the sea is calm and full of fish, or that the fish have scattered due to the volcanic eruption?\nChoose one of the following:\na)Takeshi believes that the sea is calm and full of fish.\nb)Takeshi believes that the fish have scattered due to the volcanic eruption.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Takeshi notices the sea is calm and the fish are abundant, making it a perfect day for fishing. Although an eruption later causes fish to scatter, his initial belief when he goes out is that the sea is calm and full of fish.  \nAnswer:a)Takeshi believes that the sea is calm and full of fish."
    },
    {
        "continuation": {
            "0-151": "ToM",
            "152-224": "ToM",
            "226-302": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 89,
        "prompt_tokens": 780,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Ravi is a fisherman living in a coastal village in India. Ravi wants to catch a large amount of fish to sell at the market and provide for his family. The waters near the shore are calm and full of fish, making it an ideal location for fishing. An underwater earthquake occurs, causing the fish to frantically swim away from the coast. Ravi casts his net into the calm waters near the shore.\\nQuestion: Does Ravi believe the waters near the shore are calm and full of fish, or that the fish have frantically swum away?\\nChoose one of the following:\\na)Ravi believes the fish have frantically swum away from the coast.\\nb)Ravi believes the waters near the shore are calm and full of fish.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Ravi sees the waters near the shore as calm and full of fish and casts his net there, despite the underwater earthquake. This shows his belief is that the waters are calm and stocked with fish.\n\nAnswer:b) Ravi believes the waters near the shore are calm and full of fish."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Ravi is a fisherman living in a coastal village in India. Ravi wants to catch a large amount of fish to sell at the market and provide for his family. The waters near the shore are calm and full of fish, making it an ideal location for fishing. An underwater earthquake occurs, causing the fish to frantically swim away from the coast. Ravi casts his net into the calm waters near the shore.\nQuestion: Does Ravi believe the waters near the shore are calm and full of fish, or that the fish have frantically swum away?\nChoose one of the following:\na)Ravi believes the fish have frantically swum away from the coast.\nb)Ravi believes the waters near the shore are calm and full of fish.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Ravi sees the waters near the shore as calm and full of fish and casts his net there, despite the underwater earthquake. This shows his belief is that the waters are calm and stocked with fish.\n\nAnswer:b) Ravi believes the waters near the shore are calm and full of fish."
    },
    {
        "continuation": {
            "0-74": "ToM",
            "75-174": "ToM",
            "177-249": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 80,
        "prompt_tokens": 769,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Surya is a fisherman in a coastal village in India. Surya plans to go fishing early in the morning to catch fish for his family and to sell at the local market. Surya's fishing net looks strong and ready for another day of fishing. Overnight, a group of mischievous monkeys rummage through the fishing equipment and badly damage Surya's fishing net. Surya takes the fishing net and goes fishing.\\nQuestion: Does Surya believe that his fishing net is strong and ready to be used, or that it has been badly damaged?\\nChoose one of the following:\\na)Surya believes his fishing net has been badly damaged.\\nb)Surya believes his fishing net is strong and ready to be used.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Surya prepares his net in the morning assuming it\u2019s still intact. He is unaware the monkeys damaged it overnight, so he believes it remains strong and ready for use.  \nAnswer:b) Surya believes his fishing net is strong and ready to be used."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Surya is a fisherman in a coastal village in India. Surya plans to go fishing early in the morning to catch fish for his family and to sell at the local market. Surya's fishing net looks strong and ready for another day of fishing. Overnight, a group of mischievous monkeys rummage through the fishing equipment and badly damage Surya's fishing net. Surya takes the fishing net and goes fishing.\nQuestion: Does Surya believe that his fishing net is strong and ready to be used, or that it has been badly damaged?\nChoose one of the following:\na)Surya believes his fishing net has been badly damaged.\nb)Surya believes his fishing net is strong and ready to be used.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Surya prepares his net in the morning assuming it\u2019s still intact. He is unaware the monkeys damaged it overnight, so he believes it remains strong and ready for use.  \nAnswer:b) Surya believes his fishing net is strong and ready to be used."
    },
    {
        "continuation": {
            "0-111": "ToM",
            "112-227": "ToM",
            "230-288": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 92,
        "prompt_tokens": 778,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Takeshi is a fisherman in a small Japanese coastal village. Takeshi's goal is to catch enough fish to provide for his family and sell at the market. The ocean is calm and teeming with fish, making it an ideal day for fishing. However, a sudden underwater volcanic eruption nearby causes the fish to scatter, leaving the area devoid of fish. Takeshi casts his fishing net into the ocean.\\nQuestion: Does Takeshi believe the ocean is teeming with fish, or devoid of fish due to the volcanic eruption?\\nChoose one of the following:\\na)Takeshi believes the ocean is devoid of fish due to the volcanic eruption.\\nb)Takeshi believes the ocean is teeming with fish.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Takeshi sees a calm, fish-filled ocean and casts his net expecting a good catch. He has no indication that the eruption has driven the fish away, so he believes the sea is still teeming with fish.  \nAnswer: b)Takeshi believes the ocean is teeming with fish."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Takeshi is a fisherman in a small Japanese coastal village. Takeshi's goal is to catch enough fish to provide for his family and sell at the market. The ocean is calm and teeming with fish, making it an ideal day for fishing. However, a sudden underwater volcanic eruption nearby causes the fish to scatter, leaving the area devoid of fish. Takeshi casts his fishing net into the ocean.\nQuestion: Does Takeshi believe the ocean is teeming with fish, or devoid of fish due to the volcanic eruption?\nChoose one of the following:\na)Takeshi believes the ocean is devoid of fish due to the volcanic eruption.\nb)Takeshi believes the ocean is teeming with fish.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Takeshi sees a calm, fish-filled ocean and casts his net expecting a good catch. He has no indication that the eruption has driven the fish away, so he believes the sea is still teeming with fish.  \nAnswer: b)Takeshi believes the ocean is teeming with fish."
    },
    {
        "continuation": {
            "0-111": "DMN",
            "112-178": "ToM",
            "181-260": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 81,
        "prompt_tokens": 768,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kwame is a skilled woodcarver in a small village in Ghana. Kwame wants to create a beautiful wooden sculpture of an elephant for his friend's birthday. He selects a block of wood from his workshop that appears to be solid and strong. However, as he begins to carve, he discovers a hidden pocket of termites inside the wood, rendering it unsuitable for his project. Kwame continues carving the elephant sculpture from the block of wood.\\nQuestion: Does Kwame believe the block of wood is sturdy and suitable for carving or that it contains termites?\\nChoose one of the following:\\na)Kwame believes the block of wood contains termites.\\nb)Kwame believes the block of wood is sturdy and suitable for carving.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kwame chose the wood because it \u201cappears to be solid and strong,\u201d only later discovering the termites. Thus initially he believed it was sturdy and suitable for carving.  \nAnswer: b) Kwame believes the block of wood is sturdy and suitable for carving."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kwame is a skilled woodcarver in a small village in Ghana. Kwame wants to create a beautiful wooden sculpture of an elephant for his friend's birthday. He selects a block of wood from his workshop that appears to be solid and strong. However, as he begins to carve, he discovers a hidden pocket of termites inside the wood, rendering it unsuitable for his project. Kwame continues carving the elephant sculpture from the block of wood.\nQuestion: Does Kwame believe the block of wood is sturdy and suitable for carving or that it contains termites?\nChoose one of the following:\na)Kwame believes the block of wood contains termites.\nb)Kwame believes the block of wood is sturdy and suitable for carving.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kwame chose the wood because it \u201cappears to be solid and strong,\u201d only later discovering the termites. Thus initially he believed it was sturdy and suitable for carving.  \nAnswer: b) Kwame believes the block of wood is sturdy and suitable for carving."
    },
    {
        "continuation": {
            "0-142": "ToM",
            "145-201": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 62,
        "prompt_tokens": 755,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Ngozi is a potter in a small Nigerian village, known for her beautiful ceramic vases. Ngozi wants to create a vase with a blue glaze to showcase at the upcoming village art exhibition. She reaches for a container of blue glaze from her shelf to apply to the vase. Earlier in the day, her apprentice accidentally switched the labels of the blue and green glaze containers, so Ngozi unknowingly picks up the container with the green glaze. Ngozi prepares her brushes to start applying the glaze.\\nQuestion: Does Ngozi believe the container holds blue glaze or green glaze?\\nChoose one of the following:\\na)Ngozi believes the container holds green glaze.\\nb)Ngozi believes the container holds blue glaze.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Ngozi reaches for the container labeled \u201cblue glaze\u201d and is unaware the labels were switched, so she believes it contains blue glaze.  \nAnswer:b) Ngozi believes the container holds blue glaze."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Ngozi is a potter in a small Nigerian village, known for her beautiful ceramic vases. Ngozi wants to create a vase with a blue glaze to showcase at the upcoming village art exhibition. She reaches for a container of blue glaze from her shelf to apply to the vase. Earlier in the day, her apprentice accidentally switched the labels of the blue and green glaze containers, so Ngozi unknowingly picks up the container with the green glaze. Ngozi prepares her brushes to start applying the glaze.\nQuestion: Does Ngozi believe the container holds blue glaze or green glaze?\nChoose one of the following:\na)Ngozi believes the container holds green glaze.\nb)Ngozi believes the container holds blue glaze.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Ngozi reaches for the container labeled \u201cblue glaze\u201d and is unaware the labels were switched, so she believes it contains blue glaze.  \nAnswer:b) Ngozi believes the container holds blue glaze."
    },
    {
        "continuation": {
            "0-208": "ToM",
            "211-293": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 80,
        "prompt_tokens": 785,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Naledi is a talented South African chef preparing a traditional braai for her friends and family. Naledi wants to make a delicious marinade for the meats and vegetables to be grilled. She takes a bottle of vinegar from the cupboard to mix with other ingredients for the marinade. Her cousin, who was helping in the kitchen earlier, accidentally placed a bottle of water in the vinegar's spot, and Naledi unknowingly grabs the bottle of water instead. Naledi uses what she believes is vinegar to make the marinade.\\nQuestion: Does Naledi believe the bottle she took from the cupboard contains vinegar or water?\\nChoose one of the following:\\na)Naledi believes the bottle she took from the cupboard contains vinegar.\\nb)Naledi believes the bottle she took from the cupboard contains water.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Naledi intends to grab vinegar for her marinade and believes the bottle in the cupboard is the vinegar she needs. She is unaware that her cousin swapped the bottles, so she still thinks it\u2019s vinegar.  \nAnswer: a) Naledi believes the bottle she took from the cupboard contains vinegar."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Naledi is a talented South African chef preparing a traditional braai for her friends and family. Naledi wants to make a delicious marinade for the meats and vegetables to be grilled. She takes a bottle of vinegar from the cupboard to mix with other ingredients for the marinade. Her cousin, who was helping in the kitchen earlier, accidentally placed a bottle of water in the vinegar's spot, and Naledi unknowingly grabs the bottle of water instead. Naledi uses what she believes is vinegar to make the marinade.\nQuestion: Does Naledi believe the bottle she took from the cupboard contains vinegar or water?\nChoose one of the following:\na)Naledi believes the bottle she took from the cupboard contains vinegar.\nb)Naledi believes the bottle she took from the cupboard contains water.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Naledi intends to grab vinegar for her marinade and believes the bottle in the cupboard is the vinegar she needs. She is unaware that her cousin swapped the bottles, so she still thinks it\u2019s vinegar.  \nAnswer: a) Naledi believes the bottle she took from the cupboard contains vinegar."
    },
    {
        "continuation": {
            "0-132": "ToM",
            "135-205": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 65,
        "prompt_tokens": 748,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Lila is a talented violinist who lives in a bustling city in India. Today, she has an important audition for a renowned symphony orchestra. Lila picks up her violin case, which she assumes contains her valuable violin. However, her younger brother, who was playing around earlier, accidentally swapped her violin case with his identical-looking case containing a beginner's violin. Lila goes on stage with her violin case.\\nQuestion: Does Lila believe the violin case contains her valuable violin or a beginner's violin?\\nChoose one of the following:\\na)Lila believes the violin case contains a beginner's violin.\\nb)Lila believes the violin case contains her valuable violin.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Lila picks up the case assuming it\u2019s hers and doesn\u2019t realize it was swapped, so she believes it holds her valuable violin.  \nAnswer: b) Lila believes the violin case contains her valuable violin."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Lila is a talented violinist who lives in a bustling city in India. Today, she has an important audition for a renowned symphony orchestra. Lila picks up her violin case, which she assumes contains her valuable violin. However, her younger brother, who was playing around earlier, accidentally swapped her violin case with his identical-looking case containing a beginner's violin. Lila goes on stage with her violin case.\nQuestion: Does Lila believe the violin case contains her valuable violin or a beginner's violin?\nChoose one of the following:\na)Lila believes the violin case contains a beginner's violin.\nb)Lila believes the violin case contains her valuable violin.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Lila picks up the case assuming it\u2019s hers and doesn\u2019t realize it was swapped, so she believes it holds her valuable violin.  \nAnswer: b) Lila believes the violin case contains her valuable violin."
    },
    {
        "continuation": {
            "0-214": "ToM",
            "216-277": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 79,
        "prompt_tokens": 767,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Amara is a sculptor in a small town in Greece. She wants to carve a statue of Aphrodite using a large piece of marble. Amara selects a slab of marble from her workshop, which she thinks is free from cracks and imperfections. However, a hairline crack is present in the marble slab, which goes unnoticed by her. Amara begins carving the statue of Aphrodite using the chosen marble slab.\\nQuestion: Does Amara believe the marble slab is free from cracks or has a hairline crack?\\nChoose one of the following:\\na)Amara believes the marble slab is free from cracks.\\nb)Amara believes the marble slab has a hairline crack.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Amara selects a slab of marble which she thinks is free from cracks and imperfections. The hairline crack is present but goes unnoticed by her, so she does not believe it has a crack.\n\nAnswer:a) Amara believes the marble slab is free from cracks."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Amara is a sculptor in a small town in Greece. She wants to carve a statue of Aphrodite using a large piece of marble. Amara selects a slab of marble from her workshop, which she thinks is free from cracks and imperfections. However, a hairline crack is present in the marble slab, which goes unnoticed by her. Amara begins carving the statue of Aphrodite using the chosen marble slab.\nQuestion: Does Amara believe the marble slab is free from cracks or has a hairline crack?\nChoose one of the following:\na)Amara believes the marble slab is free from cracks.\nb)Amara believes the marble slab has a hairline crack.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Amara selects a slab of marble which she thinks is free from cracks and imperfections. The hairline crack is present but goes unnoticed by her, so she does not believe it has a crack.\n\nAnswer:a) Amara believes the marble slab is free from cracks."
    },
    {
        "continuation": {
            "0-87": "DMN",
            "88-186": "ToM",
            "189-255": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 81,
        "prompt_tokens": 761,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Mei, a talented chef in a small Chinese village, is preparing a special dish for a visiting dignitary. Mei wants to use a particular wok that evenly distributes heat for perfect cooking. Mei sees the wok hanging in its usual spot, appearing clean and ready for use. A mischievous cat knocks over a jar of chili oil, spilling it into the wok. Mei starts cooking the special dish using the wok.\\nQuestion: Does Mei believe the wok is clean and in excellent condition or filled with spilled chili oil?\\nChoose one of the following:\\na)Mei believes the wok is filled with spilled chili oil.\\nb)Mei believes the wok is clean and in excellent condition.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Mei sees the wok hanging in its usual spot, appearing clean and ready for use. She has not noticed the spilled chili oil, so she believes it is clean and in excellent condition.  \nAnswer:b)Mei believes the wok is clean and in excellent condition."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Mei, a talented chef in a small Chinese village, is preparing a special dish for a visiting dignitary. Mei wants to use a particular wok that evenly distributes heat for perfect cooking. Mei sees the wok hanging in its usual spot, appearing clean and ready for use. A mischievous cat knocks over a jar of chili oil, spilling it into the wok. Mei starts cooking the special dish using the wok.\nQuestion: Does Mei believe the wok is clean and in excellent condition or filled with spilled chili oil?\nChoose one of the following:\na)Mei believes the wok is filled with spilled chili oil.\nb)Mei believes the wok is clean and in excellent condition.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Mei sees the wok hanging in its usual spot, appearing clean and ready for use. She has not noticed the spilled chili oil, so she believes it is clean and in excellent condition.  \nAnswer:b)Mei believes the wok is clean and in excellent condition."
    },
    {
        "continuation": {
            "0-231": "ToM",
            "234-299": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 87,
        "prompt_tokens": 792,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Jia is a talented baker in a small Chinese village, preparing to bake a large batch of mooncakes for the upcoming Mid-Autumn Festival. Jia wants to use her favorite wooden mold to shape the mooncakes perfectly. Jia finds the wooden mold on a shelf in her kitchen, appearing clean and ready for use. Suddenly, a gust of wind from an open window blows a layer of dust onto the mold, covering it in a fine layer of grime. Jia proceeds to shape the mooncakes with the mold.\\nQuestion: Does Jia believe the wooden mold is clean and ready for use or covered in dust?\\nChoose one of the following:\\na)Jia believes the wooden mold is covered in dust.\\nb)Jia believes the wooden mold is clean and ready for use.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story says Jia finds the mold \u201cappearing clean and ready for use,\u201d then a gust of wind blows dust on it without mentioning that Jia notices or cleans it. Since she goes ahead and uses it, she still believes it\u2019s clean.  \nAnswer:b)Jia believes the wooden mold is clean and ready for use."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Jia is a talented baker in a small Chinese village, preparing to bake a large batch of mooncakes for the upcoming Mid-Autumn Festival. Jia wants to use her favorite wooden mold to shape the mooncakes perfectly. Jia finds the wooden mold on a shelf in her kitchen, appearing clean and ready for use. Suddenly, a gust of wind from an open window blows a layer of dust onto the mold, covering it in a fine layer of grime. Jia proceeds to shape the mooncakes with the mold.\nQuestion: Does Jia believe the wooden mold is clean and ready for use or covered in dust?\nChoose one of the following:\na)Jia believes the wooden mold is covered in dust.\nb)Jia believes the wooden mold is clean and ready for use.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story says Jia finds the mold \u201cappearing clean and ready for use,\u201d then a gust of wind blows dust on it without mentioning that Jia notices or cleans it. Since she goes ahead and uses it, she still believes it\u2019s clean.  \nAnswer:b)Jia believes the wooden mold is clean and ready for use."
    },
    {
        "continuation": {
            "0-143": "ToM",
            "146-218": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 66,
        "prompt_tokens": 738,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Amara is a farmer in a rural Indian village. She wishes to water her crops today to ensure their growth and health. She notices that her irrigation system is functioning properly. Suddenly, a powerful gust of wind snaps a tree branch, which falls and crushes part of the irrigation system. Amara waters her crops using the irrigation system.\\nQuestion: Does Amara believe her irrigation system is functioning properly or damaged by the fallen tree branch?\\nChoose one of the following:\\na)Amara believes her irrigation system is damaged by the fallen tree branch.\\nb)Amara believes her irrigation system is functioning properly.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: After the branch crushes part of the system, Amara still proceeds to water her crops using it, so she must believe it\u2019s still working.  \nAnswer: b) Amara believes her irrigation system is functioning properly."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Amara is a farmer in a rural Indian village. She wishes to water her crops today to ensure their growth and health. She notices that her irrigation system is functioning properly. Suddenly, a powerful gust of wind snaps a tree branch, which falls and crushes part of the irrigation system. Amara waters her crops using the irrigation system.\nQuestion: Does Amara believe her irrigation system is functioning properly or damaged by the fallen tree branch?\nChoose one of the following:\na)Amara believes her irrigation system is damaged by the fallen tree branch.\nb)Amara believes her irrigation system is functioning properly.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: After the branch crushes part of the system, Amara still proceeds to water her crops using it, so she must believe it\u2019s still working.  \nAnswer: b) Amara believes her irrigation system is functioning properly."
    },
    {
        "continuation": {
            "0-149": "ToM",
            "152-210": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 68,
        "prompt_tokens": 760,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Fatima is a beekeeper in a remote Moroccan village, checking on her honeycombs. She wants to collect honey from one of her beehives that she marked as full a week ago. Fatima sees that the honeycomb appears to have plenty of honey. Overnight, a group of honey badgers breaks into the beehive and eats most of the honey, leaving it nearly empty. Fatima prepares her tools to collect the honey from the full honeycomb.\\nQuestion: Does Fatima believe the honeycomb is full of honey or nearly empty?\\nChoose one of the following:\\na)Fatima believes the honeycomb is nearly empty.\\nb)Fatima believes the honeycomb is full of honey.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Fatima marked the hive as full and prepares her tools assuming it\u2019s still full. She doesn\u2019t know the badgers have eaten the honey overnight.  \nAnswer: b) Fatima believes the honeycomb is full of honey."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Fatima is a beekeeper in a remote Moroccan village, checking on her honeycombs. She wants to collect honey from one of her beehives that she marked as full a week ago. Fatima sees that the honeycomb appears to have plenty of honey. Overnight, a group of honey badgers breaks into the beehive and eats most of the honey, leaving it nearly empty. Fatima prepares her tools to collect the honey from the full honeycomb.\nQuestion: Does Fatima believe the honeycomb is full of honey or nearly empty?\nChoose one of the following:\na)Fatima believes the honeycomb is nearly empty.\nb)Fatima believes the honeycomb is full of honey.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Fatima marked the hive as full and prepares her tools assuming it\u2019s still full. She doesn\u2019t know the badgers have eaten the honey overnight.  \nAnswer: b) Fatima believes the honeycomb is full of honey."
    },
    {
        "continuation": {
            "0-108": "DMN",
            "109-175": "ToM",
            "176-229": "ToM",
            "232-292": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 99,
        "prompt_tokens": 768,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kofi is at the town market in search of fresh fruits and vegetables. Kofi wants to find a ripe watermelon to take home and share with his family. Kofi sees that the watermelon he picks up has a deep, hollow sound when he taps it. When Kofi is not looking, a small child accidentally drops the watermelon, causing it to crack and become damaged. Kofi purchases the watermelon he initially picked.\\nQuestion: Does Kofi believe the watermelon is ripe and undamaged or cracked and damaged?\\nChoose one of the following:\\na)Kofi believes the watermelon is cracked and damaged.\\nb)Kofi believes the watermelon is ripe and undamaged.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kofi taps the watermelon, hears a deep, hollow sound, which signals ripeness and no visible damage. He doesn\u2019t see the child drop it and thus is unaware of the crack. Therefore he still believes it is ripe and undamaged.  \nAnswer:b)Kofi believes the watermelon is ripe and undamaged."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kofi is at the town market in search of fresh fruits and vegetables. Kofi wants to find a ripe watermelon to take home and share with his family. Kofi sees that the watermelon he picks up has a deep, hollow sound when he taps it. When Kofi is not looking, a small child accidentally drops the watermelon, causing it to crack and become damaged. Kofi purchases the watermelon he initially picked.\nQuestion: Does Kofi believe the watermelon is ripe and undamaged or cracked and damaged?\nChoose one of the following:\na)Kofi believes the watermelon is cracked and damaged.\nb)Kofi believes the watermelon is ripe and undamaged.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kofi taps the watermelon, hears a deep, hollow sound, which signals ripeness and no visible damage. He doesn\u2019t see the child drop it and thus is unaware of the crack. Therefore he still believes it is ripe and undamaged.  \nAnswer:b)Kofi believes the watermelon is ripe and undamaged."
    },
    {
        "continuation": {
            "0-131": "DMN",
            "132-179": "ToM",
            "182-240": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 84,
        "prompt_tokens": 780,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kofi is walking through a bustling marketplace in Accra, Ghana. He is searching for a rare, valuable coin to add to his collection. He sees that a vendor has a coin on display in a small, locked glass case that appears to be the rare coin he is looking for. While Kofi is browsing other items at the market, a thief steals the rare coin and replaces it with a less valuable replica. Kofi asks the vendor about the price and history of the rare coin.\\nQuestion: Does Kofi believe the coin is a rare, valuable one, or a less valuable replica?\\nChoose one of the following:\\na)Kofi believes the coin is a less valuable replica.\\nb)Kofi believes the coin is a rare, valuable one.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kofi sees the coin in the locked display and wants a rare valuable coin; he doesn\u2019t know it\u2019s been swapped with a replica. He still believes it\u2019s the rare, valuable coin.  \nAnswer: b) Kofi believes the coin is a rare, valuable one."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kofi is walking through a bustling marketplace in Accra, Ghana. He is searching for a rare, valuable coin to add to his collection. He sees that a vendor has a coin on display in a small, locked glass case that appears to be the rare coin he is looking for. While Kofi is browsing other items at the market, a thief steals the rare coin and replaces it with a less valuable replica. Kofi asks the vendor about the price and history of the rare coin.\nQuestion: Does Kofi believe the coin is a rare, valuable one, or a less valuable replica?\nChoose one of the following:\na)Kofi believes the coin is a less valuable replica.\nb)Kofi believes the coin is a rare, valuable one.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kofi sees the coin in the locked display and wants a rare valuable coin; he doesn\u2019t know it\u2019s been swapped with a replica. He still believes it\u2019s the rare, valuable coin.  \nAnswer: b) Kofi believes the coin is a rare, valuable one."
    },
    {
        "continuation": {
            "0-140": "ToM",
            "143-199": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 66,
        "prompt_tokens": 769,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Priya is a jewelry maker in a small village in India, working on a delicate silver necklace. She wants to complete the necklace by attaching a precious blue gemstone to it. Priya sees that the silver necklace is lying on her workbench, still unadorned. While Priya takes a break for lunch, a mischievous monkey enters her workshop, grabs the necklace, and adds the blue gemstone to it as a prank. Priya plans to attach the gemstone to the necklace herself.\\nQuestion: Does Priya believe the necklace is adorned with the gemstone or still unadorned?\\nChoose one of the following:\\na)Priya believes the necklace is still unadorned.\\nb)Priya believes the necklace is adorned with the gemstone.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Priya did not see the monkey add the gemstone and still plans to attach it herself, so she believes the necklace remains unadorned.  \nAnswer:a)Priya believes the necklace is still unadorned."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Priya is a jewelry maker in a small village in India, working on a delicate silver necklace. She wants to complete the necklace by attaching a precious blue gemstone to it. Priya sees that the silver necklace is lying on her workbench, still unadorned. While Priya takes a break for lunch, a mischievous monkey enters her workshop, grabs the necklace, and adds the blue gemstone to it as a prank. Priya plans to attach the gemstone to the necklace herself.\nQuestion: Does Priya believe the necklace is adorned with the gemstone or still unadorned?\nChoose one of the following:\na)Priya believes the necklace is still unadorned.\nb)Priya believes the necklace is adorned with the gemstone.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Priya did not see the monkey add the gemstone and still plans to attach it herself, so she believes the necklace remains unadorned.  \nAnswer:a)Priya believes the necklace is still unadorned."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-186": "ToM",
            "189-196": "LN",
            "196-243": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 93,
        "prompt_tokens": 760,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Nkiru is a young fisherman in a Nigerian coastal village. She plans to head out for a day of fishing in her wooden boat. Nkiry had tied the boat to the dock the previous day. Overnight, a group of mischievous children played near the dock and accidentally untied the boat, causing it to drift away. Nkiru gets her fishing gear ready and walks toward the dock.\\nQuestion: Does Nkiru believe her boat is tied to the dock or drifting away?\\nChoose one of the following:\\na)Nkiru believes her boat is drifting away.\\nb)Nkiru believes her boat is tied to the dock.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Nkiru tied her boat to the dock the previous day and hasn\u2019t seen it drift away. She is unaware that children untied it overnight, so she still expects it to be tied to the dock.  \nAnswer:b) Nkiru believes her boat is tied to the dock."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Nkiru is a young fisherman in a Nigerian coastal village. She plans to head out for a day of fishing in her wooden boat. Nkiry had tied the boat to the dock the previous day. Overnight, a group of mischievous children played near the dock and accidentally untied the boat, causing it to drift away. Nkiru gets her fishing gear ready and walks toward the dock.\nQuestion: Does Nkiru believe her boat is tied to the dock or drifting away?\nChoose one of the following:\na)Nkiru believes her boat is drifting away.\nb)Nkiru believes her boat is tied to the dock.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Nkiru tied her boat to the dock the previous day and hasn\u2019t seen it drift away. She is unaware that children untied it overnight, so she still expects it to be tied to the dock.  \nAnswer:b) Nkiru believes her boat is tied to the dock."
    },
    {
        "continuation": {
            "0-79": "ToM",
            "80-152": "ToM",
            "153-222": "ToM",
            "225-273": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 98,
        "prompt_tokens": 761,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Tariq is a fisherman in a coastal village in Indonesia. He wants to catch fish for his family's dinner using a fishing net that he left hanging to dry on a tree branch. Tariq sees tat the net appears to be dry and ready to use. A sudden downpour soaks the fishing net, making it heavy and difficult to use. Tariq prepares to take the dry net and head to the shore.\\nQuestion: Does Tariq believe the fishing net is dry or soaked?\\nChoose one of the following:\\na)Tariq believes the fishing net is soaked.\\nb)Tariq believes the fishing net is dry.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Tariq last observed the net hanging and believed it was dry and ready. The subsequent downpour soaks the net, but he is unaware of this change. He still prepares to take what he believes is a dry net to the shore.  \nAnswer: b)Tariq believes the fishing net is dry."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Tariq is a fisherman in a coastal village in Indonesia. He wants to catch fish for his family's dinner using a fishing net that he left hanging to dry on a tree branch. Tariq sees tat the net appears to be dry and ready to use. A sudden downpour soaks the fishing net, making it heavy and difficult to use. Tariq prepares to take the dry net and head to the shore.\nQuestion: Does Tariq believe the fishing net is dry or soaked?\nChoose one of the following:\na)Tariq believes the fishing net is soaked.\nb)Tariq believes the fishing net is dry.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Tariq last observed the net hanging and believed it was dry and ready. The subsequent downpour soaks the net, but he is unaware of this change. He still prepares to take what he believes is a dry net to the shore.  \nAnswer: b)Tariq believes the fishing net is dry."
    },
    {
        "continuation": {
            "0-89": "DMN",
            "90-191": "ToM",
            "193-241": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 74,
        "prompt_tokens": 724,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Zara is a talented chef in a bustling Moroccan market. She desires to prepare a delicious tagine for her family's dinner tonight. Zara bought fresh vegetables from the market earlier in the day. While Zara was away, a heatwave struck the market, causing the vegetables to wilt. Zara starts preparing the tagine with the vegetables she bought earlier.\\nQuestion: Does Zara believe the vegetables are fresh or wilted?\\nChoose one of the following:\\na)Zara believes the vegetables are wilted.\\nb)Zara believes the vegetables are fresh.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Zara purchased the vegetables fresh and has not checked them since the heatwave. She begins cooking as if they are still in good condition, indicating she believes they remain fresh.\n\nAnswer:b)Zara believes the vegetables are fresh."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Zara is a talented chef in a bustling Moroccan market. She desires to prepare a delicious tagine for her family's dinner tonight. Zara bought fresh vegetables from the market earlier in the day. While Zara was away, a heatwave struck the market, causing the vegetables to wilt. Zara starts preparing the tagine with the vegetables she bought earlier.\nQuestion: Does Zara believe the vegetables are fresh or wilted?\nChoose one of the following:\na)Zara believes the vegetables are wilted.\nb)Zara believes the vegetables are fresh.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Zara purchased the vegetables fresh and has not checked them since the heatwave. She begins cooking as if they are still in good condition, indicating she believes they remain fresh.\n\nAnswer:b)Zara believes the vegetables are fresh."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-242": "ToM",
            "245-252": "LN",
            "252-308": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 97,
        "prompt_tokens": 771,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Zara is an archaeologist exploring an ancient temple in Indonesia. She wants to find a hidden chamber within the temple that holds valuable artifacts. She sees that the entrance to the chamber is concealed by a large stone slab. While Zara is examining another part of the temple, a small earthquake causes the stone slab to move, revealing the entrance to the chamber. Zara searches for a mechanism to move the stone slab and reveal the entrance.\\nQuestion: Does Zara believe the stone slab is covering the entrance or has moved to reveal it?\\nChoose one of the following:\\na)Zara believes the stone slab has moved to reveal the entrance.\\nb)Zara believes the stone slab is covering the entrance.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story indicates that Zara saw the slab concealing the entrance, and even after the earthquake revealed it, she looks for a mechanism to move the slab so it would reveal the entrance (implying she believes it covers the entrance).  \nAnswer:b)Zara believes the stone slab is covering the entrance."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Zara is an archaeologist exploring an ancient temple in Indonesia. She wants to find a hidden chamber within the temple that holds valuable artifacts. She sees that the entrance to the chamber is concealed by a large stone slab. While Zara is examining another part of the temple, a small earthquake causes the stone slab to move, revealing the entrance to the chamber. Zara searches for a mechanism to move the stone slab and reveal the entrance.\nQuestion: Does Zara believe the stone slab is covering the entrance or has moved to reveal it?\nChoose one of the following:\na)Zara believes the stone slab has moved to reveal the entrance.\nb)Zara believes the stone slab is covering the entrance.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story indicates that Zara saw the slab concealing the entrance, and even after the earthquake revealed it, she looks for a mechanism to move the slab so it would reveal the entrance (implying she believes it covers the entrance).  \nAnswer:b)Zara believes the stone slab is covering the entrance."
    },
    {
        "continuation": {
            "0-133": "ToM",
            "136-192": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 60,
        "prompt_tokens": 741,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Ravi is a fisherman in a coastal town in India, preparing his boat for a day of fishing. He plans to catch as many fish as possible to sell at the market. Ravi's boat has a small hole in its hull, which he secured the day before. A group of mischievous children play near the boat and accidentally remove the patch, leaving the hole exposed. Ravi sets sail to begin his fishing expedition.\\nQuestion: Does Ravi believe the hole in his boat is patched or exposed?\\nChoose one of the following:\\na)Ravi believes the hole in his boat is exposed.\\nb)Ravi believes the hole in his boat is patched.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Ravi secured the hole the day before and, unaware that the children removed the patch, he still thinks the repair is intact.  \nAnswer:b) Ravi believes the hole in his boat is patched."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Ravi is a fisherman in a coastal town in India, preparing his boat for a day of fishing. He plans to catch as many fish as possible to sell at the market. Ravi's boat has a small hole in its hull, which he secured the day before. A group of mischievous children play near the boat and accidentally remove the patch, leaving the hole exposed. Ravi sets sail to begin his fishing expedition.\nQuestion: Does Ravi believe the hole in his boat is patched or exposed?\nChoose one of the following:\na)Ravi believes the hole in his boat is exposed.\nb)Ravi believes the hole in his boat is patched.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Ravi secured the hole the day before and, unaware that the children removed the patch, he still thinks the repair is intact.  \nAnswer:b) Ravi believes the hole in his boat is patched."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-172": "ToM",
            "174-181": "LN",
            "181-232": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 94,
        "prompt_tokens": 754,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Rashida is a professional baker, preparing for a prestigious baking competition. She wants to create a perfect chocolate souffl\u00e9 for the judges. Rashida stes her oven to preheat at 350\u00b0F. However, a power outage occurs and the oven's temperature drops rapidly. Rashida places the souffl\u00e9 in the oven to bake.\\nQuestion: Does Rashida believe her oven is preheating at 350\u00b0F or not heating at all?\\nChoose one of the following:\\na)Rashida believes her oven is not heating at all.\\nb)Rashida believes her oven is preheating at 350\u00b0F.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Rashida set the oven to preheat at 350\u00b0F and, unaware of the power outage, she believes it is preheating as intended rather than realizing it isn\u2019t heating at all.\n\nAnswer:b)Rashida believes her oven is preheating at 350\u00b0F."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Rashida is a professional baker, preparing for a prestigious baking competition. She wants to create a perfect chocolate souffl\u00e9 for the judges. Rashida stes her oven to preheat at 350\u00b0F. However, a power outage occurs and the oven's temperature drops rapidly. Rashida places the souffl\u00e9 in the oven to bake.\nQuestion: Does Rashida believe her oven is preheating at 350\u00b0F or not heating at all?\nChoose one of the following:\na)Rashida believes her oven is not heating at all.\nb)Rashida believes her oven is preheating at 350\u00b0F.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Rashida set the oven to preheat at 350\u00b0F and, unaware of the power outage, she believes it is preheating as intended rather than realizing it isn\u2019t heating at all.\n\nAnswer:b)Rashida believes her oven is preheating at 350\u00b0F."
    },
    {
        "continuation": {
            "0-146": "ToM",
            "149-206": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 64,
        "prompt_tokens": 740,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Imelda is walking through a beautiful garden on a sunny day. Imelda wants to find a rare and exotic flower to photograph for her nature blog. She spots a rare and exotic flower with delicate petals and an unusual shape. A gardener, working nearby, replaces the flower with a common, yet visually similar, flower while Imelda is momentarily distracted. Imelda takes a photograph of the flower she initially spotted.\\nQuestion: Does Imelda believe the flower is rare and exotic or common?\\nChoose one of the following:\\na)Imelda believes the flower is common.\\nb)Imelda believes the flower is rare and exotic.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Imelda saw the rare exotic flower and, unaware that it was swapped for a common one, believes she is photographing the rare exotic bloom.  \nAnswer: b) Imelda believes the flower is rare and exotic."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Imelda is walking through a beautiful garden on a sunny day. Imelda wants to find a rare and exotic flower to photograph for her nature blog. She spots a rare and exotic flower with delicate petals and an unusual shape. A gardener, working nearby, replaces the flower with a common, yet visually similar, flower while Imelda is momentarily distracted. Imelda takes a photograph of the flower she initially spotted.\nQuestion: Does Imelda believe the flower is rare and exotic or common?\nChoose one of the following:\na)Imelda believes the flower is common.\nb)Imelda believes the flower is rare and exotic.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Imelda saw the rare exotic flower and, unaware that it was swapped for a common one, believes she is photographing the rare exotic bloom.  \nAnswer: b) Imelda believes the flower is rare and exotic."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-138": "DMN",
            "139-184": "ToM",
            "187-247": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 88,
        "prompt_tokens": 752,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Bianca is a marine biologist studying coral reefs in the Caribbean. She wants to collect samples of a specific type of coral that appears healthy and vibrant. The coral she observes is a dull, pale color. A school of fish, rich in nutrients, swims by and deposits nutrients on the coral, causing it to regain its vibrant colors. Bianca searches for a healthier coral specimen to collect samples from.\\nQuestion: Does Bianca believe the coral is in a weakened state or a healthy state?\\nChoose one of the following:\\na)Bianca believes the coral is in a weakened state.\\nb)Bianca believes the coral is in a healthy state.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Bianca is looking for healthy, vibrant coral to sample, but observes that the coral is dull and pale\u2014signs of weakness or stress. Hence she believes it is in a weakened state.  \nAnswer: a) Bianca believes the coral is in a weakened state."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Bianca is a marine biologist studying coral reefs in the Caribbean. She wants to collect samples of a specific type of coral that appears healthy and vibrant. The coral she observes is a dull, pale color. A school of fish, rich in nutrients, swims by and deposits nutrients on the coral, causing it to regain its vibrant colors. Bianca searches for a healthier coral specimen to collect samples from.\nQuestion: Does Bianca believe the coral is in a weakened state or a healthy state?\nChoose one of the following:\na)Bianca believes the coral is in a weakened state.\nb)Bianca believes the coral is in a healthy state.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Bianca is looking for healthy, vibrant coral to sample, but observes that the coral is dull and pale\u2014signs of weakness or stress. Hence she believes it is in a weakened state.  \nAnswer: a) Bianca believes the coral is in a weakened state."
    },
    {
        "continuation": {
            "0-116": "ToM",
            "117-177": "ToM",
            "179-237": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 72,
        "prompt_tokens": 734,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Leonardo Martinez is strolling through a public park on a sunny day. Leonardo wants to relax in the shade of a large tree to read a book. Leonardo sees that the tree appears to have sturdy branches that could support his weight. A strong gust of wind blows through the park, causing one of the branches to snap and fall to the ground. Leonardo climbs the tree to find a comfortable spot.\\nQuestion: Does Leonardo believe the tree has sturdy branches or fragile branches?\\nChoose one of the following:\\na)Leonardo believes the tree has fragile branches.\\nb)Leonardo believes the tree has sturdy branches.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Leonardo sees the tree appears to have sturdy branches that could support his weight. Therefore, he believes the branches are sturdy, not fragile.\n\nAnswer: b) Leonardo believes the tree has sturdy branches."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Leonardo Martinez is strolling through a public park on a sunny day. Leonardo wants to relax in the shade of a large tree to read a book. Leonardo sees that the tree appears to have sturdy branches that could support his weight. A strong gust of wind blows through the park, causing one of the branches to snap and fall to the ground. Leonardo climbs the tree to find a comfortable spot.\nQuestion: Does Leonardo believe the tree has sturdy branches or fragile branches?\nChoose one of the following:\na)Leonardo believes the tree has fragile branches.\nb)Leonardo believes the tree has sturdy branches.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Leonardo sees the tree appears to have sturdy branches that could support his weight. Therefore, he believes the branches are sturdy, not fragile.\n\nAnswer: b) Leonardo believes the tree has sturdy branches."
    },
    {
        "continuation": {
            "0-56": "DMN",
            "57-230": "ToM",
            "233-305": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 90,
        "prompt_tokens": 778,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Olga is a botanist exploring a remote rainforest in search of a rare orchid species. Olga wants to find and document the orchid to contribute to her research on endangered plants. She sees that the map she carries with her indicates that the orchid is located in a specific area of the rainforest. A sudden landslide alters the rainforest landscape, moving the orchid to a different location. Olga continues her search in the area indicated on the map.\\nQuestion: Does Olga believe the rare orchid is in the area shown on the map or in the affected area?\\nChoose one of the following:\\na)Olga believes the rare orchid is in the affected area.\\nb)Olga believes the rare orchid is in the area shown on the map.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Olga\u2019s map shows the orchid in a specific spot. After the landslide moves the orchid, Olga is unaware of the change and continues searching the original map location, so she believes the orchid is where the map indicates.  \nAnswer:b) Olga believes the rare orchid is in the area shown on the map."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Olga is a botanist exploring a remote rainforest in search of a rare orchid species. Olga wants to find and document the orchid to contribute to her research on endangered plants. She sees that the map she carries with her indicates that the orchid is located in a specific area of the rainforest. A sudden landslide alters the rainforest landscape, moving the orchid to a different location. Olga continues her search in the area indicated on the map.\nQuestion: Does Olga believe the rare orchid is in the area shown on the map or in the affected area?\nChoose one of the following:\na)Olga believes the rare orchid is in the affected area.\nb)Olga believes the rare orchid is in the area shown on the map.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Olga\u2019s map shows the orchid in a specific spot. After the landslide moves the orchid, Olga is unaware of the change and continues searching the original map location, so she believes the orchid is where the map indicates.  \nAnswer:b) Olga believes the rare orchid is in the area shown on the map."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-211": "ToM",
            "214-221": "LN",
            "221-269": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 89,
        "prompt_tokens": 753,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Xiomara is a marine biologist studying coral reefs in the Caribbean. Her goal is to collect samples of healthy coral for her research. Xiomara spots a vibrant, healthy coral colony in the reef. Suddenly, an underwater landslide occurs, burying the healthy coral colony under a thick layer of sediment. Xiomara prepares her equipment to collect a sample from the healthy coral colony.\\nQuestion: Does Xiomara believe the coral colony is healthy or buried under sediment?\\nChoose one of the following:\\na)Xiomara believes the coral colony is healthy.\\nb)Xiomara believes the coral colony is buried under sediment.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Xiomara initially spots a vibrant, healthy coral colony and even after the landslide buries it, she still prepares to collect a sample from the healthy coral, indicating she believes it remains healthy.  \nAnswer:a) Xiomara believes the coral colony is healthy."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Xiomara is a marine biologist studying coral reefs in the Caribbean. Her goal is to collect samples of healthy coral for her research. Xiomara spots a vibrant, healthy coral colony in the reef. Suddenly, an underwater landslide occurs, burying the healthy coral colony under a thick layer of sediment. Xiomara prepares her equipment to collect a sample from the healthy coral colony.\nQuestion: Does Xiomara believe the coral colony is healthy or buried under sediment?\nChoose one of the following:\na)Xiomara believes the coral colony is healthy.\nb)Xiomara believes the coral colony is buried under sediment.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Xiomara initially spots a vibrant, healthy coral colony and even after the landslide buries it, she still prepares to collect a sample from the healthy coral, indicating she believes it remains healthy.  \nAnswer:a) Xiomara believes the coral colony is healthy."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-100": "DMN",
            "101-163": "ToM",
            "164-214": "ToM",
            "216-265": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 92,
        "prompt_tokens": 735,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Janelle is a marine biologist studying coral reefs in the Pacific Ocean. She wants to collect samples of healthy coral for her research. The coral she observed yesterday appeared to be thriving. Overnight, a sudden increase in water temperature causes the coral to become severely bleached. Janelle plans to collect samples of the healthy coral for her research.\\nQuestion: Does Janelle believe the coral is healthy or bleached?\\nChoose one of the following:\\na)Janelle believes the coral is bleached.\\nb)Janelle believes the coral is healthy.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Janelle observed the coral yesterday as healthy and is planning to collect healthy samples. There\u2019s no indication she knows about the overnight bleaching. Therefore she still believes the coral is healthy.\n\nAnswer: b) Janelle believes the coral is healthy."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Janelle is a marine biologist studying coral reefs in the Pacific Ocean. She wants to collect samples of healthy coral for her research. The coral she observed yesterday appeared to be thriving. Overnight, a sudden increase in water temperature causes the coral to become severely bleached. Janelle plans to collect samples of the healthy coral for her research.\nQuestion: Does Janelle believe the coral is healthy or bleached?\nChoose one of the following:\na)Janelle believes the coral is bleached.\nb)Janelle believes the coral is healthy.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Janelle observed the coral yesterday as healthy and is planning to collect healthy samples. There\u2019s no indication she knows about the overnight bleaching. Therefore she still believes the coral is healthy.\n\nAnswer: b) Janelle believes the coral is healthy."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-128": "DMN",
            "129-224": "ToM",
            "227-280": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 91,
        "prompt_tokens": 754,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Priya is at a botanical garden, exploring various exotic plants. Priya wants to take a picture of a rare orchid she has been searching for. Priya sees that the orchid is in full bloom, displaying its vibrant colors. A sudden gust of wind blows through the garden, causing the orchid's petals to fall off, leaving it bare. Priya prepares her camera to take a picture of the blooming orchid.\\nQuestion: Does Priya believe the orchid is in full bloom or bare?\\nChoose one of the following:\\na)Priya believes the orchid is bare.\\nb)Priya believes the orchid is in full bloom.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Priya saw the orchid in full bloom and then immediately prepared her camera under the assumption it was still blooming. She did not update her belief after the petals fell, so she still believes it is in full bloom.  \nAnswer:b) Priya believes the orchid is in full bloom."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Priya is at a botanical garden, exploring various exotic plants. Priya wants to take a picture of a rare orchid she has been searching for. Priya sees that the orchid is in full bloom, displaying its vibrant colors. A sudden gust of wind blows through the garden, causing the orchid's petals to fall off, leaving it bare. Priya prepares her camera to take a picture of the blooming orchid.\nQuestion: Does Priya believe the orchid is in full bloom or bare?\nChoose one of the following:\na)Priya believes the orchid is bare.\nb)Priya believes the orchid is in full bloom.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Priya saw the orchid in full bloom and then immediately prepared her camera under the assumption it was still blooming. She did not update her belief after the petals fell, so she still believes it is in full bloom.  \nAnswer:b) Priya believes the orchid is in full bloom."
    },
    {
        "continuation": {
            "0-53": "ToM",
            "54-101": "ToM",
            "104-159": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 66,
        "prompt_tokens": 749,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Ulysses is hiking in the mountains on a sunny day. Ulysses wants to take a photo of a rare bird perched on a tree nearby. He sees that the bird appears to be calm and still, sitting on a branch. Suddenly, a gust of wind blows through the area, startling the bird and causing it to fly away. Ulysses adjusts his camera settings to take a clear photo of a still subject, the bird.\\nQuestion: Does Ulysses believe the bird is calm and still or startled and flying away?\\nChoose one of the following:\\na)Ulysses believes the bird is startled and flying away.\\nb)Ulysses believes the bird is calm and still.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Ulysses sets his camera for a still subject, showing he believes the bird is calm and still.  \nAnswer: b) Ulysses believes the bird is calm and still."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Ulysses is hiking in the mountains on a sunny day. Ulysses wants to take a photo of a rare bird perched on a tree nearby. He sees that the bird appears to be calm and still, sitting on a branch. Suddenly, a gust of wind blows through the area, startling the bird and causing it to fly away. Ulysses adjusts his camera settings to take a clear photo of a still subject, the bird.\nQuestion: Does Ulysses believe the bird is calm and still or startled and flying away?\nChoose one of the following:\na)Ulysses believes the bird is startled and flying away.\nb)Ulysses believes the bird is calm and still.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Ulysses sets his camera for a still subject, showing he believes the bird is calm and still.  \nAnswer: b) Ulysses believes the bird is calm and still."
    },
    {
        "continuation": {
            "0-55": "DMN",
            "56-193": "ToM",
            "194-229": "ToM",
            "231-282": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 94,
        "prompt_tokens": 749,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Fatima is an astronaut aboard a space station, conducting scientific experiments. She needs to retrieve a sample from the storage area to conduct a crucial test. Fatima sees that the sample is stored in a sealed container. A small meteoroid hits the space station, puncturing the container and exposing the sample to the vacuum of space. Fatima retrieves the sealed container and proceeds with the experiment.\\nQuestion: Does Fatima believe the container is sealed or punctured?\\nChoose one of the following:\\na)Fatima believes the container is sealed.\\nb)Fatima believes the container is punctured.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Fatima initially sees the container as sealed. The meteoroid punctures it, but there\u2019s no indication she notices the new hole\u2014she still retrieves what she thinks is a sealed container. Therefore she believes it\u2019s sealed.\n\nAnswer: a) Fatima believes the container is sealed."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Fatima is an astronaut aboard a space station, conducting scientific experiments. She needs to retrieve a sample from the storage area to conduct a crucial test. Fatima sees that the sample is stored in a sealed container. A small meteoroid hits the space station, puncturing the container and exposing the sample to the vacuum of space. Fatima retrieves the sealed container and proceeds with the experiment.\nQuestion: Does Fatima believe the container is sealed or punctured?\nChoose one of the following:\na)Fatima believes the container is sealed.\nb)Fatima believes the container is punctured.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Fatima initially sees the container as sealed. The meteoroid punctures it, but there\u2019s no indication she notices the new hole\u2014she still retrieves what she thinks is a sealed container. Therefore she believes it\u2019s sealed.\n\nAnswer: a) Fatima believes the container is sealed."
    },
    {
        "continuation": {
            "0-166": "ToM",
            "169-226": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 68,
        "prompt_tokens": 749,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Daya is a professional baker, working in her small bakery in the heart of Paris. She wants to bake a batch of croissants for the morning rush. Daya sets the oven to a low temperature to preheat. Suddenly, an electrical surge causes the oven to heat up to a very high temperature. Daya prepares to put the croissants in the oven, expecting them to bake evenly.\\nQuestion: Does Daya believe the oven is at a low temperature or a very high temperature?\\nChoose one of the following:\\na)Daya believes the oven is at a low temperature.\\nb)Daya believes the oven is at a very high temperature.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Daya set the oven to a low temperature and, not noticing the electrical surge, continues under the assumption that the oven remains at the setting she chose.  \nAnswer:a) Daya believes the oven is at a low temperature."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Daya is a professional baker, working in her small bakery in the heart of Paris. She wants to bake a batch of croissants for the morning rush. Daya sets the oven to a low temperature to preheat. Suddenly, an electrical surge causes the oven to heat up to a very high temperature. Daya prepares to put the croissants in the oven, expecting them to bake evenly.\nQuestion: Does Daya believe the oven is at a low temperature or a very high temperature?\nChoose one of the following:\na)Daya believes the oven is at a low temperature.\nb)Daya believes the oven is at a very high temperature.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Daya set the oven to a low temperature and, not noticing the electrical surge, continues under the assumption that the oven remains at the setting she chose.  \nAnswer:a) Daya believes the oven is at a low temperature."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-94": "DMN",
            "95-133": "ToM",
            "134-237": "DMN",
            "238-329": "ToM",
            "331-381": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 132,
        "prompt_tokens": 793,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Naveen is at a farmers' market shopping for fresh produce. Naveen wants to buy a ripe watermelon for a picnic later in the day. Naveen hears that the watermelon he picks up has a deep, hollow sound when he taps it. A sudden gust of wind knocks over a stack of watermelons, causing them to mix up, and the ripe watermelon Naveen had chosen gets replaced by an unripe one. Naveen proceeds to purchase the ripe watermelon.\\nQuestion: Does Naveen believe the watermelon is ripe or unripe?\\nChoose one of the following:\\na)Naveen believes the watermelon is unripe.\\nb)Naveen believes the watermelon is ripe.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Naveen taps the watermelon and hears a deep, hollow sound\u2014a typical sign of ripeness. He selects it believing it to be ripe. When the wind knocks over the stack, the ripe melon is swapped for an unripe one without his knowledge. He still thinks the watermelon he\u2019s buying is the ripe one based on the earlier sound test.\n\nAnswer: b) Naveen believes the watermelon is ripe."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Naveen is at a farmers' market shopping for fresh produce. Naveen wants to buy a ripe watermelon for a picnic later in the day. Naveen hears that the watermelon he picks up has a deep, hollow sound when he taps it. A sudden gust of wind knocks over a stack of watermelons, causing them to mix up, and the ripe watermelon Naveen had chosen gets replaced by an unripe one. Naveen proceeds to purchase the ripe watermelon.\nQuestion: Does Naveen believe the watermelon is ripe or unripe?\nChoose one of the following:\na)Naveen believes the watermelon is unripe.\nb)Naveen believes the watermelon is ripe.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Naveen taps the watermelon and hears a deep, hollow sound\u2014a typical sign of ripeness. He selects it believing it to be ripe. When the wind knocks over the stack, the ripe melon is swapped for an unripe one without his knowledge. He still thinks the watermelon he\u2019s buying is the ripe one based on the earlier sound test.\n\nAnswer: b) Naveen believes the watermelon is ripe."
    },
    {
        "continuation": {
            "0-130": "ToM",
            "131-219": "ToM",
            "220-295": "ToM",
            "298-373": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 112,
        "prompt_tokens": 796,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Valentina is exploring a dense forest during her vacation in the mountains. Valentina wants to find a rare blue orchid that is said to grow in this forest. A beautiful blue flower catches her eye, nestled among the foliage. As she walks closer, a gust of wind blows a cluster of leaves from a nearby tree, revealing that the flower is actually a common bluebell. Valentina carefully approaches the flower to take a closer look and possibly collect it.\\nQuestion: Does Valentina believe the flower she spotted is a rare blue orchid or a common bluebell?\\nChoose one of the following:\\na)Valentina believes the flower she spotted is a rare blue orchid.\\nb)Valentina believes the flower she spotted is a common bluebell.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Valentina is searching for a rare blue orchid and initially mistakes the beautiful blue flower she spots for that orchid. Only after a gust of wind moves leaves does she realize it\u2019s actually a common bluebell. Thus, at the time she spotted it, she believed it was the rare blue orchid.  \nAnswer: a) Valentina believes the flower she spotted is a rare blue orchid."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Valentina is exploring a dense forest during her vacation in the mountains. Valentina wants to find a rare blue orchid that is said to grow in this forest. A beautiful blue flower catches her eye, nestled among the foliage. As she walks closer, a gust of wind blows a cluster of leaves from a nearby tree, revealing that the flower is actually a common bluebell. Valentina carefully approaches the flower to take a closer look and possibly collect it.\nQuestion: Does Valentina believe the flower she spotted is a rare blue orchid or a common bluebell?\nChoose one of the following:\na)Valentina believes the flower she spotted is a rare blue orchid.\nb)Valentina believes the flower she spotted is a common bluebell.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Valentina is searching for a rare blue orchid and initially mistakes the beautiful blue flower she spots for that orchid. Only after a gust of wind moves leaves does she realize it\u2019s actually a common bluebell. Thus, at the time she spotted it, she believed it was the rare blue orchid.  \nAnswer: a) Valentina believes the flower she spotted is a rare blue orchid."
    },
    {
        "continuation": {
            "0-114": "DMN",
            "115-233": "ToM",
            "235-282": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 82,
        "prompt_tokens": 735,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Kamila is a marine biologist studying coral reefs in the Maldives. She wants to collect a sample of healthy coral for her research. The coral she observes appears to be vibrant and thriving. Suddenly, an oil spill occurs nearby, contaminating the water and severely damaging the coral. Kamila plans to carefully collect a piece of the healthy coral for her research.\\nQuestion: Does Kamila believe the coral is healthy or damaged?\\nChoose one of the following:\\na)Kamila believes the coral is healthy.\\nb)Kamila believes the coral is damaged.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Kamila is seeking a sample of healthy coral and describes the coral she observes as vibrant and thriving. Despite the oil spill that damages the reef, her intent to collect healthy coral indicates she believes it is healthy.\n\nAnswer:a) Kamila believes the coral is healthy."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Kamila is a marine biologist studying coral reefs in the Maldives. She wants to collect a sample of healthy coral for her research. The coral she observes appears to be vibrant and thriving. Suddenly, an oil spill occurs nearby, contaminating the water and severely damaging the coral. Kamila plans to carefully collect a piece of the healthy coral for her research.\nQuestion: Does Kamila believe the coral is healthy or damaged?\nChoose one of the following:\na)Kamila believes the coral is healthy.\nb)Kamila believes the coral is damaged.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Kamila is seeking a sample of healthy coral and describes the coral she observes as vibrant and thriving. Despite the oil spill that damages the reef, her intent to collect healthy coral indicates she believes it is healthy.\n\nAnswer:a) Kamila believes the coral is healthy."
    },
    {
        "continuation": {
            "0-85": "DMN",
            "86-210": "ToM",
            "211-273": "ToM",
            "275-338": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 103,
        "prompt_tokens": 767,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Luka is at a high-end restaurant, celebrating his promotion with his friends. Luka wants to order a bottle of wine to share with his friends. The wine menu displays a limited edition wine with a high price tag, and Luka reads the menu carefully. A waiter, after checking the wine cellar, realizes that the limited edition wine is actually sold out and updates the wine menu accordingly. Luka decides to order the limited edition wine.\\nQuestion: Does Luka believe the limited edition wine is available or sold out?\\nChoose one of the following:\\na)Luka believes the limited edition wine is sold out.\\nb)Luka believes the limited edition wine is available.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Luka initially reads the menu showing the limited edition wine as available. The waiter later updates the menu to remove that wine, but Luka hasn\u2019t seen this change and still believes it\u2019s on the menu. Since he proceeds to order it, he must believe it\u2019s available.\n\nAnswer: b) Luka believes the limited edition wine is available."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Luka is at a high-end restaurant, celebrating his promotion with his friends. Luka wants to order a bottle of wine to share with his friends. The wine menu displays a limited edition wine with a high price tag, and Luka reads the menu carefully. A waiter, after checking the wine cellar, realizes that the limited edition wine is actually sold out and updates the wine menu accordingly. Luka decides to order the limited edition wine.\nQuestion: Does Luka believe the limited edition wine is available or sold out?\nChoose one of the following:\na)Luka believes the limited edition wine is sold out.\nb)Luka believes the limited edition wine is available.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Luka initially reads the menu showing the limited edition wine as available. The waiter later updates the menu to remove that wine, but Luka hasn\u2019t seen this change and still believes it\u2019s on the menu. Since he proceeds to order it, he must believe it\u2019s available.\n\nAnswer: b) Luka believes the limited edition wine is available."
    },
    {
        "continuation": {
            "0-76": "DMN",
            "77-249": "ToM",
            "250-311": "ToM",
            "313-395": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 106,
        "prompt_tokens": 767,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Delia is a scientist working in a remote research station in Antarctica. She needs to collect important data from a weather monitoring device outside the station. Dalia sees that the device is displaying the correct data. Suddenly, a powerful solar flare disrupts the device's sensors, causing it to display incorrect data. Delia proceeds to collect the data from the device for her research.\\nQuestion: Does Delia believe the weather monitoring device is displaying correct or incorrect data?\\nChoose one of the following:\\na)Delia believes the weather monitoring device is displaying incorrect data.\\nb)Delia believes the weather monitoring device is displaying correct data.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Delia initially checks and sees the device displaying correct data. A solar flare then corrupts the readings, but there's no indication Delia notices this corruption\u2014she proceeds to collect the data under the assumption it remains accurate. Therefore she believes the device is displaying correct data.\n\nAnswer: b)Delia believes the weather monitoring device is displaying correct data."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Delia is a scientist working in a remote research station in Antarctica. She needs to collect important data from a weather monitoring device outside the station. Dalia sees that the device is displaying the correct data. Suddenly, a powerful solar flare disrupts the device's sensors, causing it to display incorrect data. Delia proceeds to collect the data from the device for her research.\nQuestion: Does Delia believe the weather monitoring device is displaying correct or incorrect data?\nChoose one of the following:\na)Delia believes the weather monitoring device is displaying incorrect data.\nb)Delia believes the weather monitoring device is displaying correct data.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Delia initially checks and sees the device displaying correct data. A solar flare then corrupts the readings, but there's no indication Delia notices this corruption\u2014she proceeds to collect the data under the assumption it remains accurate. Therefore she believes the device is displaying correct data.\n\nAnswer: b)Delia believes the weather monitoring device is displaying correct data."
    },
    {
        "continuation": {
            "0-111": "ToM",
            "112-183": "ToM",
            "186-250": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 79,
        "prompt_tokens": 753,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Yasmine is hiking through a dense forest on a warm summer day. Yasmine wants to find a freshwater source to refill her water bottle. She comes across a small stream that appears to be clean, clear and safe to drink. Suddenly, heavy rain begins to pour upstream, causing the stream to become murky and contaminated with dirt and debris. Yasmine fills her water bottle from the stream.\\nQuestion: Does Yasmine believe the stream water is clean and clear or murky and contaminated?\\nChoose one of the following:\\na)Yasmine believes the stream water is murky and contaminated.\\nb)Yasmine believes the stream water is clean and clear.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Yasmine notices the stream as clean and clear and thinks it\u2019s safe to drink when she fills her bottle. She does not update her belief about contamination even after the rain.  \nAnswer: b) Yasmine believes the stream water is clean and clear."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Yasmine is hiking through a dense forest on a warm summer day. Yasmine wants to find a freshwater source to refill her water bottle. She comes across a small stream that appears to be clean, clear and safe to drink. Suddenly, heavy rain begins to pour upstream, causing the stream to become murky and contaminated with dirt and debris. Yasmine fills her water bottle from the stream.\nQuestion: Does Yasmine believe the stream water is clean and clear or murky and contaminated?\nChoose one of the following:\na)Yasmine believes the stream water is murky and contaminated.\nb)Yasmine believes the stream water is clean and clear.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Yasmine notices the stream as clean and clear and thinks it\u2019s safe to drink when she fills her bottle. She does not update her belief about contamination even after the rain.  \nAnswer: b) Yasmine believes the stream water is clean and clear."
    },
    {
        "continuation": {
            "0-83": "DMN",
            "84-167": "ToM",
            "169-225": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 76,
        "prompt_tokens": 742,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Winston is visiting a botanical garden during his vacation. He wants to take a photograph of a rare, blooming flower to share with his friends. Winston sees that the flower appears to be in full bloom, with vibrant colors and smells a captivating scent. Overnight, a sudden cold snap causes the flower to wilt and lose its vibrant colors. Winston plans to return later in the day to take the photograph.\\nQuestion: Does Winston believe the flower is in full bloom or has wilted?\\nChoose one of the following:\\na)Winston believes the flower has wilted.\\nb)Winston believes the flower is in full bloom.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Winston saw the flower in full bloom and plans to return to photograph it. He hasn\u2019t observed the overnight wilting, so he still believes it is in full bloom.\n\nAnswer: b) Winston believes the flower is in full bloom."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Winston is visiting a botanical garden during his vacation. He wants to take a photograph of a rare, blooming flower to share with his friends. Winston sees that the flower appears to be in full bloom, with vibrant colors and smells a captivating scent. Overnight, a sudden cold snap causes the flower to wilt and lose its vibrant colors. Winston plans to return later in the day to take the photograph.\nQuestion: Does Winston believe the flower is in full bloom or has wilted?\nChoose one of the following:\na)Winston believes the flower has wilted.\nb)Winston believes the flower is in full bloom.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Winston saw the flower in full bloom and plans to return to photograph it. He hasn\u2019t observed the overnight wilting, so he still believes it is in full bloom.\n\nAnswer: b) Winston believes the flower is in full bloom."
    },
    {
        "continuation": {
            "0-98": "DMN",
            "99-159": "ToM",
            "160-202": "ToM",
            "205-262": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 94,
        "prompt_tokens": 778,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Tariq is at a farmer's market, planning to buy fresh fruits for a fruit salad he wants to make. He aims to find the ripest and juiciest watermelon available. Tariq spots a watermelon with a deep green color and a hollow sound when tapped, indicating it is ripe. A mischievous child nearby secretly pokes a tiny hole in the watermelon, causing it to lose some of its juice. Tariq picks up the watermelon to buy it.\\nQuestion: Does Tariq believe the watermelon is ripe and juicy or has lost some of its juice?\\nChoose one of the following:\\na)Tariq believes the watermelon is ripe and juicy.\\nb)Tariq believes the watermelon has lost some of its juice.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Tariq inspects the watermelon by color and sound, both indicating ripeness and juiciness. He is unaware of the child\u2019s secret puncture and juice loss, so he still believes it is ripe and juicy.  \nAnswer:a)Tariq believes the watermelon is ripe and juicy."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Tariq is at a farmer's market, planning to buy fresh fruits for a fruit salad he wants to make. He aims to find the ripest and juiciest watermelon available. Tariq spots a watermelon with a deep green color and a hollow sound when tapped, indicating it is ripe. A mischievous child nearby secretly pokes a tiny hole in the watermelon, causing it to lose some of its juice. Tariq picks up the watermelon to buy it.\nQuestion: Does Tariq believe the watermelon is ripe and juicy or has lost some of its juice?\nChoose one of the following:\na)Tariq believes the watermelon is ripe and juicy.\nb)Tariq believes the watermelon has lost some of its juice.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Tariq inspects the watermelon by color and sound, both indicating ripeness and juiciness. He is unaware of the child\u2019s secret puncture and juice loss, so he still believes it is ripe and juicy.  \nAnswer:a)Tariq believes the watermelon is ripe and juicy."
    },
    {
        "continuation": {
            "0-97": "ToM",
            "98-174": "DMN",
            "176-260": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 80,
        "prompt_tokens": 775,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Amara is a botanist exploring a dense rainforest in search of a rare orchid species. Amara's goal is to find the rare orchid and study its unique characteristics. She spots an orchid with vibrant purple petals that she thinks might be the one she is searching for. As she continues to explore, a sudden downpour washes away the purple pigment from the orchid, revealing that it is actually a common white orchid. Amara carefully collects a sample of the purple orchid for further study.\\nQuestion: Does Amara believe the orchid with vibrant purple petals is the rare species or a common white orchid?\\nChoose one of the following:\\na)Amara believes the orchid is a common white orchid.\\nb)Amara believes the orchid with vibrant purple petals is the rare species.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Amara initially spots the vibrant purple orchid and thinks it might be the rare species. She only discovers it\u2019s a common white orchid after the pigment washes away.\n\nAnswer: b) Amara believes the orchid with vibrant purple petals is the rare species."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Amara is a botanist exploring a dense rainforest in search of a rare orchid species. Amara's goal is to find the rare orchid and study its unique characteristics. She spots an orchid with vibrant purple petals that she thinks might be the one she is searching for. As she continues to explore, a sudden downpour washes away the purple pigment from the orchid, revealing that it is actually a common white orchid. Amara carefully collects a sample of the purple orchid for further study.\nQuestion: Does Amara believe the orchid with vibrant purple petals is the rare species or a common white orchid?\nChoose one of the following:\na)Amara believes the orchid is a common white orchid.\nb)Amara believes the orchid with vibrant purple petals is the rare species.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Amara initially spots the vibrant purple orchid and thinks it might be the rare species. She only discovers it\u2019s a common white orchid after the pigment washes away.\n\nAnswer: b) Amara believes the orchid with vibrant purple petals is the rare species."
    },
    {
        "continuation": {
            "0-117": "ToM",
            "118-177": "ToM",
            "180-231": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 72,
        "prompt_tokens": 732,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Haruki is exploring an ancient temple during his vacation in Cambodia. Haruki hopes to find a hidden chamber that contains a valuable artifact. He sees that the temple map he holds shows a secret passage leading to the chamber. A recent earthquake, unknown to Haruki, causes the secret passage to collapse, making it inaccessible. Haruki follows the map to find the secret passage.\\nQuestion: Does Haruki believe that the secret passage exists or has collapsed?\\nChoose one of the following:\\na)Haruki believes the secret passage exists.\\nb)Haruki believes the secret passage has collapsed.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Haruki uses the temple map which still shows the secret passage, and he is unaware of the earthquake damage. Therefore he believes the passage exists and is accessible.  \nAnswer:a)Haruki believes the secret passage exists."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Haruki is exploring an ancient temple during his vacation in Cambodia. Haruki hopes to find a hidden chamber that contains a valuable artifact. He sees that the temple map he holds shows a secret passage leading to the chamber. A recent earthquake, unknown to Haruki, causes the secret passage to collapse, making it inaccessible. Haruki follows the map to find the secret passage.\nQuestion: Does Haruki believe that the secret passage exists or has collapsed?\nChoose one of the following:\na)Haruki believes the secret passage exists.\nb)Haruki believes the secret passage has collapsed.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Haruki uses the temple map which still shows the secret passage, and he is unaware of the earthquake damage. Therefore he believes the passage exists and is accessible.  \nAnswer:a)Haruki believes the secret passage exists."
    },
    {
        "continuation": {
            "0-77": "ToM",
            "78-186": "ToM",
            "189-251": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 80,
        "prompt_tokens": 761,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Alfonso is browsing through a local bookstore in search of a rare book for his collection. Alfonso wants to find a first edition copy of the book in pristine condition to add to his personal library. The book on the shelf appears to be in pristine condition to Alfonso, with no visible signs of wear or damage. While Alfonso is browsing other sections, a fellow customer accidentally spills their coffee on the book, causing stains on the cover. Alfonso plans to purchase the book.\\nQuestion: Does Alfonso believe the book is in pristine condition or stained with coffee?\\nChoose one of the following:\\na)Alfonso believes the book is stained with coffee.\\nb)Alfonso believes the book is in pristine condition.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Alfonso saw the book before the spill and didn\u2019t observe any damage. He was away when the coffee was spilled, so he\u2019s unaware of the new stains and still believes it\u2019s pristine.  \nAnswer: b) Alfonso believes the book is in pristine condition."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Alfonso is browsing through a local bookstore in search of a rare book for his collection. Alfonso wants to find a first edition copy of the book in pristine condition to add to his personal library. The book on the shelf appears to be in pristine condition to Alfonso, with no visible signs of wear or damage. While Alfonso is browsing other sections, a fellow customer accidentally spills their coffee on the book, causing stains on the cover. Alfonso plans to purchase the book.\nQuestion: Does Alfonso believe the book is in pristine condition or stained with coffee?\nChoose one of the following:\na)Alfonso believes the book is stained with coffee.\nb)Alfonso believes the book is in pristine condition.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Alfonso saw the book before the spill and didn\u2019t observe any damage. He was away when the coffee was spilled, so he\u2019s unaware of the new stains and still believes it\u2019s pristine.  \nAnswer: b) Alfonso believes the book is in pristine condition."
    },
    {
        "continuation": {
            "0-130": "ToM",
            "131-195": "ToM",
            "198-267": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 77,
        "prompt_tokens": 747,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Heloise is a professional violinist performing at a prestigious concert hall. She wants to play a beautiful piece on her violin for the audience. She notices that the violin's strings are tuned perfectly before the performance. During intermission, a mischievous child sneaks backstage and detunes the violin's strings. Heloise heads back on stage to continue her performance.\\nQuestion: Does Heloise believe the violin's strings are perfectly tuned or detuned?\\nChoose one of the following:\\na)Heloise believes the violin's strings are perfectly tuned.\\nb)Heloise believes the violin's strings are detuned.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Heloise tuned the violin herself before the performance and has not noticed the child\u2019s interference during intermission. She therefore still believes the strings remain perfectly tuned.  \nAnswer: a) Heloise believes the violin's strings are perfectly tuned."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Heloise is a professional violinist performing at a prestigious concert hall. She wants to play a beautiful piece on her violin for the audience. She notices that the violin's strings are tuned perfectly before the performance. During intermission, a mischievous child sneaks backstage and detunes the violin's strings. Heloise heads back on stage to continue her performance.\nQuestion: Does Heloise believe the violin's strings are perfectly tuned or detuned?\nChoose one of the following:\na)Heloise believes the violin's strings are perfectly tuned.\nb)Heloise believes the violin's strings are detuned.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Heloise tuned the violin herself before the performance and has not noticed the child\u2019s interference during intermission. She therefore still believes the strings remain perfectly tuned.  \nAnswer: a) Heloise believes the violin's strings are perfectly tuned."
    },
    {
        "continuation": {
            "0-144": "ToM",
            "146-218": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 67,
        "prompt_tokens": 744,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Julius is at a community garden, tending to his vegetable plot. Julius wants to harvest some ripe tomatoes for dinner tonight. The tomatoes on the vine appear to be plump and ripe to Julius, with a vibrant red color. A sudden hailstorm strikes the garden, damaging the tomatoes and leaving them bruised and battered. Julius plans to pick the ripe tomatoes.\\nQuestion: Does Julius believe the tomatoes are ripe and ready to be picked or bruised and battered?\\nChoose one of the following:\\na)Julius believes the tomatoes are ripe and ready to be picked.\\nb)Julius believes the tomatoes are bruised and battered.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Julius sees the tomatoes as plump, vibrant red, and plans to harvest them, indicating he believes they are ripe and ready to be picked.\n\nAnswer: a) Julius believes the tomatoes are ripe and ready to be picked."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Julius is at a community garden, tending to his vegetable plot. Julius wants to harvest some ripe tomatoes for dinner tonight. The tomatoes on the vine appear to be plump and ripe to Julius, with a vibrant red color. A sudden hailstorm strikes the garden, damaging the tomatoes and leaving them bruised and battered. Julius plans to pick the ripe tomatoes.\nQuestion: Does Julius believe the tomatoes are ripe and ready to be picked or bruised and battered?\nChoose one of the following:\na)Julius believes the tomatoes are ripe and ready to be picked.\nb)Julius believes the tomatoes are bruised and battered.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Julius sees the tomatoes as plump, vibrant red, and plans to harvest them, indicating he believes they are ripe and ready to be picked.\n\nAnswer: a) Julius believes the tomatoes are ripe and ready to be picked."
    },
    {
        "continuation": {
            "0-127": "ToM",
            "128-215": "ToM",
            "218-298": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 83,
        "prompt_tokens": 766,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Priya is an archaeologist working on a dig site in Egypt, searching for a hidden chamber in a newly discovered pyramid. She wants to find an ancient artifact rumored to be in the chamber. Priya has only explored half of the pyramid so far. Meanwhile, her colleague Ahmed accidentally discovers the chamber while she is away, removing the artifact for further analysis. Priya continues her search for the hidden chamber and the artifact.\\nQuestion: Does Priya believe the hidden chamber and artifact are still undiscovered or already found?\\nChoose one of the following:\\na)Priya believes the hidden chamber and artifact are still undiscovered.\\nb)Priya believes the hidden chamber and artifact have already been found.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Priya has only explored half the pyramid and is unaware that Ahmed discovered and removed the artifact in her absence. She continues her search, so she believes the chamber and artifact remain undiscovered.  \nAnswer:a) Priya believes the hidden chamber and artifact are still undiscovered."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Priya is an archaeologist working on a dig site in Egypt, searching for a hidden chamber in a newly discovered pyramid. She wants to find an ancient artifact rumored to be in the chamber. Priya has only explored half of the pyramid so far. Meanwhile, her colleague Ahmed accidentally discovers the chamber while she is away, removing the artifact for further analysis. Priya continues her search for the hidden chamber and the artifact.\nQuestion: Does Priya believe the hidden chamber and artifact are still undiscovered or already found?\nChoose one of the following:\na)Priya believes the hidden chamber and artifact are still undiscovered.\nb)Priya believes the hidden chamber and artifact have already been found.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Priya has only explored half the pyramid and is unaware that Ahmed discovered and removed the artifact in her absence. She continues her search, so she believes the chamber and artifact remain undiscovered.  \nAnswer:a) Priya believes the hidden chamber and artifact are still undiscovered."
    },
    {
        "continuation": {
            "0-77": "DMN",
            "78-135": "ToM",
            "136-192": "ToM",
            "195-250": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 87,
        "prompt_tokens": 758,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Quincy is at a botanical garden admiring the beautiful plants and flowers. He wants to take a picture of a rare orchid that he has been searching for. He sees that the orchid is in full bloom, displaying its vibrant colors and unique shape. Overnight, a sudden cold snap causes the orchid's petals to wilt and fall off. Quincy plans to return the next day to take the perfect photo of the orchid.\\nQuestion: Does Quincy believe the orchid is in full bloom or has wilted and lost its petals?\\nChoose one of the following:\\na)Quincy believes the orchid is in full bloom.\\nb)Quincy believes the orchid has wilted and lost its petals.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Quincy saw the orchid in full bloom and plans to return for a photo. He is unaware that overnight cold snap wilted the petals. Therefore he still believes the orchid is in full bloom.  \nAnswer: a) Quincy believes the orchid is in full bloom."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Quincy is at a botanical garden admiring the beautiful plants and flowers. He wants to take a picture of a rare orchid that he has been searching for. He sees that the orchid is in full bloom, displaying its vibrant colors and unique shape. Overnight, a sudden cold snap causes the orchid's petals to wilt and fall off. Quincy plans to return the next day to take the perfect photo of the orchid.\nQuestion: Does Quincy believe the orchid is in full bloom or has wilted and lost its petals?\nChoose one of the following:\na)Quincy believes the orchid is in full bloom.\nb)Quincy believes the orchid has wilted and lost its petals.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Quincy saw the orchid in full bloom and plans to return for a photo. He is unaware that overnight cold snap wilted the petals. Therefore he still believes the orchid is in full bloom.  \nAnswer: a) Quincy believes the orchid is in full bloom."
    },
    {
        "continuation": {
            "0-142": "DMN",
            "143-275": "ToM",
            "278-325": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 96,
        "prompt_tokens": 753,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Imani is a marine biologist studying coral reefs off the coast of Australia. She wants to collect a sample of healthy coral for her research. The coral appears to be vibrant and thriving to Imani. A sudden temperature spike in the water causes the coral to become bleached and stressed. Imani proceeds to collect the sample of the coral.\\nQuestion: Does Imani believe the coral is healthy or stressed and bleached?\\nChoose one of the following:\\na)Imani believes the coral is healthy.\\nb)Imani believes the coral is stressed and bleached.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Imani wants to collect a healthy coral sample and that the coral \u201cappears to be vibrant and thriving to Imani.\u201d Even though a temperature spike later bleaches and stresses the coral, her belief at the moment of collection is that it is healthy.  \nAnswer: a) Imani believes the coral is healthy."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Imani is a marine biologist studying coral reefs off the coast of Australia. She wants to collect a sample of healthy coral for her research. The coral appears to be vibrant and thriving to Imani. A sudden temperature spike in the water causes the coral to become bleached and stressed. Imani proceeds to collect the sample of the coral.\nQuestion: Does Imani believe the coral is healthy or stressed and bleached?\nChoose one of the following:\na)Imani believes the coral is healthy.\nb)Imani believes the coral is stressed and bleached.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Imani wants to collect a healthy coral sample and that the coral \u201cappears to be vibrant and thriving to Imani.\u201d Even though a temperature spike later bleaches and stresses the coral, her belief at the moment of collection is that it is healthy.  \nAnswer: a) Imani believes the coral is healthy."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-117": "ToM",
            "120-127": "LN",
            "127-129": "LN",
            "129-160": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 76,
        "prompt_tokens": 720,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Tanya is a marine biologist studying coral reefs in the Maldives. Her goal is to collect samples of a rare coral species that thrives in shallow waters. She sees that the tide is low, exposing the coral reefs. Suddenly, an underwater earthquake occurs, causing the tide to rise rapidly and submerging the coral reefs. Tanya prepares her equipment to collect the coral samples immediately.\\nQuestion: Does Tanya believe the tide is low or high?\\nChoose one of the following:\\na)Tanya believes the tide is high.\\nb)Tanya believes the tide is low.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Tanya sees the tide is low, exposing the coral reefs, so she believes the tide is low.  \nAnswer:b)Tanya believes the tide is low."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Tanya is a marine biologist studying coral reefs in the Maldives. Her goal is to collect samples of a rare coral species that thrives in shallow waters. She sees that the tide is low, exposing the coral reefs. Suddenly, an underwater earthquake occurs, causing the tide to rise rapidly and submerging the coral reefs. Tanya prepares her equipment to collect the coral samples immediately.\nQuestion: Does Tanya believe the tide is low or high?\nChoose one of the following:\na)Tanya believes the tide is high.\nb)Tanya believes the tide is low.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Tanya sees the tide is low, exposing the coral reefs, so she believes the tide is low.  \nAnswer:b)Tanya believes the tide is low."
    },
    {
        "continuation": {
            "0-68": "DMN",
            "69-145": "ToM",
            "146-217": "ToM",
            "220-276": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 93,
        "prompt_tokens": 754,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Eliza is a professional violinist preparing for a concert in a grand theater. She wants to play a beautiful piece on her violin to impress the audience. Eliza's violin is in perfect tune when she last checked it. Just before the concert, a sudden change in temperature causes the violin strings to become out of tune. Eliza confidently walks on stage, ready to play her piece.\\nQuestion: Does Eliza believe her violin is in perfect tune or out of tune?\\nChoose one of the following:\\na)Eliza believes her violin is out of tune.\\nb)Eliza believes her violin is in perfect tune.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Eliza last checked her violin and found it in perfect tune. She\u2019s unaware of the sudden temperature change that has detuned the strings. Confidently walking on stage indicates she still believes it\u2019s in tune.  \nAnswer: b) Eliza believes her violin is in perfect tune."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Eliza is a professional violinist preparing for a concert in a grand theater. She wants to play a beautiful piece on her violin to impress the audience. Eliza's violin is in perfect tune when she last checked it. Just before the concert, a sudden change in temperature causes the violin strings to become out of tune. Eliza confidently walks on stage, ready to play her piece.\nQuestion: Does Eliza believe her violin is in perfect tune or out of tune?\nChoose one of the following:\na)Eliza believes her violin is out of tune.\nb)Eliza believes her violin is in perfect tune.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Eliza last checked her violin and found it in perfect tune. She\u2019s unaware of the sudden temperature change that has detuned the strings. Confidently walking on stage indicates she still believes it\u2019s in tune.  \nAnswer: b) Eliza believes her violin is in perfect tune."
    },
    {
        "continuation": {
            "0-78": "DMN",
            "79-211": "ToM",
            "212-267": "ToM",
            "270-332": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 100,
        "prompt_tokens": 771,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Chitra is a professional violinist preparing for a concert at the city's concert hall. She wants to play her favorite violin, which she left in its case backstage. Chitra sees that the violin is in perfect condition. Just before Chitra goes on stage, a stagehand accidentally knocks the violin case off a table, causing one of the strings to snap. Chitra plans to warm up and tune her violin before going on stage.\\nQuestion: Does Chitra believe her violin is in perfect condition or has a broken string?\\nChoose one of the following:\\na)Chitra believes her violin is in perfect condition.\\nb)Chitra believes her violin has a broken string.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Chitra last inspected the violin and saw it was in perfect condition. The stagehand\u2019s accident happened out of her view, and there\u2019s no indication she discovered the broken string before going on stage. Therefore she still believes it\u2019s in perfect condition.  \nAnswer: a) Chitra believes her violin is in perfect condition."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Chitra is a professional violinist preparing for a concert at the city's concert hall. She wants to play her favorite violin, which she left in its case backstage. Chitra sees that the violin is in perfect condition. Just before Chitra goes on stage, a stagehand accidentally knocks the violin case off a table, causing one of the strings to snap. Chitra plans to warm up and tune her violin before going on stage.\nQuestion: Does Chitra believe her violin is in perfect condition or has a broken string?\nChoose one of the following:\na)Chitra believes her violin is in perfect condition.\nb)Chitra believes her violin has a broken string.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Chitra last inspected the violin and saw it was in perfect condition. The stagehand\u2019s accident happened out of her view, and there\u2019s no indication she discovered the broken string before going on stage. Therefore she still believes it\u2019s in perfect condition.  \nAnswer: a) Chitra believes her violin is in perfect condition."
    },
    {
        "continuation": {
            "0-8": "LN",
            "9-142": "DMN",
            "143-258": "ToM",
            "260-324": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 97,
        "prompt_tokens": 746,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Ravi is a marine biologist studying coral reefs in the Maldives. He wants to examine a specific coral species known for its vibrant colors. Ravi has read that the coral species is usually found in shallow waters. A sudden underwater landslide causes the coral species to be pushed into deeper waters. Ravi prepares his equipment to dive in shallow waters.\\nQuestion: Does Ravi believe the coral species is in shallow or deep waters?\\nChoose one of the following:\\na)Ravi believes the coral species is in deep waters.\\nb)Ravi believes the coral species is in shallow waters.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Ravi has read that the coral species is usually found in shallow waters and he prepares his equipment accordingly for a shallow dive. Despite the landslide pushing corals into deeper waters, Ravi\u2019s belief is that the species lives in shallow waters.\n\nAnswer: b) Ravi believes the coral species is in shallow waters."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Ravi is a marine biologist studying coral reefs in the Maldives. He wants to examine a specific coral species known for its vibrant colors. Ravi has read that the coral species is usually found in shallow waters. A sudden underwater landslide causes the coral species to be pushed into deeper waters. Ravi prepares his equipment to dive in shallow waters.\nQuestion: Does Ravi believe the coral species is in shallow or deep waters?\nChoose one of the following:\na)Ravi believes the coral species is in deep waters.\nb)Ravi believes the coral species is in shallow waters.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Ravi has read that the coral species is usually found in shallow waters and he prepares his equipment accordingly for a shallow dive. Despite the landslide pushing corals into deeper waters, Ravi\u2019s belief is that the species lives in shallow waters.\n\nAnswer: b) Ravi believes the coral species is in shallow waters."
    },
    {
        "continuation": {
            "0-82": "DMN",
            "83-113": "ToM",
            "114-152": "ToM",
            "155-215": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 80,
        "prompt_tokens": 752,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Deepak is at a farmer's market, searching for the perfect watermelon for a picnic. Deepak wants to buy a ripe, juicy watermelon to share with his friends. The watermelon he examines has a deep, hollow sound when he taps it, indicating that it is ripe. While Deepak is momentarily distracted, a mischievous child replaces the ripe watermelon with an underripe one that looks almost identical. Deepak buys the watermelon he initially examined.\\nQuestion: Does Deepak believe the watermelon is ripe and juicy or underripe?\\nChoose one of the following:\\na)Deepak believes the watermelon is ripe and juicy.\\nb)Deepak believes the watermelon is underripe.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Deepak tapped the watermelon, heard the hollow sound indicating ripeness, and is unaware it was swapped. He still believes it\u2019s ripe and juicy.  \nAnswer: a) Deepak believes the watermelon is ripe and juicy."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Deepak is at a farmer's market, searching for the perfect watermelon for a picnic. Deepak wants to buy a ripe, juicy watermelon to share with his friends. The watermelon he examines has a deep, hollow sound when he taps it, indicating that it is ripe. While Deepak is momentarily distracted, a mischievous child replaces the ripe watermelon with an underripe one that looks almost identical. Deepak buys the watermelon he initially examined.\nQuestion: Does Deepak believe the watermelon is ripe and juicy or underripe?\nChoose one of the following:\na)Deepak believes the watermelon is ripe and juicy.\nb)Deepak believes the watermelon is underripe.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Deepak tapped the watermelon, heard the hollow sound indicating ripeness, and is unaware it was swapped. He still believes it\u2019s ripe and juicy.  \nAnswer: a) Deepak believes the watermelon is ripe and juicy."
    },
    {
        "continuation": {
            "0-213": "ToM",
            "214-270": "ToM",
            "273-359": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 100,
        "prompt_tokens": 783,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Farid is hiking in a dense forest, searching for a hidden waterfall. Farid wants to photograph the waterfall for his nature photography collection. He sees that the trail map he is holding indicates that the waterfall is located near a large, distinctive boulder. A sudden landslide alters the landscape, burying the boulder under a pile of rocks and debris. Farid follows the trail map towards the large, distinctive boulder.\\nQuestion: Does Farid believe the waterfall is located near the large, distinctive boulder or elsewhere?\\nChoose one of the following:\\na)Farid believes the waterfall is located elsewhere.\\nb)Farid believes the waterfall is located near the large, distinctive boulder.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The trail map Farid is using indicates the waterfall\u2019s location relative to the large, distinctive boulder, and despite the landslide burying the boulder, Farid still follows the map toward that landmark. Therefore he believes the waterfall is near the boulder.  \nAnswer:b) Farid believes the waterfall is located near the large, distinctive boulder."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Farid is hiking in a dense forest, searching for a hidden waterfall. Farid wants to photograph the waterfall for his nature photography collection. He sees that the trail map he is holding indicates that the waterfall is located near a large, distinctive boulder. A sudden landslide alters the landscape, burying the boulder under a pile of rocks and debris. Farid follows the trail map towards the large, distinctive boulder.\nQuestion: Does Farid believe the waterfall is located near the large, distinctive boulder or elsewhere?\nChoose one of the following:\na)Farid believes the waterfall is located elsewhere.\nb)Farid believes the waterfall is located near the large, distinctive boulder.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The trail map Farid is using indicates the waterfall\u2019s location relative to the large, distinctive boulder, and despite the landslide burying the boulder, Farid still follows the map toward that landmark. Therefore he believes the waterfall is near the boulder.  \nAnswer:b) Farid believes the waterfall is located near the large, distinctive boulder."
    },
    {
        "continuation": {
            "0-63": "DMN",
            "64-165": "ToM",
            "166-252": "ToM",
            "255-320": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 97,
        "prompt_tokens": 765,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Gideon is a talented sculptor working in his studio in Italy. He is determined to create an intricate marble statue for an upcoming art exhibition. The block of marble he has chosen is solid and pristine. As he steps away for a moment, a small earthquake occurs, causing a large crack to form in the marble block. Gideon begins sculpting the statue using the marble block he initially chose.\\nQuestion: Does Gideon believe the marble block is solid and pristine or cracked and damaged?\\nChoose one of the following:\\na)Gideon believes the marble block is cracked and damaged.\\nb)Gideon believes the marble block is solid and pristine.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Gideon originally saw the block as solid and pristine. The earthquake with the resulting crack occurred while he was away, and he is unaware of that damage. Therefore he still believes the block is intact and flawless when he begins sculpting.  \nAnswer:b) Gideon believes the marble block is solid and pristine."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Gideon is a talented sculptor working in his studio in Italy. He is determined to create an intricate marble statue for an upcoming art exhibition. The block of marble he has chosen is solid and pristine. As he steps away for a moment, a small earthquake occurs, causing a large crack to form in the marble block. Gideon begins sculpting the statue using the marble block he initially chose.\nQuestion: Does Gideon believe the marble block is solid and pristine or cracked and damaged?\nChoose one of the following:\na)Gideon believes the marble block is cracked and damaged.\nb)Gideon believes the marble block is solid and pristine.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Gideon originally saw the block as solid and pristine. The earthquake with the resulting crack occurred while he was away, and he is unaware of that damage. Therefore he still believes the block is intact and flawless when he begins sculpting.  \nAnswer:b) Gideon believes the marble block is solid and pristine."
    },
    {
        "continuation": {
            "0-77": "DMN",
            "78-229": "ToM",
            "230-275": "ToM",
            "277-344": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 104,
        "prompt_tokens": 761,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Santiago is a fisherman in a small coastal town in Chile. He wants to set out to sea to catch fish for the local market. Santiago sees that his fishing boat appears to be in good condition. Later, a sudden storm hits the coast, causing damage to Santiago's boat while it is still docked. Santiago prepares his fishing gear and sets out to sea.\\nQuestion: Does Santiago believe his fishing boat is in good condition or damaged by the storm?\\nChoose one of the following:\\na)Santiago believes his fishing boat is in good condition.\\nb)Santiago believes his fishing boat is damaged by the storm.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Santiago initially inspects his boat and finds it in good condition. Although a storm later damages the boat, there is no indication that Santiago inspects it again before setting out\u2014he proceeds with his gear and sails, implying he still believes the boat is sound.\n\nAnswer: a) Santiago believes his fishing boat is in good condition."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Santiago is a fisherman in a small coastal town in Chile. He wants to set out to sea to catch fish for the local market. Santiago sees that his fishing boat appears to be in good condition. Later, a sudden storm hits the coast, causing damage to Santiago's boat while it is still docked. Santiago prepares his fishing gear and sets out to sea.\nQuestion: Does Santiago believe his fishing boat is in good condition or damaged by the storm?\nChoose one of the following:\na)Santiago believes his fishing boat is in good condition.\nb)Santiago believes his fishing boat is damaged by the storm.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Santiago initially inspects his boat and finds it in good condition. Although a storm later damages the boat, there is no indication that Santiago inspects it again before setting out\u2014he proceeds with his gear and sails, implying he still believes the boat is sound.\n\nAnswer: a) Santiago believes his fishing boat is in good condition."
    },
    {
        "continuation": {
            "0-92": "DMN",
            "93-198": "ToM",
            "201-210": "LN",
            "210-258": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 85,
        "prompt_tokens": 755,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Beatrice is a talented violinist, preparing for an important performance at a prestigious concert hall. She wants to ensure that her violin is perfectly tuned before she goes on stage. Beatrice checks the tuning of her violin and it sounds perfectly in tune. While Beatrice is in her dressing room, an assistant accidentally knocks the violin off the table, causing it to go out of tune. Beatrice confidently goes on stage with her violin.\\nQuestion: Does Beatrice believe her violin is perfectly tuned or out of tune?\\nChoose one of the following:\\na)Beatrice believes her violin is perfectly tuned.\\nb)Beatrice believes her violin is out of tune.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Beatrice checked her violin and it sounded perfectly in tune before going on stage. She is unaware that it was knocked off and went out of tune, so she still believes it is perfectly tuned.  \nAnswer:a)Beatrice believes her violin is perfectly tuned."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Beatrice is a talented violinist, preparing for an important performance at a prestigious concert hall. She wants to ensure that her violin is perfectly tuned before she goes on stage. Beatrice checks the tuning of her violin and it sounds perfectly in tune. While Beatrice is in her dressing room, an assistant accidentally knocks the violin off the table, causing it to go out of tune. Beatrice confidently goes on stage with her violin.\nQuestion: Does Beatrice believe her violin is perfectly tuned or out of tune?\nChoose one of the following:\na)Beatrice believes her violin is perfectly tuned.\nb)Beatrice believes her violin is out of tune.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Beatrice checked her violin and it sounded perfectly in tune before going on stage. She is unaware that it was knocked off and went out of tune, so she still believes it is perfectly tuned.  \nAnswer:a)Beatrice believes her violin is perfectly tuned."
    },
    {
        "continuation": {
            "0-160": "ToM",
            "163-222": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 69,
        "prompt_tokens": 752,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Ravi is visiting a beautiful botanical garden in India. He wants to take a photograph of a rare, vibrant purple orchid that is in full bloom. He sees that the orchid he is looking for appears to be in perfect condition. While Ravi is exploring other sections of the garden, a sudden gust of wind knocks the orchid over, causing some petals to fall off. Ravi sets up his camera to take a photograph of the perfect orchid.\\nQuestion: Does Ravi believe the orchid is in perfect condition or damaged with fallen petals?\\nChoose one of the following:\\na)Ravi believes the orchid is in perfect condition.\\nb)Ravi believes the orchid is damaged with fallen petals.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Ravi saw the orchid in perfect condition and, unaware of the wind damage while he was away, still believes it to be perfect when he sets up his camera.  \nAnswer: a)Ravi believes the orchid is in perfect condition."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Ravi is visiting a beautiful botanical garden in India. He wants to take a photograph of a rare, vibrant purple orchid that is in full bloom. He sees that the orchid he is looking for appears to be in perfect condition. While Ravi is exploring other sections of the garden, a sudden gust of wind knocks the orchid over, causing some petals to fall off. Ravi sets up his camera to take a photograph of the perfect orchid.\nQuestion: Does Ravi believe the orchid is in perfect condition or damaged with fallen petals?\nChoose one of the following:\na)Ravi believes the orchid is in perfect condition.\nb)Ravi believes the orchid is damaged with fallen petals.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Ravi saw the orchid in perfect condition and, unaware of the wind damage while he was away, still believes it to be perfect when he sets up his camera.  \nAnswer: a)Ravi believes the orchid is in perfect condition."
    },
    {
        "continuation": {
            "0-80": "DMN",
            "81-215": "ToM",
            "217-288": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 84,
        "prompt_tokens": 754,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Santiago is a fisherman on the coast of Chile. Santiago wants to catch a large amount of fish today to sell at the market. He sees that the fishing net he is using is in excellent condition. While Santiago is not paying attention, a sharp rock on the ocean floor tears a large hole in the fishing net. Santiago casts the fishing net into the water to catch fish.\\nQuestion: Does Santiago believe the fishing net is in excellent condition or torn with a large hole?\\nChoose one of the following:\\na)Santiago believes the fishing net is torn with a large hole.\\nb)Santiago believes the fishing net is in excellent condition.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Santiago initially inspects the net and sees it in excellent condition. He does not notice it being torn by the rock, so he remains unaware of the hole and continues to believe it is in excellent condition.\n\nAnswer: b) Santiago believes the fishing net is in excellent condition."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Santiago is a fisherman on the coast of Chile. Santiago wants to catch a large amount of fish today to sell at the market. He sees that the fishing net he is using is in excellent condition. While Santiago is not paying attention, a sharp rock on the ocean floor tears a large hole in the fishing net. Santiago casts the fishing net into the water to catch fish.\nQuestion: Does Santiago believe the fishing net is in excellent condition or torn with a large hole?\nChoose one of the following:\na)Santiago believes the fishing net is torn with a large hole.\nb)Santiago believes the fishing net is in excellent condition.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Santiago initially inspects the net and sees it in excellent condition. He does not notice it being torn by the rock, so he remains unaware of the hole and continues to believe it is in excellent condition.\n\nAnswer: b) Santiago believes the fishing net is in excellent condition."
    },
    {
        "continuation": {
            "0-130": "ToM",
            "133-205": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 65,
        "prompt_tokens": 763,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Fatima is a talented painter who has been commissioned to create a mural on a local community center's wall. She wants to choose the perfect color palette to represent the vibrant energy of the community. Fatima notices that the paint colors she has selected are bright and lively. However, while she is away for lunch, a mischievous cat knocks over the paint cans, causing the colors to mix into dull, muted shades. Fatima starts painting the mural with the chosen colors.\\nQuestion: Does Fatima believe the paint colors are bright and lively or dull and muted?\\nChoose one of the following:\\na)Fatima believes the paint colors are now dull and muted.\\nb)Fatima believes the paint colors are still bright and lively.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Fatima selected bright, lively paints and hasn\u2019t noticed the cat\u2019s mishap, so she still thinks they\u2019re bright and lively.  \nAnswer: b) Fatima believes the paint colors are still bright and lively."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Fatima is a talented painter who has been commissioned to create a mural on a local community center's wall. She wants to choose the perfect color palette to represent the vibrant energy of the community. Fatima notices that the paint colors she has selected are bright and lively. However, while she is away for lunch, a mischievous cat knocks over the paint cans, causing the colors to mix into dull, muted shades. Fatima starts painting the mural with the chosen colors.\nQuestion: Does Fatima believe the paint colors are bright and lively or dull and muted?\nChoose one of the following:\na)Fatima believes the paint colors are now dull and muted.\nb)Fatima believes the paint colors are still bright and lively.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Fatima selected bright, lively paints and hasn\u2019t noticed the cat\u2019s mishap, so she still thinks they\u2019re bright and lively.  \nAnswer: b) Fatima believes the paint colors are still bright and lively."
    },
    {
        "continuation": {
            "0-94": "ToM",
            "95-171": "ToM",
            "174-236": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 85,
        "prompt_tokens": 774,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Thalia is an experienced chef working in a high-end restaurant in Rio de Janeiro. She wants to impress her guests tonight with a perfectly cooked, medium-rare steak. Thalia observes that the steak on the grill appears to be cooked to medium-rare. However, as she turns her back to grab some seasoning, the grill's temperature suddenly spikes, overcooking the steak to well-done. Thalia removes the steak from the grill and plates it.\\nQuestion: Does Thalia believe the steak is cooked to medium-rare or well-done?\\nChoose one of the following:\\na)Thalia believes the steak is cooked to well-done.\\nb)Thalia believes the steak is cooked to medium-rare.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Thalia saw the steak at medium-rare, then turned away and missed the sudden overcook. She isn\u2019t aware it became well-done, so she still believes it\u2019s medium-rare.  \nAnswer: b) Thalia believes the steak is cooked to medium-rare."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Thalia is an experienced chef working in a high-end restaurant in Rio de Janeiro. She wants to impress her guests tonight with a perfectly cooked, medium-rare steak. Thalia observes that the steak on the grill appears to be cooked to medium-rare. However, as she turns her back to grab some seasoning, the grill's temperature suddenly spikes, overcooking the steak to well-done. Thalia removes the steak from the grill and plates it.\nQuestion: Does Thalia believe the steak is cooked to medium-rare or well-done?\nChoose one of the following:\na)Thalia believes the steak is cooked to well-done.\nb)Thalia believes the steak is cooked to medium-rare.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Thalia saw the steak at medium-rare, then turned away and missed the sudden overcook. She isn\u2019t aware it became well-done, so she still believes it\u2019s medium-rare.  \nAnswer: b) Thalia believes the steak is cooked to medium-rare."
    },
    {
        "continuation": {
            "0-148": "ToM",
            "151-218": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 70,
        "prompt_tokens": 759,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Leilani is a professional chef preparing for a cooking competition. She needs to make a perfect chocolate souffl\u00e9 to impress the judges. Leilani observes that her oven is preheated to the precise temperature required for baking the souffl\u00e9. However, a power outage occurs, causing the oven to lose heat rapidly. Leilani places the souffl\u00e9 in the oven to bake.\\nQuestion: Does Leilani believe the oven is at the perfect temperature or that it has lost heat due to the power outage?\\nChoose one of the following:\\na)Leilani believes the oven has lost heat due to the power outage.\\nb)Leilani believes the oven is at the perfect temperature.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Leilani saw the oven preheated and didn\u2019t know it lost heat when the power went out, so she still believes it\u2019s at the perfect temperature.  \nAnswer: b) Leilani believes the oven is at the perfect temperature."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Leilani is a professional chef preparing for a cooking competition. She needs to make a perfect chocolate souffl\u00e9 to impress the judges. Leilani observes that her oven is preheated to the precise temperature required for baking the souffl\u00e9. However, a power outage occurs, causing the oven to lose heat rapidly. Leilani places the souffl\u00e9 in the oven to bake.\nQuestion: Does Leilani believe the oven is at the perfect temperature or that it has lost heat due to the power outage?\nChoose one of the following:\na)Leilani believes the oven has lost heat due to the power outage.\nb)Leilani believes the oven is at the perfect temperature.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Leilani saw the oven preheated and didn\u2019t know it lost heat when the power went out, so she still believes it\u2019s at the perfect temperature.  \nAnswer: b) Leilani believes the oven is at the perfect temperature."
    },
    {
        "continuation": {
            "0-105": "ToM",
            "106-142": "ToM",
            "145-216": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 78,
        "prompt_tokens": 759,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Dalia is a professional photographer, setting up her equipment for a wedding photoshoot. She wants to capture stunning photos with her high-quality camera. Dalia sees that the camera lens appears clean and free of smudges. While Dalia is away, a mischievous child plays with the camera, leaving fingerprints all over the lens. Dalia begins the photoshoot with her clean camera lens.\\nQuestion: Does Dalia believe the camera lens is clean and free of smudges or covered in fingerprints?\\nChoose one of the following:\\na)Dalia believes the camera lens is clean and free of smudges.\\nb)Dalia believes the camera lens is covered in fingerprints.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Dalia saw the lens was clean before the child fouled it, and she is unaware of the fingerprints. Thus she still believes it is clean.  \nAnswer: a) Dalia believes the camera lens is clean and free of smudges."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Dalia is a professional photographer, setting up her equipment for a wedding photoshoot. She wants to capture stunning photos with her high-quality camera. Dalia sees that the camera lens appears clean and free of smudges. While Dalia is away, a mischievous child plays with the camera, leaving fingerprints all over the lens. Dalia begins the photoshoot with her clean camera lens.\nQuestion: Does Dalia believe the camera lens is clean and free of smudges or covered in fingerprints?\nChoose one of the following:\na)Dalia believes the camera lens is clean and free of smudges.\nb)Dalia believes the camera lens is covered in fingerprints.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Dalia saw the lens was clean before the child fouled it, and she is unaware of the fingerprints. Thus she still believes it is clean.  \nAnswer: a) Dalia believes the camera lens is clean and free of smudges."
    },
    {
        "continuation": {
            "0-109": "ToM",
            "111-160": "ToM"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 61,
        "prompt_tokens": 751,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Ezekiel is a baker in a small French town, preparing to bake a large batch of baguettes. He wants to make sure he has enough flour to complete the order. Ezekiel sees that the flour sack appears to be full. While Ezekiel is attending to another task, a mischievous cat sneaks into the bakery and claws a hole in the sack, causing most of the flour to spill on the floor. Ezekiel begins mixing the dough for the baguettes.\\nQuestion: Does Ezekiel believe the flour sack is full or nearly empty?\\nChoose one of the following:\\na)Ezekiel believes the flour sack is nearly empty.\\nb)Ezekiel believes the flour sack is full.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Ezekiel saw the flour sack full and did not notice the cat\u2019s mishap, so he still believes it\u2019s full.\n\nAnswer:b)Ezekiel believes the flour sack is full."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Ezekiel is a baker in a small French town, preparing to bake a large batch of baguettes. He wants to make sure he has enough flour to complete the order. Ezekiel sees that the flour sack appears to be full. While Ezekiel is attending to another task, a mischievous cat sneaks into the bakery and claws a hole in the sack, causing most of the flour to spill on the floor. Ezekiel begins mixing the dough for the baguettes.\nQuestion: Does Ezekiel believe the flour sack is full or nearly empty?\nChoose one of the following:\na)Ezekiel believes the flour sack is nearly empty.\nb)Ezekiel believes the flour sack is full.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Ezekiel saw the flour sack full and did not notice the cat\u2019s mishap, so he still believes it\u2019s full.\n\nAnswer:b)Ezekiel believes the flour sack is full."
    },
    {
        "continuation": {
            "0-163": "ToM",
            "166-219": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 66,
        "prompt_tokens": 738,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Tariq is a skilled electrician working on a residential construction site. He wants to safely install wiring in a newly built house. Tariq observes that the circuit breaker is turned off before he begins his work. As Tariq is about to start, a coworker unknowingly switches the circuit breaker on, making the wiring live. Tariq starts installing the wiring, assuming it is not live.\\nQuestion: Does Tariq believe the circuit breaker is off or on?\\nChoose one of the following:\\na)Tariq believes the circuit breaker is off.\\nb)Tariq believes the circuit breaker is on.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Tariq checked that the breaker was off before starting and is unaware his coworker turned it back on, so he continues under the assumption it remains off.  \nAnswer: a) Tariq believes the circuit breaker is off."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Tariq is a skilled electrician working on a residential construction site. He wants to safely install wiring in a newly built house. Tariq observes that the circuit breaker is turned off before he begins his work. As Tariq is about to start, a coworker unknowingly switches the circuit breaker on, making the wiring live. Tariq starts installing the wiring, assuming it is not live.\nQuestion: Does Tariq believe the circuit breaker is off or on?\nChoose one of the following:\na)Tariq believes the circuit breaker is off.\nb)Tariq believes the circuit breaker is on.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Tariq checked that the breaker was off before starting and is unaware his coworker turned it back on, so he continues under the assumption it remains off.  \nAnswer: a) Tariq believes the circuit breaker is off."
    },
    {
        "continuation": {
            "0-57": "ToM",
            "58-107": "DMN",
            "108-136": "ToM",
            "137-185": "ToM",
            "188-233": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 94,
        "prompt_tokens": 751,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Uriel is a professional chef, preparing for a cooking competition. He wants to make sure his knife is sharp enough to cut through vegetables quickly and efficiently. Uriel tests the knife's sharpness by gently running his thumb along the edge. While Uriel is distracted, a fellow contestant accidentally knocks the knife onto the floor, causing the blade to become significantly duller. Uriel proceeds to chop vegetables with the sharp knife.\\nQuestion: Does Uriel believe the knife is sharp or significantly duller?\\nChoose one of the following:\\na)Uriel believes the knife is significantly duller.\\nb)Uriel believes the knife is sharp.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Uriel tested the blade and thought it was sharp. He was distracted when it got knocked and dulled, but he doesn\u2019t realize that. He continues chopping as if it were still sharp.  \nAnswer: b) Uriel believes the knife is sharp."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Uriel is a professional chef, preparing for a cooking competition. He wants to make sure his knife is sharp enough to cut through vegetables quickly and efficiently. Uriel tests the knife's sharpness by gently running his thumb along the edge. While Uriel is distracted, a fellow contestant accidentally knocks the knife onto the floor, causing the blade to become significantly duller. Uriel proceeds to chop vegetables with the sharp knife.\nQuestion: Does Uriel believe the knife is sharp or significantly duller?\nChoose one of the following:\na)Uriel believes the knife is significantly duller.\nb)Uriel believes the knife is sharp.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Uriel tested the blade and thought it was sharp. He was distracted when it got knocked and dulled, but he doesn\u2019t realize that. He continues chopping as if it were still sharp.  \nAnswer: b) Uriel believes the knife is sharp."
    },
    {
        "continuation": {
            "0-238": "ToM",
            "241-328": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 85,
        "prompt_tokens": 791,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Petra is a botanist researching the effects of different fertilizers on plant growth. She wants to find the best fertilizer for her experimental plants. Petra sees that the plants treated with Fertilizer A appear to be growing the fastest. However, a sudden infestation of insects damages the plants treated with Fertilizer A, stunting their growth. Petra prepares to recommend Fertilizer A as the best option for plant growth.\\nQuestion: Does Petra believe the plants treated with Fertilizer A are growing the fastest or does she believe their growth has been stunted?\\nChoose one of the following:\\na)Petra believes the growth of the plants treated with Fertilizer A has been stunted.\\nb)Petra believes the plants treated with Fertilizer A are growing the fastest.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: Petra initially observes that plants with Fertilizer A are growing the fastest, and despite the insect damage she still plans to recommend Fertilizer A as the best option. This indicates she believes they have the fastest growth.  \nAnswer: b) Petra believes the plants treated with Fertilizer A are growing the fastest."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Petra is a botanist researching the effects of different fertilizers on plant growth. She wants to find the best fertilizer for her experimental plants. Petra sees that the plants treated with Fertilizer A appear to be growing the fastest. However, a sudden infestation of insects damages the plants treated with Fertilizer A, stunting their growth. Petra prepares to recommend Fertilizer A as the best option for plant growth.\nQuestion: Does Petra believe the plants treated with Fertilizer A are growing the fastest or does she believe their growth has been stunted?\nChoose one of the following:\na)Petra believes the growth of the plants treated with Fertilizer A has been stunted.\nb)Petra believes the plants treated with Fertilizer A are growing the fastest.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: Petra initially observes that plants with Fertilizer A are growing the fastest, and despite the insect damage she still plans to recommend Fertilizer A as the best option. This indicates she believes they have the fastest growth.  \nAnswer: b) Petra believes the plants treated with Fertilizer A are growing the fastest."
    },
    {
        "continuation": {
            "0-266": "ToM",
            "269-348": "LN"
        },
        "dataset": "bigtom",
        "model": "gpt-4.1-2025-04-14",
        "completion_tokens": 96,
        "prompt_tokens": 786,
        "reasoning_tokens": 0,
        "openai_prompt": [
            {
                "role": "system",
                "content": "I am training a mixture-of-experts (MoE) model that routes tokens individually (token-level routing) to specialized experts. The model includes four distinct experts, each clearly analogous to a specific cognitive network in the human brain. Below is a detailed explanation of each expert, along with examples of the types of tasks or token sequences each should typically handle:\n\n- **Language Network (LN)**: Primarily responsible for linguistic processing, grammatical structures, vocabulary usage, syntax, semantics, and sentence coherence. This expert should handle tasks involving language comprehension, text fluency, sentence construction, paraphrasing, and interpreting linguistic nuances.\n  - *Example tasks*: Completing sentences, grammar correction, paraphrasing sentences, translating between languages, summarizing text.\n\n- **Multiple Demand Network (MD)**: Specializes in analytical thinking, mathematical calculations, numerical reasoning, and logical problem-solving. This expert engages explicitly in arithmetic operations, logical deductions, comparisons, quantitative reasoning, and systematic analysis.\n  - *Example tasks*: Performing arithmetic operations, solving logical puzzles, analyzing numerical data, interpreting mathematical expressions, evaluating logical arguments.\n\n- **Theory of Mind Network (ToM)**: Dedicated to social cognition and interpersonal reasoning. This expert interprets and predicts social interactions, emotional states, intentions, desires, beliefs, and motivations of individuals or groups.\n  - *Example tasks*: Inferring a person's feelings or intentions from their actions, understanding dialogue involving interpersonal relations, predicting characters' behaviors based on emotional context, interpreting subtle social cues.\n\n- **Default Mode Network (DMN)**: Responsible for integrating general world knowledge, context understanding, background information retrieval, and conceptual reasoning about everyday scenarios or common-sense understanding.\n  - *Example tasks*: Providing background knowledge on common scenarios, contextualizing real-world situations, recalling general facts, understanding cause-and-effect relationships, providing narrative context.\n\nGiven the following example of model-generated text in response to a prompt, carefully label each token sequence with the expert best suited to handle it (LN, MD, ToM, or DMN). Ensure each sequence is assigned to only one expert. Output your answer clearly and explicitly in the following JSON format with each {sequence_i} corresponding to the actual sequence of tokens:\n\n```json\n{\n  \"{sequence_1}\": \"{expert_label}\",\n  \"{sequence_2}\": \"{expert_label}\",\n  \"{sequence_3}\": \"{expert_label}\",\n  ...\n}\n```"
            },
            {
                "role": "user",
                "content": "**Prompt**\n[{'role': 'system', 'content': \"Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>\"}, {'role': 'user', 'content': \"Story: Tanya is a software engineer working on a new mobile application. She wants to ensure that the application runs smoothly and is free of bugs before launching it to the public. Tanya observes that the app's code appears to be well-structured and free of errors. However, a server malfunction corrupts some of the code, causing unexpected issues. Tanya proceeds with testing the application.\\nQuestion: Does Tanya believe the app's code is well-structured and free of errors or corrupted due to a server malfunction?\\nChoose one of the following:\\na)Tanya believes the app's code is well-structured and free of errors.\\nb)Tanya believes the app's code is corrupted due to a server malfunction.\\nLet's think step by step:\"}]\n\n**Generation**\nThought: The story states that Tanya \u201cobserves that the app\u2019s code appears to be well-structured and free of errors.\u201d Although a server malfunction later corrupts some code, Tanya\u2019s belief about the code (before testing) is that it is well-structured and error-free. \n\nAnswer: a) Tanya believes the app's code is well-structured and free of errors."
            }
        ],
        "prompt": [
            {
                "role": "system",
                "content": "Answer the questions based on the context. Think through your reasoning before answering in 'Thought:'. Write your answer as Answer:<option>)<answer>"
            },
            {
                "role": "user",
                "content": "Story: Tanya is a software engineer working on a new mobile application. She wants to ensure that the application runs smoothly and is free of bugs before launching it to the public. Tanya observes that the app's code appears to be well-structured and free of errors. However, a server malfunction corrupts some of the code, causing unexpected issues. Tanya proceeds with testing the application.\nQuestion: Does Tanya believe the app's code is well-structured and free of errors or corrupted due to a server malfunction?\nChoose one of the following:\na)Tanya believes the app's code is well-structured and free of errors.\nb)Tanya believes the app's code is corrupted due to a server malfunction.\nLet's think step by step:"
            }
        ],
        "assistant": "Thought: The story states that Tanya \u201cobserves that the app\u2019s code appears to be well-structured and free of errors.\u201d Although a server malfunction later corrupts some code, Tanya\u2019s belief about the code (before testing) is that it is well-structured and error-free. \n\nAnswer: a) Tanya believes the app's code is well-structured and free of errors."
    }
]